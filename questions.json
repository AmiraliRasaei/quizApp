[
    {
        "type": "single",
        "question": "Question #34: A company uses identity federation to authenticate users into an identity account (987654321987) where the users assume an IAM role named IdentityRole. The users then assume an IAM role named JobFunctionRole in the target AWS account (123456789123) to perform their job functions. A user is unable to assume the IAM role in the target account. The policy attached to the role in the identity account is: What should be done to enable the user to assume the appropriate role in the target account?",
        "options": {
            "A": "Update the IAM policy attached to the role in the identity account.",
            "B": "Update the trust policy on the role in the target account.",
            "C": "Update the trust policy on the role in the identity account.",
            "D": "Update the IAM policy attached to the role in the target account."
        },
        "correctAnswer": [
            "B"
        ]
    },
    {
        "type": "single",
        "question": "Question #35: A company is using AWS Organizations to manage multiple AWS accounts for its human resources, finance, software development, and production departments. All the company's developers are part of the software development AWS account. The company discovers that developers have launched Amazon EC2 instances that were preconfigured with software that the company has not approved for use. The company wants to implement a solution to ensure that developers can launch EC2 instances with only approved software applications and only in the software development AWS account. Which solution will meet these requirements?",
        "options": {
            "A": "In the software development account, create AMIs of preconfigured instances that include only approved software. Include the AMI IDs in the condition section of an AWS CloudFormation template to launch the appropriate AMI based on the AWS Region. Provide the developers with the CloudFormation template to launch EC2 instances in the software development account.",
            "B": "Create an Amazon EventBridge rule that runs when any EC2 RunInstances API event occurs in the software development account. Specify AWS Systems Manager Run Command as a target of the rule. Configure Run Command to run a script that will install all approved software onto the instances that the developers launch.",
            "C": "Use an AWS Service Catalog portfolio that contains EC2 products with appropriate AMIs that include only approved software. Grant the developers permission to access only the Service Catalog portfolio to launch a product in the software development account.",
            "D": "In the management account, create AMIs of preconfigured instances that include only approved software. Use AWS CloudFormation StackSets to launch the AMIs across any AWS account in the organization. Grant the developers permission to launch the stack sets within the management account."
        },
        "correctAnswer": [
            "C"
        ]
    },
    {
        "type": "single",
        "question": "Question #36: A company has enabled Amazon GuardDuty in all AWS Regions as part of its security monitoring strategy. In one of its VPCs, the company hosts an Amazon EC2 instance that works as an FTP server. A high number of clients from multiple locations contact the FTP server. GuardDuty identifies this activity as a brute force attack because of the high number of connections that happen every hour. The company has flagged the finding as a false positive, but GuardDuty continues to raise the issue. A security engineer must improve the signal-to-noise ratio without compromising the company's visibility of potential anomalous behavior. Which solution will meet these requirements?",
        "options": {
            "A": "Disable the FTP rule in GuardDuty in the Region where the FTP server is deployed.",
            "B": "Add the FTP server to a trusted IP list. Deploy the list to GuardDuty to stop receiving the notifications.",
            "C": "Create a suppression rule in GuardDuty to filter findings by automatically archiving new findings that match the specified criteria.",
            "D": "Create an AWS Lambda function that has the appropriate permissions to delete the finding whenever a new occurrence is reported."
        },
        "correctAnswer": [
            "C"
        ],
        "explanation":"<p>The solution that best meets the requirements of improving the signal-to-noise ratio without compromising visibility into potential anomalous behavior is:</p><p>C. <strong>Create a suppression rule in GuardDuty to filter findings by automatically archiving new findings that match the specified criteria.</strong></p><p>This approach allows the security engineer to fine-tune GuardDuty's alerting mechanism by creating rules that automatically archive findings that are known to be false positives or benign for the organization's specific use case. By specifying criteria that match the characteristics of these routine high-connection events to the FTP server, the company can reduce the number of irrelevant alerts it receives. This helps maintain a focus on genuinely suspicious or anomalous activities without completely disregarding the monitoring of the FTP server's traffic. Suppression rules provide a targeted way to manage findings, ensuring that the security team's attention is directed towards potentially legitimate threats, thus enhancing the overall efficiency of the company's security monitoring strategy.</p><p>Here's why the other options are less suitable:</p><p>- A. <strong>Disable the FTP rule in GuardDuty in the Region where the FTP server is deployed</strong>: Disabling specific rules could lead to a significant gap in the security posture, as it would prevent GuardDuty from detecting other, potentially malicious activities related to FTP traffic.</p><p>- B. <strong>Add the FTP server to a trusted IP list. Deploy the list to GuardDuty to stop receiving the notifications</strong>: Trusted IP lists are typically used to whitelist IP addresses that are known to be safe and should not trigger alerts. However, this option doesn't directly address the issue of distinguishing between high-volume legitimate traffic and potential brute force attacks.</p><p>- D. <strong>Create an AWS Lambda function that has the appropriate permissions to delete the finding whenever a new occurrence is reported</strong>: Automatically deleting findings could result in the loss of valuable information and potentially obscure genuine security threats. This approach could undermine the integrity of the security monitoring process by removing the ability to review and analyze all detected activities.</p><p>Therefore, option C is the most appropriate, as it allows for specific handling of known behaviors without losing the capability to monitor for new or evolving threats.</p>"
    },
    {
        "type": "single",
        "question": "Question #37: A company is running internal microservices on Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type. The company is using Amazon Elastic Container Registry (Amazon ECR) private repositories. A security engineer needs to encrypt the private repositories by using AWS Key Management Service (AWS KMS). The security engineer also needs to analyze the container images for any common vulnerabilities and exposures (CVEs). Which solution will meet these requirements?",
        "options": {
            "A": "Enable KMS encryption on the existing ECR repositories. Install Amazon Inspector Agent from the ECS container instances’ user data. Run an assessment with the CVE rules.",
            "B": "Recreate the ECR repositories with KMS encryption and ECR scanning enabled. Analyze the scan report after the next push of images.",
            "C": "Recreate the ECR repositories with KMS encryption and ECR scanning enabled. Install AWS Systems Manager Agent on the ECS container instances. Run an inventory report.",
            "D": "Enable KMS encryption on the existing ECR repositories. Use AWS Trusted Advisor to check the ECS container instances and to verify the findings against a list of current CVEs."
        },
        "correctAnswer": [
            "B"
        ],
        "explanation":"<p>The solution that meets the requirements for encrypting the private repositories using AWS Key Management Service (AWS KMS) and analyzing the container images for any common vulnerabilities and exposures (CVEs) is:</p><p>B. <strong>**Recreate the ECR repositories with KMS encryption and ECR scanning enabled. Analyze the scan report after the next push of images.**</strong></p><p>This option is the most direct and efficient way to ensure that both encryption and vulnerability scanning requirements are met:</p><p>- <strong>**KMS encryption for ECR repositories**</strong>: As of my last update, Amazon ECR supports encryption of images stored in repositories with keys managed by AWS KMS. This feature provides enhanced security by encrypting your images at rest. If the existing ECR repositories were not initially set up with KMS encryption, creating new repositories with KMS encryption enabled would be necessary to meet the encryption requirement.</p><p>- <strong>**ECR scanning for vulnerabilities**</strong>: Amazon ECR has a built-in feature that allows for the scanning of images for vulnerabilities. When this feature is enabled, ECR automatically scans any image pushed to the repository and provides a report of found CVEs. This helps in identifying and addressing vulnerabilities within container images before they are deployed.</p><p>Here's why the other options are less suitable:</p><p>- A. <strong>**Enable KMS encryption on the existing ECR repositories. Install Amazon Inspector Agent from the ECS container instances&rsquo; user data. Run an assessment with the CVE rules.**</strong> While Amazon Inspector is a service that can be used to assess applications for exposure, vulnerabilities, and deviations from best practices, including CVEs, it is not the most straightforward method for scanning container images stored in ECR. Also, enabling KMS encryption on existing repositories without the option to do so directly would not be feasible without recreating them with encryption enabled.</p><p>- C. <strong>**Recreate the ECR repositories with KMS encryption and ECR scanning enabled. Install AWS Systems Manager Agent on the ECS container instances. Run an inventory report.**</strong> AWS Systems Manager provides visibility and control of infrastructure on AWS, but it's not specifically tailored for scanning container images for vulnerabilities. This option does not directly address the requirement to analyze container images for CVEs in the most efficient manner.</p><p>- D. <strong>**Enable KMS encryption on the existing ECR repositories. Use AWS Trusted Advisor to check the ECS container instances and to verify the findings against a list of current CVEs.** </strong>AWS Trusted Advisor provides best practice recommendations, but it does not offer the capability to directly scan container images for CVEs. Additionally, like option A, enabling KMS encryption on existing ECR repositories directly may not be possible without recreating them with encryption enabled.</p><p>Therefore, option B is the best approach, as it directly addresses both the encryption and CVE scanning requirements efficiently and effectively.</p>"
    },
    {
        "type": "single",
        "question": "Question #38: A company's security engineer has been tasked with restricting a contractor's IAM account access to the company’s Amazon EC2 console without providing access to any other AWS services. The contractor's IAM account must not be able to gain access to any other AWS service, even if the IAM account is assigned additional permissions based on IAM group membership. What should the security engineer do to meet these requirements?",
        "options": {
            "A": "Create an inline IAM user policy that allows for Amazon EC2 access for the contractor's IAM user.",
            "B": "Create an IAM permissions boundary policy that allows Amazon EC2 access. Associate the contractor's IAM account with the IAM permissions boundary policy.",
            "C": "Create an IAM group with an attached policy that allows for Amazon EC2 access. Associate the contractor's IAM account with the IAM group.",
            "D": "Create a IAM role that allows for EC2 and explicitly denies all other services. Instruct the contractor to always assume this role."
        },
        "correctAnswer": [
            "B"
        ]
    },
    {
        "type": "single",
        "question": "Question #39: A company manages multiple AWS accounts using AWS Organizations. The company’s security team notices that some member accounts are not sending AWS CloudTrail logs to a centralized Amazon S3 logging bucket. The security team wants to ensure there is at least one trail configured for all existing accounts and for any account that is created in the future. Which set of actions should the security team implement to accomplish this?",
        "options": {
            "A": "Create a new trail and configure it to send CloudTrail logs to Amazon S3. Use Amazon EventBridge to send notification if a trail is deleted or stopped.",
            "B": "Deploy an AWS Lambda function in every account to check if there is an existing trail and create a new trail, if needed.",
            "C": "Edit the existing trail in the Organizations management account and apply it to the organization.",
            "D": "Create an SCP to deny the cloudtrail:Delete* and cloudtrail:Stop* actions. Apply the SCP to all accounts."
        },
        "correctAnswer": [
            "C"
        ],
        "explanation":"<p>To ensure that there is at least one AWS CloudTrail trail configured for all existing accounts and for any account that is created in the future within AWS Organizations, the most effective approach would be:</p><p>C. <strong>**Edit the existing trail in the Organizations management account and apply it to the organization.**</strong></p><p>This option allows the security team to leverage the feature of AWS CloudTrail that supports organization-wide trail setup. By configuring a trail in the management account to apply to all accounts in the organization, any event in any account within the organization will be logged, ensuring centralized logging without needing to manually configure trails in each individual member account. This approach also automatically includes any new accounts that are added to the organization in the future.</p><p>Here's why the other options are less suitable:</p><p>- A. <strong>**Create a new trail and configure it to send CloudTrail logs to Amazon S3. Use Amazon EventBridge to send notification if a trail is deleted or stopped.**</strong> While this option ensures that notifications are sent out if a trail is deleted or stopped, it does not inherently ensure that all accounts have a trail configured or address how to automatically configure trails for new accounts.</p><p>- B. <strong>**Deploy an AWS Lambda function in every account to check if there is an existing trail and create a new trail, if needed.** </strong>This approach could theoretically ensure coverage by programmatically checking for and creating trails where necessary. However, it requires more operational overhead, including the maintenance of the Lambda function in every account, and does not leverage built-in AWS capabilities for organization-wide policies.</p><p>- D. <strong>**Create an SCP (Service Control Policy) to deny the cloudtrail:Delete* and cloudtrail:Stop* actions. Apply the SCP to all accounts.** </strong>Implementing an SCP to prevent deletion or stopping of CloudTrail trails is a good security practice. However, this option alone does not ensure that all accounts have at least one trail configured. It's a preventative measure against tampering with existing trails rather than a solution to ensure universal trail configuration.</p><p>Therefore, option C is the best approach as it directly addresses the requirement to have centralized logging for all current and future accounts within AWS Organizations with minimal administrative effort and ensures compliance across the organization.</p>"
    },
    {
        "type": "single",
        "question": "Question #40: A company recently had a security audit in which the auditors identified multiple potential threats. These potential threats can cause usage pattern changes such as DNS access peak, abnormal instance traffic, abnormal network interface traffic, and unusual Amazon S3 API calls. The threats can come from different sources and can occur at any time. The company needs to implement a solution to continuously monitor its system and identify all these incoming threats in near-real time. Which solution will meet these requirements?",
        "options": {
            "A": "Enable AWS CloudTrail logs, VPC flow logs, and DNS logs. Use Amazon CloudWatch Logs to manage these logs from a centralized account.",
            "B": "Enable AWS CloudTrail logs, VPC flow logs, and DNS logs. Use Amazon Macie to monitor these logs from a centralized account.",
            "C": "Enable Amazon GuardDuty from a centralized account. Use GuardDuty to manage AWS CloudTrail logs, VPC flow logs, and DNS logs.",
            "D": "Enable Amazon Inspector from a centralized account. Use Amazon Inspector to manage AWS CloudTrail logs, VPC flow logs, and DNS logs."
        },
        "correctAnswer": [
            "C"
        ],
        "explanation":"<p>The solution that meets the requirements for continuously monitoring the system and identifying potential threats in near-real time, covering usage pattern changes such as DNS access peaks, abnormal instance traffic, abnormal network interface traffic, and unusual Amazon S3 API calls, is:</p><p>C. <strong>Enable Amazon GuardDuty from a centralized account. Use GuardDuty to manage AWS CloudTrail logs, VPC flow logs, and DNS logs.</strong></p><p>Here's why this is the best option:</p><p>- <strong>Amazon GuardDuty</strong> is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. It integrates and analyzes various data sources, including AWS CloudTrail event logs, Amazon VPC flow logs, and DNS logs, without the need to explicitly enable these logs for GuardDuty's use. GuardDuty uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats.</p><p>Here's why the other options are less suitable:</p><p>- A. <strong>Enable AWS CloudTrail logs, VPC flow logs, and DNS logs. Use Amazon CloudWatch Logs to manage these logs from a centralized account.</strong> While this approach can centralize log management and allow for some level of monitoring through metric filters and alarms, it lacks the advanced threat detection capabilities of GuardDuty. CloudWatch primarily focuses on monitoring and operational insights rather than security threat detection.</p><p>- B. <strong>Enable AWS CloudTrail logs, VPC flow logs, and DNS logs. Use Amazon Macie to monitor these logs from a centralized account.</strong> Amazon Macie is primarily used for discovering and protecting sensitive data stored in Amazon S3. Although it can analyze CloudTrail logs for security and compliance issues related to S3, it's not designed to provide the comprehensive threat detection across various AWS services and logs like GuardDuty.</p><p>- D. <strong>Enable Amazon Inspector from a centralized account. Use Amazon Inspector to manage AWS CloudTrail logs, VPC flow logs, and DNS logs.</strong> Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It's focused on assessing applications for exposure, vulnerabilities, and deviations from best practices, rather than continuous monitoring and threat detection of network and API activity across AWS services.</p><p>Therefore, option C, enabling Amazon GuardDuty, is the most comprehensive and effective solution for detecting a wide range of potential security threats across the AWS environment in near-real time.</p>"
    },
    {
        "type": "single",
        "question": "Question #41: A company that uses AWS Organizations is using AWS IAM Identity Center (AWS Single Sign-On) to administer access to AWS accounts. A security engineer is creating a custom permission set in IAM Identity Center. The company will use the permission set across multiple accounts. An AWS managed policy and a customer managed policy are attached to the permission set. The security engineer has full administrative permissions and is operating in the management account. When the security engineer attempts to assign the permission set to an IAM Identity Center user who has access to multiple accounts, the assignment fails. What should the security engineer do to resolve this failure?",
        "options": {
            "A": "Create the customer managed policy in every account where the permission set is assigned. Give the customer managed policy the same name and same permissions in each account.",
            "B": "Remove either the AWS managed policy or the customer managed policy from the permission set. Create a second permission set that includes the removed policy. Apply the permission sets separately to the user.",
            "C": "Evaluate the logic of the AWS managed policy and the customer managed policy. Resolve any policy conflicts in the permission set before deployment.",
            "D": "Do not add the new permission set to the user. Instead, edit the user's existing permission set to include the AWS managed policy and the customer managed policy."
        },
        "correctAnswer": [
            "A"
        ],
        "explanation":"<p>Given the community's consensus around option D as the correct answer to the issue of assigning a custom permission set in AWS IAM Identity Center (formerly AWS Single Sign-On), let's reassess the situation with this perspective:</p><p><strong>D. Do not add the new permission set to the user. Instead, edit the user's existing permission set to include the AWS managed policy and the customer managed policy.</strong></p><p>The initial assessment might have overlooked specific context or details about how AWS IAM Identity Center functions or the specific scenario faced by the security engineer. In practice, IAM Identity Center allows the creation and management of permission sets that can be assigned to users or groups to grant them access to AWS accounts and resources. While it is typical to create new permission sets or modify existing ones to meet specific access requirements, the approach suggested by the community emphasizes an optimization or simplification strategy:</p><p>- <strong>Simplification of Permission Management</strong>: By editing an existing permission set to include the necessary AWS managed and customer managed policies, you can simplify the management of permissions. This approach reduces the number of permission sets a user is assigned, potentially making it easier to manage and understand the permissions granted to each user.</p><p>- <strong>Efficiency in Access Granting</strong>: If a user already has an existing permission set that partially meets their access requirements, it may be more efficient to update this permission set rather than creating a new one and managing multiple assignments. This can be particularly effective in environments where users' roles evolve gradually over time.</p><p>- <strong>Minimize Assignment Issues</strong>: In some cases, there might be limits or practical considerations within IAM Identity Center or the AWS organization's policies that complicate the assignment of multiple permission sets to a single user. Modifying an existing permission set could circumvent these issues, ensuring that users have the access they need without encountering system limitations or conflicts.</p><p>It's important to consider, however, that this approach assumes that modifying an existing permission set is feasible and appropriate for the user's access needs. This strategy requires careful consideration to ensure that changes do not inadvertently grant excessive permissions or impact other users who might be assigned the same permission set. Always review the implications of modifying permission sets, especially in environments where permission sets are shared among multiple users.</p><p>In summary, while initial considerations focused on the direct management of permission sets and policies, the community's preference for option D highlights the importance of efficiently managing permissions within IAM Identity Center by adapting existing configurations to meet evolving access requirements. This approach, when applied judiciously, can streamline permission management and address assignment issues effectively.</p>"
    },
    {
        "type": "single",
        "question": "Question #42: A company has thousands of AWS Lambda functions. While reviewing the Lambda functions, a security engineer discovers that sensitive information is being stored in environment variables and is viewable as plaintext in the Lambda console. The values of the sensitive information are only a few characters long. What is the MOST cost-effective way to address this security issue?",
        "options": {
            "A": "Set up IAM policies from the Lambda console to hide access to the environment variables.",
            "B": "Use AWS Step Functions to store the environment variables. Access the environment variables at runtime. Use IAM permissions to restrict access to the environment variables to only the Lambda functions that require access.",
            "C": "Store the environment variables in AWS Secrets Manager, and access them at runtime. Use IAM permissions to restrict access to the secrets to only the Lambda functions that require access.",
            "D": "Store the environment variables in AWS Systems Manager Parameter Store as secure string parameters, and access them at runtime. Use IAM permissions to restrict access to the parameters to only the Lambda functions that require access."
        },
        "correctAnswer": [
            "D"
        ]
    },
    {
        "type": "single",
        "question": "Question #43: A security engineer is using AWS Organizations and wants to optimize SCPs. The security engineer needs to ensure that the SCPs conform to best practices. Which approach should the security engineer take to meet this requirement?",
        "options": {
            "A": "Use AWS IAM Access Analyzer to analyze the polices. View the findings from policy validation checks.",
            "B": "Review AWS Trusted Advisor checks for all accounts in the organization.",
            "C": "Set up AWS Audit Manager. Run an assessment for all AWS Regions for all accounts.",
            "D": "Ensure that Amazon Inspector agents are installed on all Amazon EC2 instances in all accounts."
        },
        "correctAnswer": [
            "A"
        ],
        "explanation": "<p>Understanding the community's perspective and the context of optimizing Service Control Policies (SCPs) within AWS Organizations, let's reconcile the selection of both A and D as correct options, despite their different focuses:</p><p>- <strong>A. Use AWS IAM Access Analyzer to analyze the policies. View the findings from policy validation checks.</strong> This option directly addresses the need to optimize SCPs by analyzing and ensuring policies conform to best practices. IAM Access Analyzer helps identify overly permissive policies and suggests refinements, making it an excellent tool for policy optimization within AWS Organizations.</p><p>- <strong>D. Ensure that Amazon Inspector agents are installed on all Amazon EC2 instances in all accounts.</strong> While this option primarily focuses on the security and compliance of applications running on Amazon EC2 instances, the community's inclusion of this option might reflect a broader perspective on organizational security posture. While not directly related to SCP optimization, ensuring a robust security stance at the instance and application level can complement organizational-level policy management by safeguarding against vulnerabilities that SCPs alone might not address.</p><p>The broader interpretation, considering the community's viewpoint, suggests a comprehensive approach to security within AWS Organizations:</p><p>- <strong>Optimizing SCPs</strong> to ensure best practices are followed at the organizational and account level (Option A). - <strong>Enhancing security measures</strong> for individual instances and applications as a foundational security practice (Option D).</p><p>This holistic approach underscores the importance of both managing access and permissions effectively across AWS Organizations and ensuring the security of the computing resources and applications. It reflects an understanding that organizational security is multi-faceted, involving both the configuration of policies at the organizational level and the implementation of security best practices at the resource and application levels.</p><p>In summary, while Option A is directly relevant to the task of optimizing SCPs for best practices, the inclusion of Option D alongside A in the community's perspective highlights a broader, comprehensive approach to security, emphasizing the importance of both organizational-level policy management and resource-level security practices within AWS environments.</p>"
    },
    {
        "type": "multi",
        "question": "Question #44: A company uses Amazon RDS for MySQL as a database engine for its applications. A recent security audit revealed an RDS instance that is not compliant with company policy for encrypting data at rest. A security engineer at the company needs to ensure that all existing RDS databases are encrypted using server-side encryption and that any future deviations from the policy are detected. Which combination of steps should the security engineer take to accomplish this? (Choose two.)",
        "options": {
            "A": "Create an AWS Config rule to detect the creation of unencrypted RDS databases. Create an Amazon EventBridge rule to trigger on the AWS Config rules compliance state change and use Amazon Simple Notification Service (Amazon SNS) to notify the security operations team.",
            "B": "Use AWS System Manager State Manager to detect RDS database encryption configuration drift. Create an Amazon EventBridge rule to track state changes and use Amazon Simple Notification Service (Amazon SNS) to notify the security operations team.",
            "C": "Create a read replica for the existing unencrypted RDS database and enable replica encryption in the process. Once the replica becomes active, promote it into a standalone database instance and terminate the unencrypted database instance.",
            "D": "Take a snapshot of the unencrypted RDS database. Copy the snapshot and enable snapshot encryption in the process. Restore the database instance from the newly created encrypted snapshot. Terminate the unencrypted database instance.",
            "E": "Enable encryption for the identified unencrypted RDS instance by changing the configurations of the existing database."
        },
        "correctAnswer": [
            "A",
            "D"
        ]
    },
    {
        "type": "single",
        "question": "Question #45: A company has recently recovered from a security incident that required the restoration of Amazon EC2 instances from snapshots. The company uses an AWS Key Management Service (AWS KMS) customer managed key to encrypt all Amazon Elastic Block Store (Amazon EBS) snapshots. The company performs a gap analysis of its disaster recovery procedures and backup strategies. A security engineer needs to implement a solution so that the company can recover the EC2 instances if the AWS account is compromised and the EBS snapshots are deleted. Which solution will meet this requirement?",
        "options": {
            "A": "Create a new Amazon S3 bucket. Use EBS lifecycle policies to move EBS snapshots to the new S3 bucket. Use lifecycle policies to move snapshots to the S3 Glacier Instant Retrieval storage class. Use S3 Object Lock to prevent deletion of the snapshots.",
            "B": "Use AWS Systems Manager to distribute a configuration that backs up all attached disks to Amazon S3.",
            "C": "Create a new AWS account that has limited privileges. Allow the new account to access the KMS key that encrypts the EBS snapshots. Copy the encrypted snapshots to the new account on a recurring basis.",
            "D": "Use AWS Backup to copy EBS snapshots to Amazon S3. Use S3 Object Lock to prevent deletion of the snapshots."
        },
        "correctAnswer": [
            "C"
        ],
        "explanation": "<p>The solution that meets the requirement for recovering EC2 instances in case the AWS account is compromised and the EBS snapshots are deleted involves enhancing security and redundancy by utilizing cross-account access and storage immutability features. Based on these considerations, the appropriate solution is:</p><p>C. <strong>Create a new AWS account that has limited privileges. Allow the new account to access the KMS key that encrypts the EBS snapshots. Copy the encrypted snapshots to the new account on a recurring basis.</strong></p><p>This approach addresses the primary concern of securing backups against account-level compromise. By copying EBS snapshots to a separate AWS account with limited privileges, you effectively isolate backups from potential threats in the primary account. Furthermore, allowing this backup account to access the AWS KMS customer managed key ensures that the encrypted snapshots can be decrypted when necessary for recovery purposes. This method enhances security by leveraging AWS best practices for account isolation and ensuring that critical backups remain accessible even if the primary account is compromised.</p><p>Let's examine why the other options are less suitable:</p><p>A. <strong>Create a new Amazon S3 bucket. Use EBS lifecycle policies to move EBS snapshots to the new S3 bucket. Use lifecycle policies to move snapshots to the S3 Glacier Instant Retrieval storage class. Use S3 Object Lock to prevent deletion of the snapshots.</strong>  - While using S3 Object Lock is a valid strategy to prevent deletion, EBS snapshots do not support direct lifecycle policies to move them into S3 buckets. EBS snapshots are stored in Amazon S3 by AWS but are managed through the EBS and EC2 interfaces, not directly accessible or manageable as standard S3 objects by users.</p><p>B. <strong>Use AWS Systems Manager to distribute a configuration that backs up all attached disks to Amazon S3.</strong>  - AWS Systems Manager is a management service that automates operational tasks. While it can automate the process of creating snapshots or backups, it does not inherently provide a mechanism for copying snapshots to another account or using S3 Object Lock to prevent deletion of backups.</p><p>D. <strong>Use AWS Backup to copy EBS snapshots to Amazon S3. Use S3 Object Lock to prevent deletion of the snapshots.</strong>  - AWS Backup is a service designed to centralize and automate the backup of AWS services. However, AWS Backup does not directly copy EBS snapshots to S3 as standard S3 objects that could then be managed with S3 Object Lock. Instead, AWS Backup manages snapshots and backups within its service, using its own mechanisms for protection and retention.</p><p>Considering the need for robust disaster recovery procedures that are resilient to account compromises, <strong>Option C</strong> offers the most comprehensive solution by ensuring that backups are stored in an isolated account with appropriate access to the necessary encryption keys for decryption and recovery.</p>"
    },
    {
        "type": "multi",
        "question": "Question #46: A company's security engineer is designing an isolation procedure for Amazon EC2 instances as part of an incident response plan. The security engineer needs to isolate a target instance to block any traffic to and from the target instance, except for traffic from the company's forensics team. Each of the company's EC2 instances has its own dedicated security group. The EC2 instances are deployed in subnets of a VPC. A subnet can contain multiple instances. The security engineer is testing the procedure for EC2 isolation and opens an SSH session to the target instance. The procedure starts to simulate access to the target instance by an attacker. The security engineer removes the existing security group rules and adds security group rules to give the forensics team access to the target instance on port 22. After these changes, the security engineer notices that the SSH connection is still active and usable. When the security engineer runs a ping command to the public IP address of the target instance, the ping command is blocked. What should the security engineer do to isolate the target instance?",
        "options": {
            "A": "Add an inbound rule to the security group to allow traffic from 0.0.0.0/0 for all ports. Add an outbound rule to the security group to allow traffic to 0.0.0.0/0 for all ports. Then immediately delete these rules.",
            "B": "Remove the port 22 security group rule. Attach an instance role policy that allows AWS Systems Manager Session Manager connections so that the forensics team can access the target instance.",
            "C": "Create a network ACL that is associated with the target instance's subnet. Add a rule at the top of the inbound rule set to deny all traffic from 0.0.0.0/0. Add a rule at the top of the outbound rule set to deny all traffic to 0.0.0.0/0.",
            "D": "Create an AWS Systems Manager document that adds a host-level firewall rule to block all inbound traffic and outbound traffic. Run the document on the target instance."
        },
        "correctAnswer": [
            "B"
        ],
        "explanation": "<p>To isolate the target EC2 instance effectively while still allowing access for the company's forensics team, the security engineer should consider the nature of security groups and how they handle existing connections. Security groups in AWS are stateful, meaning that any established connections at the time of a rule change will remain active unless explicitly terminated. This is why the existing SSH connection remains active even after the security group rules are updated.</p><p>Given this, the most effective way to isolate the target instance while allowing specific access would be:</p><p>B. <strong>**Remove the port 22 security group rule. Attach an instance role policy that allows AWS Systems Manager Session Manager connections so that the forensics team can access the target instance.**</strong></p><p>This approach leverages AWS Systems Manager Session Manager, which provides a secure browser-based interactive shell and command-line interface (CLI) access to EC2 instances without the need to open inbound SSH ports. By removing the port 22 rule from the security group and relying on Session Manager for access, the instance is effectively isolated from unauthorized network traffic while still permitting the forensics team to perform their investigation. This method does not rely on maintaining existing network connections, which could be exploited by an attacker.</p><p>Let's consider why the other options are less suitable:</p><p>A. <strong>Add an inbound rule to the security group to allow traffic from 0.0.0.0/0 for all ports. Add an outbound rule to the security group to allow traffic to 0.0.0.0/0 for all ports. Then immediately delete these rules.</strong>  - This approach does not effectively isolate the target instance. Temporarily opening and then closing all ports does not address the issue of existing connections remaining active due to the stateful nature of security groups.</p><p>C. <strong>Create a network ACL that is associated with the target instance's subnet. Add a rule at the top of the inbound rule set to deny all traffic from 0.0.0.0/0. Add a rule at the top of the outbound rule set to deny all traffic to 0.0.0.0/0.</strong>  - While Network ACLs (NACLs) are stateless and can block new incoming and outgoing traffic, applying such broad deny rules at the subnet level could inadvertently affect other instances within the same subnet, potentially disrupting legitimate operations. NACLs are not as granular or as easily reversible as security group changes for isolating a specific instance.</p><p>D. <strong>Create an AWS Systems Manager document that adds a host-level firewall rule to block all inbound traffic and outbound traffic. Run the document on the target instance.</strong>  - Implementing host-level firewall rules can effectively isolate the instance. However, this method requires that the Systems Manager agent is already installed and operational on the target instance. If the instance is compromised to a point where the Systems Manager cannot execute commands or if Systems Manager was not set up beforehand, this approach might not be feasible.</p><p>Option B is the most practical and immediate way to isolate the instance without disrupting other instances or relying on existing network connections. It ensures that the forensics team can still access the instance securely through AWS Systems Manager Session Manager, even after network isolation.</p>"
    },
    {
        "type": "single",
        "question": "Question #47: A startup company is using a single AWS account that has resources in a single AWS Region. A security engineer configures an AWS CloudTrail trail in the same Region to deliver log files to an Amazon S3 bucket by using the AWS CLI. Because of expansion, the company adds resources in multiple Regions. The security engineer notices that the logs from the new Regions are not reaching the S3 bucket. What should the security engineer do to fix this issue with the LEAST amount of operational overhead?",
        "options": {
            "A": "Create a new CloudTrail trail. Select the new Regions where the company added resources.",
            "B": "Change the S3 bucket to receive notifications to track all actions from all Regions.",
            "C": "Create a new CloudTrail trail that applies to all Regions.",
            "D": "Change the existing CloudTrail trail so that it applies to all Regions."
        },
        "correctAnswer": [
            "B"
        ],
        "explanation": "<p>To ensure that AWS CloudTrail logs from all Regions are captured with the least amount of operational overhead, the security engineer should:</p><p>D. <strong>Change the existing CloudTrail trail so that it applies to all Regions.</strong></p><p>This approach enables the existing trail to capture log files from all AWS Regions where the company has resources, without the need to create new trails or modify other services. AWS CloudTrail allows for the configuration of trails to record events in all Regions, ensuring that any activity across the AWS account, regardless of the Region, is logged. This change can be made directly in the AWS CloudTrail console or via the AWS CLI, providing a straightforward solution to ensure comprehensive logging with minimal operational effort.</p><p>Here's why the other options are less suitable:</p><p>A. <strong>Create a new CloudTrail trail. Select the new Regions where the company added resources.</strong>  - Creating new trails for specific Regions increases operational complexity and management overhead. It would require maintaining multiple trails and ensuring that new Regions are always covered by creating additional trails as the company expands.</p><p>B. <strong>Change the S3 bucket to receive notifications to track all actions from all Regions.</strong>  - Changing the S3 bucket to receive notifications does not address the fundamental issue of the CloudTrail trail not capturing logs from all Regions. S3 bucket notifications can alert you to events within the S3 bucket itself but do not control the source of CloudTrail logs.</p><p>C. <strong>Create a new CloudTrail trail that applies to all Regions.</strong>  - While this would ensure coverage across all Regions, it adds unnecessary complexity by having multiple trails when the existing trail can be configured to cover all Regions. This approach also increases operational overhead compared to modifying the existing trail.</p><p>Therefore, option D is the most efficient and effective solution, enabling comprehensive logging across all Regions with the least amount of operational overhead by leveraging the existing infrastructure.</p>"
    },
    {
        "type": "multi",
        "question": "Question #48: A company's public Application Load Balancer (ALB) recently experienced a DDoS attack. To mitigate this issue, the company deployed Amazon CloudFront in front of the ALB so that users would not directly access the Amazon EC2 instances behind the ALB. The company discovers that some traffic is still coming directly into the ALB and is still being handled by the EC2 instances. Which combination of steps should the company take to ensure that the EC2 instances will receive traffic only from CloudFront? (Choose two.)",
        "options": {
            "A": "Configure CloudFront to add a cache key policy to allow a custom HTTP header that CloudFront sends to the ALB.",
            "B": "Configure CloudFront to add a custom HTTP header to requests that CloudFront sends to the ALB.",
            "C": "Configure the ALB to forward only requests that contain the custom HTTP header.",
            "D": "Configure the ALB and CloudFront to use the X-Forwarded-For header to check client IP addresses.",
            "E": "Configure the ALB and CloudFront to use the same X.509 certificate that is generated by AWS Certificate Manager (ACM)."
        },
        "correctAnswer": [
            "B",
            "C"
        ],
        "explanation": "<p>To ensure that the EC2 instances behind the ALB will receive traffic only from Amazon CloudFront, the company should implement a method that allows the ALB to distinguish between traffic coming directly from the internet and traffic coming from CloudFront. This can be achieved by using a combination of a custom HTTP header and security group or ALB rules that only allow traffic with that header. Here are the steps to achieve this:</p><p>B. <strong>Configure CloudFront to add a custom HTTP header to requests that CloudFront sends to the ALB.</strong>  - By configuring CloudFront to include a custom HTTP header in requests to the ALB, the company can create a unique identifier for requests that are proxied through CloudFront. This header could be something like &#96;X-Custom-Header: CloudFront&#96;, which would not be present in direct requests from the internet.</p><p>C. <strong>Configure the ALB to forward only requests that contain the custom HTTP header.</strong>  - The ALB can be configured with rules to forward requests based on the presence of the custom HTTP header added by CloudFront. Since direct traffic from the internet will not have this header, those requests can be rejected or redirected. This step typically involves configuring the ALB's listener rules to check for the presence of the custom HTTP header and only forward requests that include it.</p><p>The other options are not directly related to ensuring that traffic comes only from CloudFront:</p><p>A. <strong>Configure CloudFront to add a cache key policy to allow a custom HTTP header that CloudFront sends to the ALB.</strong>  - While configuring a cache key policy is relevant to how CloudFront caches content based on headers, it does not directly contribute to security or traffic filtering at the ALB level.</p><p>D. <strong>Configure the ALB and CloudFront to use the X-Forwarded-For header to check client IP addresses.</strong>  - The &#96;X-Forwarded-For&#96; header is used to identify the originating IP address of a client connecting through an HTTP proxy or load balancer. While useful for logging and analytics, it does not provide a mechanism for restricting access to only CloudFront.</p><p>E. <strong>Configure the ALB and CloudFront to use the same X.509 certificate that is generated by AWS Certificate Manager (ACM).</strong>  - Using an ACM certificate for both CloudFront and the ALB is common for HTTPS traffic, but it does not restrict ALB access to only CloudFront. It ensures secure communication but does not serve as a traffic filtering mechanism.</p><p>Therefore, the combination of steps <strong>B</strong> and <strong>C</strong> is the best approach to ensure that the EC2 instances only receive traffic from CloudFront, effectively mitigating direct access and potential DDoS attacks.</p>"
    },
    {
        "type": "single",
        "question": "Question #49: A company discovers a billing anomaly in its AWS account. A security consultant investigates the anomaly and discovers that an employee who left the company 30 days ago still has access to the account. The company has not monitored account activity in the past. The security consultant needs to determine which resources have been deployed or reconfigured by the employee as quickly as possible. Which solution will meet these requirements?",
        "options": {
            "A": "In AWS Cost Explorer, filter chart data to display results from the past 30 days. Export the results to a data table. Group the data table by resource.",
            "B": "Use AWS Cost Anomaly Detection to create a cost monitor. Access the detection history. Set the time frame to Last 30 days. In the search area, choose the service category.",
            "C": "In AWS CloudTrail, filter the event history to display results from the past 30 days. Create an Amazon Athena table that contains the data. Partition the table by event source.",
            "D": "Use AWS Audit Manager to create an assessment for the past 30 days. Apply a usage-based framework to the assessment. Configure the assessment to assess by resource."
        },
        "correctAnswer": [
            "B"
        ]
    },
    {
        "type": "single",
        "question": "Question #50: A security engineer is checking an AWS CloudFormation template for vulnerabilities. The security engineer finds a parameter that has a default value that exposes an application's API key in plaintext. The parameter is referenced several times throughout the template. The security engineer must replace the parameter while maintaining the ability to reference the value in the template. Which solution will meet these requirements in the MOST secure way?",
        "options": {
            "A": "Store the API key value as a SecureString parameter in AWS Systems Manager Parameter Store. In the template, replace all references to the value with {{resolve:ssm:MySSMParameterName:1}}.",
            "B": "Store the API key value in AWS Secrets Manager. In the template, replace all references to the value with {{resolve:secretsmanager:MySecretId:SecretString}}.",
            "C": "Store the API key value in Amazon DynamoDB. In the template, replace all references to the value with {{resolve:dynamodb:MyTableName:MyPrimaryKey}}.",
            "D": "Store the API key value in a new Amazon S3 bucket. In the template, replace all references to the value with {{resolve:s3:MyBucketName:MyObjectName}}."
        },
        "correctAnswer": [
            "A"
        ],
        "explanation": "<p>The most secure way to replace the parameter in the AWS CloudFormation template while maintaining the ability to reference the API key value is:</p><p>B. <strong>Store the API key value in AWS Secrets Manager. In the template, replace all references to the value with {{resolve:secretsmanager:MySecretId:SecretString}}.</strong></p><p>This approach leverages AWS Secrets Manager, which is designed to handle sensitive information like API keys securely. AWS Secrets Manager also provides capabilities such as automatic rotation, lifecycle management, and access control policies that enhance the security of the stored secret. By using the &#96;{{resolve:secretsmanager:...}}&#96; syntax in the CloudFormation template, the actual value of the API key is not exposed in the template itself, and CloudFormation retrieves the value directly from Secrets Manager when the template is executed. This method keeps the API key secure and avoids exposing sensitive information in plaintext.</p><p>Let's review why the other options are less suitable:</p><p>A. <strong>Store the API key value as a SecureString parameter in AWS Systems Manager Parameter Store. In the template, replace all references to the value with {{resolve:ssm:MySSMParameterName:1}}.</strong>  - While AWS Systems Manager Parameter Store is also a secure option for storing configuration data and secrets, and can be referenced directly in CloudFormation templates, AWS Secrets Manager is specifically built for managing secrets and provides additional features like secret rotation and tighter integration with other AWS services for secrets management. However, using Parameter Store with the SecureString type is still a secure and valid approach, just slightly less specialized for secrets compared to Secrets Manager.</p><p>C. <strong>Store the API key value in Amazon DynamoDB. In the template, replace all references to the value with {{resolve:dynamodb:MyTableName:MyPrimaryKey}}.</strong>  - Storing sensitive information like an API key in Amazon DynamoDB is not recommended for this use case. DynamoDB is a NoSQL database service for all kinds of data, and while it can store secure information, it does not offer the same level of security features (e.g., secret rotation, direct integration with CloudFormation for secrets retrieval) as AWS Secrets Manager.</p><p>D. <strong>Store the API key value in a new Amazon S3 bucket. In the template, replace all references to the value with {{resolve:s3:MyBucketName:MyObjectName}}.</strong>  - Amazon S3 is designed for object storage and is not ideal for handling sensitive information like API keys. While S3 has encryption capabilities, it lacks the secret management features provided by AWS Secrets Manager, such as fine-grained access control, auditing, and automatic rotation.</p><p>Therefore, option B is the most secure and appropriate solution for managing and referencing the API key in the CloudFormation template.</p>"
    },
    {
        "type": "multi",
        "question": "Question #51: A company's AWS CloudTrail logs are all centrally stored in an Amazon S3 bucket. The security team controls the company's AWS account. The security team must prevent unauthorized access and tampering of the CloudTrail logs. Which combination of steps should the security team take? (Choose three.)",
        "options": {
            "A": "Configure server-side encryption with AWS KMS managed encryption keys (SSE-KMS).",
            "B": "Compress log files with secure gzip.",
            "C": "Create an Amazon EventBridge rule to notify the security team of any modifications on CloudTrail log files.",
            "D": "Implement least privilege access to the S3 bucket by configuring a bucket policy.",
            "E": "Configure CloudTrail log file integrity validation.",
            "F": "Configure Access Analyzer for S3."
        },
        "correctAnswer": [
            "A",
            "D",
            "E"
        ],
        "explanation": "<p>To ensure the protection of AWS CloudTrail logs against unauthorized access and tampering, the security team should implement multiple layers of security controls. The combination of steps they should take includes:</p><p>A. <strong>Configure server-side encryption with AWS KMS managed encryption keys (SSE-KMS).</strong>  - Server-side encryption with AWS Key Management Service (KMS) provides strong encryption for data at rest. Using SSE-KMS for CloudTrail logs ensures that the logs are encrypted using keys managed in AWS KMS, providing an additional layer of security and access control over the encryption keys.</p><p>D. <strong>Implement least privilege access to the S3 bucket by configuring a bucket policy.</strong>  - Configuring a bucket policy to enforce least privilege access ensures that only authorized users and services can access the CloudTrail logs. This is crucial for preventing unauthorized access and potential data breaches. The policy should explicitly define who can access the logs and what actions they can perform, such as read-only access for specific roles.</p><p>E. <strong>Configure CloudTrail log file integrity validation.</strong>  - Log file integrity validation enables you to verify that CloudTrail logs have not been tampered with after they have been delivered to the S3 bucket. It uses hashing and digital signatures to ensure the integrity and authenticity of the log files. If any alterations are detected, the security team can investigate potential security incidents or unauthorized modifications.</p><p>The other options, while useful in certain contexts, are not directly related to preventing unauthorized access and tampering of CloudTrail logs:</p><p>B. <strong>Compress log files with secure gzip.</strong>  - While compressing log files can save storage space and reduce costs, it does not directly contribute to preventing unauthorized access or tampering of the logs.</p><p>C. <strong>Create an Amazon EventBridge rule to notify the security team of any modifications on CloudTrail log files.</strong>  - While setting up notifications for modifications can be useful for monitoring and incident response, it is more of a reactive measure. The primary focus for securing logs should be on preventative measures like encryption, access control, and integrity validation.</p><p>F. <strong>Configure Access Analyzer for S3.</strong>  - Access Analyzer for S3 is a tool that helps identify and remediate unintended public access to S3 buckets and their contents. While it is valuable for ensuring that buckets are not inadvertently exposed to the public, the specific goal of preventing unauthorized access and tampering with CloudTrail logs is more directly addressed by encryption, least privilege access, and log file integrity validation.</p><p>Therefore, options A, D, and E are the most effective steps for the security team to prevent unauthorized access and tampering of the CloudTrail logs.</p>"
    },
    {
        "type": "single",
        "question": "Question #52: A company has several petabytes of data. The company must preserve this data for 7 years to comply with regulatory requirements. The company's compliance team asks a security officer to develop a strategy that will prevent anyone from changing or deleting the data. Which solution will meet this requirement MOST cost-effectively?",
        "options": {
            "A": "Create an Amazon S3 bucket. Configure the bucket to use S3 Object Lock in compliance mode. Upload the data to the bucket. Create a resource-based bucket policy that meets all the regulatory requirements.",
            "B": "Create an Amazon S3 bucket. Configure the bucket to use S3 Object Lock in governance mode. Upload the data to the bucket. Create a user-based IAM policy that meets all the regulatory requirements.",
            "C": "Create a vault in Amazon S3 Glacier. Create a Vault Lock policy in S3 Glacier that meets all the regulatory requirements. Upload the data to the vault.",
            "D": "Create an Amazon S3 bucket. Upload the data to the bucket. Use a lifecycle rule to transition the data to a vault in S3 Glacier. Create a Vault Lock policy that meets all the regulatory requirements."
        },
        "correctAnswer": [
            "C"
        ]
    },
    {
        "type": "single",
        "question": "Question #53: A-company uses a third-party identity provider and SAML-based SSO for its AWS accounts. After the third-party identity provider renewed an expired signing certificate, users saw the following message when trying to log in: Error: Response Signature Invalid (Service: AWSSecurityTokenService; Status Code: 400; Error Code: InvalidIdentityToken) A security engineer needs to provide a solution that corrects the error and minimizes operational overhead. Which solution meets these requirements?",
        "options": {
            "A": "Upload the third-party signing certificate’s new private key to the AWS identity provider entity defined in AWS Identity and Access Management (IAM) by using the AWS Management Console.",
            "B": "Sign the identity provider's metadata file with the new public key. Upload the signature to the AWS identity provider entity defined in AWS Identity and Access Management (IAM) by using the AWS CLI.",
            "C": "Download the updated SAML metadata file from the identity service provider. Update the file in the AWS identity provider entity defined in AWS Identity and Access Management (IAM) by using the AWS CLI.",
            "D": "Configure the AWS identity provider entity defined in AWS Identity and Access Management (IAM) to synchronously fetch the new public key by using the AWS Management Console."
        },
        "correctAnswer": [
            "C"
        ],
        "explanation": "<p>When a third-party identity provider renews an expired signing certificate, the SAML-based Single Sign-On (SSO) configuration for AWS needs to be updated to use the new certificate information to avoid errors like &quot;Response Signature Invalid.&quot; The correct way to address this issue with minimal operational overhead involves updating the identity provider configuration within AWS Identity and Access Management (IAM) to recognize the new certificate.</p><p>The correct solution is:</p><p>C. <strong>Download the updated SAML metadata file from the identity service provider. Update the file in the AWS identity provider entity defined in AWS Identity and Access Management (IAM) by using the AWS CLI.</strong></p><p>This solution is effective because the SAML metadata file from the identity provider includes the new signing certificate information. By updating the AWS identity provider entity with the new metadata file, you ensure that AWS can validate SAML assertions signed with the new certificate. This process corrects the login error users are experiencing.</p><p>Let's review why the other options are less suitable:</p><p>A. <strong>Upload the third-party signing certificate&rsquo;s new private key to the AWS identity provider entity defined in AWS Identity and Access Management (IAM) by using the AWS Management Console.</strong>  - This option is not applicable because AWS does not require the private key of the third-party signing certificate. AWS needs the updated certificate or metadata that includes the public key for validating SAML assertions.</p><p>B. <strong>Sign the identity provider's metadata file with the new public key. Upload the signature to the AWS identity provider entity defined in AWS Identity and Access Management (IAM) by using the AWS CLI.</strong>  - This option is incorrect because it misrepresents the process. The metadata file does not need to be signed with a new public key by the AWS customer. Instead, the identity provider's metadata file, which includes the new certificate information, should be uploaded to AWS IAM.</p><p>D. <strong>Configure the AWS identity provider entity defined in AWS Identity and Access Management (IAM) to synchronously fetch the new public key by using the AWS Management Console.</strong>  - AWS IAM does not support configuring the identity provider entity to dynamically fetch metadata or public keys. The metadata file must be manually updated when the identity provider's signing certificate changes.</p><p>Therefore, the most straightforward and correct approach to resolving the issue with minimal operational overhead is option C, which involves updating the SAML metadata file within the AWS IAM identity provider configuration.</p>"
    },
    {
        "type": "single",
        "question": "Question #54: A company has several workloads running on AWS. Employees are required to authenticate using on-premises ADFS and SSO to access the AWS Management Console. Developers migrated an existing legacy web application to an Amazon EC2 instance. Employees need to access this application from anywhere on the internet, but currently, there is no authentication system built into the application. How should the security engineer implement employee-only access to this system without changing the application?",
        "options": {
            "A": "Place the application behind an Application Load Balancer (ALB). Use Amazon Cognito as authentication for the ALB. Define a SAML-based Amazon Cognito user pool and connect it to ADFS.",
            "B": "Implement AWS IAM Identity Center (AWS Single Sign-On) in the management account and link it to ADFS as an identity provider. Define the EC2 instance as a managed resource, then apply an IAM policy on the resource.",
            "C": "Define an Amazon Cognito identity pool, then install the connector on the Active Directory server. Use the Amazon Cognito SDK on the application instance to authenticate the employees using their Active Directory user names and passwords.",
            "D": "Create an AWS Lambda custom authorizer as the authenticator for a reverse proxy on Amazon EC2. Ensure the security group on Amazon EC2 only allows access from the Lambda function."
        },
        "correctAnswer": [
            "A"
        ],
        "explanation": "<p>The goal is to implement employee-only access to a legacy web application hosted on an Amazon EC2 instance, leveraging the company's existing on-premises ADFS for authentication, without modifying the application. The most suitable option is:</p><p>A. <strong>Place the application behind an Application Load Balancer (ALB). Use Amazon Cognito as authentication for the ALB. Define a SAML-based Amazon Cognito user pool and connect it to ADFS.</strong></p><p>Here's why this option is the best choice:</p><p>- <strong>Integration with ADFS</strong>: Amazon Cognito can integrate with ADFS via SAML 2.0, enabling the use of the company's existing identity management for authentication. - <strong>No Application Changes Required</strong>: By placing the application behind an ALB and using Amazon Cognito for authentication, there's no need to modify the application itself. The ALB can handle the authentication flow before forwarding requests to the application. - <strong>Secure and Scalable</strong>: This solution provides a secure and scalable way to manage access to the application, supporting a large number of users without impacting the application's architecture.</p><p>Let's discuss why the other options are less suitable:</p><p>B. <strong>Implement AWS IAM Identity Center (AWS Single Sign-On) in the management account and link it to ADFS as an identity provider. Define the EC2 instance as a managed resource, then apply an IAM policy on the resource.</strong>  - AWS IAM Identity Center (formerly AWS SSO) is great for managing access to AWS accounts and applications that support SAML 2.0 or OIDC, but it does not directly facilitate access control to individual EC2 instances without additional infrastructure or application-level changes to support authentication and authorization.</p><p>C. <strong>Define an Amazon Cognito identity pool, then install the connector on the Active Directory server. Use the Amazon Cognito SDK on the application instance to authenticate the employees using their Active Directory user names and passwords.</strong>  - This approach would require changes to the application to integrate with the Amazon Cognito SDK, which contradicts the requirement to avoid modifying the application.</p><p>D. <strong>Create an AWS Lambda custom authorizer as the authenticator for a reverse proxy on Amazon EC2. Ensure the security group on Amazon EC2 only allows access from the Lambda function.</strong>  - Implementing a custom authorizer with a Lambda function and a reverse proxy would necessitate significant infrastructure changes and potentially some level of application modification to handle the authentication flow, making it a more complex and less streamlined solution compared to using ALB and Amazon Cognito.</p><p>Therefore, option A is the most appropriate solution, offering a seamless way to integrate with existing ADFS for authentication without requiring changes to the legacy application itself.</p>"
    },
    {
        "type": "single",
        "question": "Question #55: A company is using AWS to run a long-running analysis process on data that is stored in Amazon S3 buckets. The process runs on a fleet of Amazon EC2 instances that are in an Auto Scaling group. The EC2 instances are deployed in a private subnet of a VPC that does not have internet access. The EC2 instances and the S3 buckets are in the same AWS account. The EC2 instances access the S3 buckets through an S3 gateway endpoint that has the default access policy. Each EC2 instance is associated with an instance profile role that has a policy that explicitly allows the s3:GetObject action and the s3:PutObject action for only the required S3 buckets. The company learns that one or more of the EC2 instances are compromised and are exfiltrating data to an S3 bucket that is outside the company's organization in AWS Organizations. A security engineer must implement a solution to stop this exfiltration of data and to keep the EC2 processing job functional. Which solution will meet these requirements?",
        "options": {
            "A": "Update the policy on the S3 gateway endpoint to allow the S3 actions only if the values of the aws:ResourceOrgID and aws:PrincipalOrgID condition keys match the company's values.",
            "B": "Update the policy on the instance profile role to allow the S3 actions only if the value of the aws:ResourceOrgID condition key matches the company's value.",
            "C": "Add a network ACL rule to the subnet of the EC2 instances to block outgoing connections on port 443.",
            "D": "Apply an SCP on the AWS account to allow the S3 actions only if the values of the aws:ResourceOrgID and aws:PrincipalOrgID condition keys match the company's values."
        },
        "correctAnswer": [
            "D"
        ]
    },
    {
        "type": "single",
        "question": "Question #56: A company that operates in a hybrid cloud environment must meet strict compliance requirements. The company wants to create a report that includes evidence from on-premises workloads alongside evidence from AWS resources. A security engineer must implement a solution to collect, review, and manage the evidence to demonstrate compliance with company policy. Which solution will meet these requirements?",
        "options": {
            "A": "Create an assessment in AWS Audit Manager from a prebuilt framework or a custom framework. Upload manual evidence from the onpremises workloads. Add the evidence to the assessment. Generate an assessment report after Audit Manager collects the necessary evidence from the AWS resources.",
            "B": "Install the Amazon CloudWatch agent on the on-premises workloads. Use AWS Config to deploy a conformance pack from a sample conformance pack template or a custom YAML template. Generate an assessment report after AWS Config identifies noncompliant workloads and resources.",
            "C": "Set up the appropriate security standard in AWS Security Hub. Upload manual evidence from the on-premises workloads. Wait for Security Hub to collect the evidence from the AWS resources. Download the list of controls as a .csv file.",
            "D": "Install the Amazon CloudWatch agent on the on-premises workloads. Create a CloudWatch dashboard to monitor the on-premises workloads and the AWS resources. Run a query on the workloads and resources. Download the results."
        },
        "correctAnswer": [
            "A"
        ],
        "explanation": "<p>For the requirement to collect, review, and manage compliance evidence from both on-premises workloads and AWS resources, and then generate a comprehensive compliance report, the most suitable solution is:</p><p>A. <strong>Create an assessment in AWS Audit Manager from a prebuilt framework or a custom framework. Upload manual evidence from the on-premises workloads. Add the evidence to the assessment. Generate an assessment report after Audit Manager collects the necessary evidence from the AWS resources.</strong></p><p>Here's why this option best meets the requirements:</p><p>- <strong>Comprehensive Compliance Management</strong>: AWS Audit Manager is designed specifically for compliance evidence collection, review, and management. It supports both automated evidence collection from AWS resources and manual evidence upload, making it ideal for hybrid cloud environments. - <strong>Framework Flexibility</strong>: Audit Manager allows you to create assessments based on prebuilt or custom frameworks, enabling you to tailor the compliance checks to the company's specific requirements. - <strong>Report Generation</strong>: Upon completing the evidence collection, Audit Manager can generate detailed compliance reports. These reports can serve as evidence for internal audits or external regulatory compliance reviews, showcasing adherence to company policies across both on-premises and cloud environments.</p><p>The other options are less suitable for the described requirements:</p><p>B. <strong>Install the Amazon CloudWatch agent on the on-premises workloads. Use AWS Config to deploy a conformance pack from a sample conformance pack template or a custom YAML template. Generate an assessment report after AWS Config identifies noncompliant workloads and resources.</strong>  - AWS Config and conformance packs are powerful tools for assessing, monitoring, and evaluating the configurations of your AWS resources against desired configurations. However, they do not natively support managing or uploading manual evidence from on-premises workloads for compliance reporting.</p><p>C. <strong>Set up the appropriate security standard in AWS Security Hub. Upload manual evidence from the on-premises workloads. Wait for Security Hub to collect the evidence from the AWS resources. Download the list of controls as a .csv file.</strong>  - AWS Security Hub provides a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. While it aggregates findings from various AWS services and supports certain compliance checks, it does not facilitate the manual upload of compliance evidence from on-premises workloads or the generation of detailed compliance assessment reports like Audit Manager does.</p><p>D. <strong>Install the Amazon CloudWatch agent on the on-premises workloads. Create a CloudWatch dashboard to monitor the on-premises workloads and the AWS resources. Run a query on the workloads and resources. Download the results.</strong>  - Amazon CloudWatch provides monitoring and observability for AWS resources and applications. While installing the CloudWatch agent on on-premises workloads and creating dashboards can offer valuable operational insights, CloudWatch is not focused on compliance evidence collection or report generation for compliance purposes.</p><p>Therefore, option A is the most appropriate and efficient solution for creating a compliance report that includes evidence from both on-premises and AWS environments, meeting the company's strict compliance requirements.</p>"
    },
    {
        "type": "single",
        "question": "Question #58: A company has a web server in the AWS Cloud. The company will store the content for the web server in an Amazon S3 bucket. A security engineer must use an Amazon CloudFront distribution to speed up delivery of the content. None of the files can be publicly accessible from the S3 bucket directly. Which solution will meet these requirements?",
        "options": {
            "A": "Configure the permissions on the individual files in the S3 bucket so that only the CloudFront distribution has access to them.",
            "B": "Create an origin access control (OAC). Associate the OAC with the CloudFront distribution. Configure the S3 bucket permissions so that only the OAC can access the files in the S3 bucket.",
            "C": "Create an S3 role in AWS Identity and Access Management (IAM). Allow only the CloudFront distribution to assume the role to access the files in the S3 bucket.",
            "D": "Create an S3 bucket policy that uses only the CloudFront distribution ID as the principal and the Amazon Resource Name (ARN) as the target."
        },
        "correctAnswer": [
            "B"
        ],
        "explanation": "<p>To ensure that the content stored in an Amazon S3 bucket is accessible only through an Amazon CloudFront distribution, and not publicly accessible directly from the S3 bucket, the best practice is to use an Origin Access Identity (OAI) or, with newer configurations, an Origin Access Control (OAC). OAC is the updated method to securely serve private content through CloudFront. Given the context, the correct solution is:</p><p>B. <strong>Create an origin access control (OAC). Associate the OAC with the CloudFront distribution. Configure the S3 bucket permissions so that only the OAC can access the files in the S3 bucket.</strong></p><p>This option meets the requirements by ensuring that only requests coming through the CloudFront distribution can access the content in the S3 bucket. The OAC is associated with the CloudFront distribution, and the S3 bucket permissions are configured to allow access exclusively to the OAC. This method prevents direct public access to the files in the S3 bucket while enabling CloudFront to cache and serve the content efficiently.</p><p>Let's examine why the other options are less suitable:</p><p>A. <strong>Configure the permissions on the individual files in the S3 bucket so that only the CloudFront distribution has access to them.</strong>  - While this could theoretically restrict access to the CloudFront distribution, managing permissions on individual files can be operationally cumbersome and does not leverage the CloudFront-specific mechanisms (like OAI or OAC) designed for this purpose.</p><p>C. <strong>Create an S3 role in AWS Identity and Access Management (IAM). Allow only the CloudFront distribution to assume the role to access the files in the S3 bucket.</strong>  - CloudFront distributions do not assume IAM roles in the way that AWS services like EC2 do. Instead, access between CloudFront and S3 is typically managed through OAIs or OACs, not by assuming IAM roles.</p><p>D. <strong>Create an S3 bucket policy that uses only the CloudFront distribution ID as the principal and the Amazon Resource Name (ARN) as the target.</strong>  - S3 bucket policies do not recognize CloudFront distribution IDs as principals. The correct way to restrict access is through the use of an OAI (for older configurations) or an OAC, which then is referenced in the S3 bucket policy to grant access to the CloudFront distribution.</p><p>Given the provided options, <strong>B</strong> is the most accurate and efficient solution for serving private content from an S3 bucket exclusively through a CloudFront distribution.</p>"
    },
    {
        "type": "single",
        "question": "Question #59: A security engineer logs in to the AWS Lambda console with administrator permissions. The security engineer is trying to view logs in Amazon CloudWatch for a Lambda function that is named myFunction. When the security engineer chooses the option in the Lambda console to view logs in CloudWatch, an 'error loading Log Streams' message appears. The IAM policy for the Lambda function's execution role contains the following: How should the security engineer correct the error?",
        "options": {
            "A": "Move the logs:CreateLogGroup action to the second Allow statement.",
            "B": "Add the logs:PutDestination action to the second Allow statement.",
            "C": "Add the logs:GetLogEvents action to the second Allow statement.",
            "D": "Add the logs:CreateLogStream action to the second Allow statement."
        },
        "correctAnswer": [
            "D"
        ],
        "explanation": "<p>The error &quot;loading Log Streams&quot; in the Amazon CloudWatch console suggests that the Lambda function's execution role does not have the necessary permissions to create log streams or to put log events in CloudWatch Logs.</p><p>From the provided IAM policy, the first statement grants permissions to create log groups in CloudWatch, and the second statement allows putting log events to a log stream within a specific log group. However, for the Lambda function to create log streams and put log events, it also requires permission to create log streams within the log group.</p><p>Therefore, the correct action to correct the error would be:</p><p>D. <strong>Add the logs:CreateLogStream action to the second Allow statement.</strong></p><p>This allows the Lambda function's execution role to create log streams in the specified log group, which is necessary for it to write logs to CloudWatch. Without this permission, the function cannot create new log streams, which is likely the reason behind the error when trying to view logs.</p><p>The logs:GetLogEvents permission would be necessary for reading the log events, which may also be relevant, but the immediate issue based on the error message is related to creating log streams, not retrieving the log events. Hence, option D is the most appropriate correction.</p><p>A. <strong>Move the logs:CreateLogGroup action to the second Allow statement.</strong>  - The &#96;logs:CreateLogGroup&#96; action is already correctly placed in the first statement and is not related to the error at hand. This permission is required to create new log groups, but since the error is related to loading log streams, the issue is not with creating log groups. Moreover, the IAM policy structure does not require all related actions to be in a single statement; they can be spread across multiple statements as long as the necessary permissions are granted.</p><p>B. <strong>Add the logs:PutDestination action to the second Allow statement.</strong>  - The &#96;logs:PutDestination&#96; action is used for creating a destination to which log data can be sent. This action is not relevant to the task of writing log events or creating log streams, which is what's needed for a Lambda function to send logs to CloudWatch. Thus, adding this permission will not resolve the issue of viewing log streams.</p><p>C. <strong>Add the logs:GetLogEvents action to the second Allow statement.</strong>  - The &#96;logs:GetLogEvents&#96; permission allows for the retrieval of log events from a specified log stream, which is useful for reading logs. However, the error in question is related to the inability to load log streams, not to read from them. While this permission is important for viewing logs, the policy already has permission to put log events, and the error message suggests an issue with creating or listing log streams rather than retrieving log event data.</p><p>D. <strong>Add the logs:CreateLogStream action to the second Allow statement.</strong> (Correct Answer)  - The &#96;logs:CreateLogStream&#96; permission is necessary to create log streams within the specified log group in CloudWatch Logs. If a Lambda function cannot create log streams, it cannot write logs to CloudWatch, leading to the error mentioned. This permission is directly related to the error and its addition should resolve the issue.</p><p>In summary, the other options do not address the specific error message related to loading log streams, whereas option D directly resolves the permission issue preventing the Lambda function from creating log streams in CloudWatch Logs.</p>"
    },
    {
        "type": "single",
        "question": "Question #60: A company has a new partnership with a vendor. The vendor will process data from the company's customers. The company will upload data files as objects into an Amazon S3 bucket. The vendor will download the objects to perform data processing. The objects will contain sensitive data. A security engineer must implement a solution that prevents objects from residing in the S3 bucket for longer than 72 hours. Which solution will meet these requirements?",
        "options": {
            "A": "Use Amazon Macie to scan the S3 bucket for sensitive data every 72 hours. Configure Macie to delete the objects that contain sensitive data when they are discovered.",
            "B": "Configure an S3 Lifecycle rule on the S3 bucket to expire objects that have been in the S3 bucket for 72 hours.",
            "C": "Create an Amazon EventBridge scheduled rule that invokes an AWS Lambda function every day. Program the Lambda function to remove any objects that have been in the S3 bucket for 72 hours.",
            "D": "Use the S3 Intelligent-Tiering storage class for all objects that are uploaded to the S3 bucket. Use S3 Intelligent-Tiering to expire objects that have been in the $3 bucket for 72 hours."
        },
        "correctAnswer": [
            "B"
        ],
        "explanation": "<p>The solution that meets the requirement to automatically delete objects from the S3 bucket after 72 hours is:</p><p>B. <strong>Configure an S3 Lifecycle rule on the S3 bucket to expire objects that have been in the S3 bucket for 72 hours.</strong></p><p>S3 Lifecycle policies are designed to manage objects and reduce costs by automatically transitioning them to different storage classes or deleting them after a certain period. By setting up a lifecycle rule to expire objects after 72 hours, you ensure that the data is automatically removed from the bucket, meeting the company's requirement.</p><p>Here's why the other options are not suitable:</p><p>A. <strog>Use Amazon Macie to scan the S3 bucket for sensitive data every 72 hours. Configure Macie to delete the objects that contain sensitive data when they are discovered.</strong>  - Amazon Macie is a security service that uses machine learning and pattern matching to discover and protect sensitive data in AWS. While Macie can identify sensitive data, it is not designed to automatically delete objects based on a time schedule.</p><p>C. <strong>Create an Amazon EventBridge scheduled rule that invokes an AWS Lambda function every day. Program the Lambda function to remove any objects that have been in the S3 bucket for 72 hours.</strong>  - Although this approach could work, it introduces unnecessary complexity and requires custom code maintenance. An S3 Lifecycle rule is a simpler and more direct solution that requires less operational overhead.</p><p>D. <strong>Use the S3 Intelligent-Tiering storage class for all objects that are uploaded to the S3 bucket. Use S3 Intelligent-Tiering to expire objects that have been in the $3 bucket for 72 hours.</strong>  - The S3 Intelligent-Tiering storage class is designed to optimize storage costs by automatically moving objects between two access tiers when access patterns change. However, it does not have a built-in mechanism for expiring objects based on a defined time period; this would still require a lifecycle policy.</p><p>Option B is the most straightforward and efficient solution to ensure that objects are deleted from the S3 bucket after 72 hours.</p>"
    },
    {
        "type": "multi",
        "question": "Question #61: A company accidentally deleted the private key for an Amazon Elastic Block Store (Amazon EBS)-backed Amazon EC2 instance. A security engineer needs to regain access to the instance. Which combination of steps will meet this requirement? (Choose two.)",
        "options": {
            "A": "Stop the instance. Detach the root volume. Generate a new key pair.",
            "B": "Keep the instance running. Detach the root volume. Generate a new key pair.",
            "C": "When the volume is detached from the original instance, attach the volume to another instance as a data volume. Modify the authorized_keys file with a new public key. Move the volume back to the original instance. Start the instance.",
            "D": "When the volume is detached from the original instance, attach the volume to another instance as a data volume. Modify the authorized_keys file with a new private key. Move the volume back to the original instance. Start the instance.",
            "E": "When the volume is detached from the original instance, attach the volume to another instance as a data volume. Modify the authorized_keys file with a new public key. Move the volume back to the original instance that is running."
        },
        "correctAnswer": [
            "A",
            "C"
        ],
        "explanation": "<p>To regain access to an EC2 instance after the private key is lost, you need to attach the instance's root volume to another instance and modify the &#96;authorized_keys&#96; file with a new public key. Here are the steps to accomplish this:</p><p>A. <strong>Stop the instance. Detach the root volume. Generate a new key pair.</strong>  - The first step is to stop the EC2 instance because the root volume (where the operating system is installed) cannot be detached while the instance is running. After stopping the instance, you can detach the root volume. Additionally, you need to generate a new key pair in AWS to regain SSH access to the instance. You will use the public key from this new key pair in the &#96;authorized_keys&#96; file.</p><p>C. <strong>When the volume is detached from the original instance, attach the volume to another instance as a data volume. Modify the &#96;authorized_keys&#96; file with a new public key. Move the volume back to the original instance. Start the instance.</strong>  - Once the root volume is detached, attach it to another instance as a secondary (data) volume. Then, access the file system of the attached volume and modify the &#96;.ssh/authorized_keys&#96; file within the root volume's file system to include the public key of the new key pair you've generated. After saving the changes, detach the volume from the helper instance and reattach it to the original instance as the root volume. Finally, start the original instance, and you should now be able to access it using the new key pair.</p><p>The reasons why the other options are incorrect:</p><p>B. <strong>Keep the instance running. Detach the root volume. Generate a new key pair.</strong>  - The root volume cannot be detached while the instance is running. Attempting to do so would result in an error.</p><p>D. <strong>When the volume is detached from the original instance, attach the volume to another instance as a data volume. Modify the &#96;authorized_keys&#96; file with a new private key. Move the volume back to the original instance. Start the instance.</strong>  - The &#96;authorized_keys&#96; file needs to contain the public key, not the private key. The private key is kept by the user to authenticate against the public key.</p><p>E. <strong>When the volume is detached from the original instance, attach the volume to another instance as a data volume. Modify the &#96;authorized_keys&#96; file with a new public key. Move the volume back to the original instance that is running.</strong>  - You cannot reattach the root volume to the original instance if it is still running. The instance needs to be stopped before you can detach or reattach the root volume.</p><p>Therefore, the correct combination of steps is A and C. These steps ensure that the root volume is safely detached and modified while the instance is stopped and then reattached to allow access with the new key pair.</p>"
    },
    {
        "type": "single",
        "question": "Question #62: A company purchased a subscription to a third-party cloud security scanning solution that integrates with AWS Security Hub. A security engineer needs to implement a solution that will remediate the findings from the third-party scanning solution automatically. Which solution will meet this requirement?",
        "options": {
            "A": "Set up an Amazon EventBridge rule that reacts to new Security Hub findings. Configure an AWS Lambda function as the target for the rule to remediate the findings.",
            "B": "Set up a custom action in Security Hub. Configure the custom action to call AWS Systems Manager Automation runbooks to remediate the findings.",
            "C": "Set up a custom action in Security Hub. Configure an AWS Lambda function as the target for the custom action to remediate the findings.",
            "D": "Set up AWS Config rules to use AWS Systems Manager Automation runbooks to remediate the findings."
        },
        "correctAnswer": [
            "A"
        ],
        "explanation": "<p>To implement a solution that will remediate the findings from the third-party scanning solution automatically and integrates with AWS Security Hub, the most appropriate approach would be:</p><p>A. <strong>Set up an Amazon EventBridge rule that reacts to new Security Hub findings. Configure an AWS Lambda function as the target for the rule to remediate the findings.</strong></p><p>EventBridge can be configured to trigger on events from AWS Security Hub, which would include findings from the integrated third-party cloud security scanning solution. When a new finding is detected, EventBridge can invoke a Lambda function that contains the logic to remediate the issue. This allows for an automated response to security findings without manual intervention.</p><p>Here's why the other options are less suitable:</p><p>B. <strong>Set up a custom action in Security Hub. Configure the custom action to call AWS Systems Manager Automation runbooks to remediate the findings.</strong>  - Custom actions in Security Hub are manually triggered by a user. They are not suitable for automatic remediation because they require human intervention to execute the action.</p><p>C. <strong>Set up a custom action in Security Hub. Configure an AWS Lambda function as the target for the custom action to remediate the findings.</strong>  - As with the previous option, custom actions are not automated. They are designed for situations where a Security Hub user needs to take an action on a finding manually.</p><p>D. <strong>Set up AWS Config rules to use AWS Systems Manager Automation runbooks to remediate the findings.</strong>  - AWS Config rules are used to evaluate the configuration of AWS resources and can trigger remediation actions if a resource is non-compliant. However, AWS Config is not directly integrated with AWS Security Hub for third-party findings remediation. It's more focused on compliance with AWS resource configurations rather than general security findings from a third-party solution.</p><p>Therefore, option A is the correct solution as it allows for automatic and immediate remediation of findings reported by the third-party solution to AWS Security Hub.</p>"
    },
    {
        "type": "single",
        "question": "Question #63: An application is running on an Amazon EC2 instance that has an IAM role attached. The IAM role provides access to an AWS Key Management Service (AWS KMS) customer managed key and an Amazon S3 bucket. The key is used to access 2 TB of sensitive data that is stored in the S3 bucket. A security engineer discovers a potential vulnerability on the EC2 instance that could result in the compromise of the sensitive data. Due to other critical operations, the security engineer cannot immediately shut down the EC2 instance for vulnerability patching. What is the FASTEST way to prevent the sensitive data from being exposed?",
        "options": {
            "A": "Download the data from the existing S3 bucket to a new EC2 instance. Then delete the data from the S3 bucket. Re-encrypt the data with a client-based key. Upload the data to a new S3 bucket.",
            "B": "Block access to the public range of S3 endpoint IP addresses by using a host-based firewall. Ensure that internet-bound traffic from the affected EC2 instance is routed through the host-based firewall.",
            "C": "Revoke the IAM role's active session permissions. Update the S3 bucket policy to deny access to the IAM role. Remove the IAM role from the EC2 instance profile.",
            "D": "Disable the current key. Create a new KMS key that the IAM role does not have access to, and re-encrypt all the data with the new key. Schedule the compromised key for deletion."
        },
        "correctAnswer": [
            "C"
        ]
    },
    {
        "type": "single",
        "question": "Question #64: A company is building an application on AWS that will store sensitive information. The company has a support team with access to the IT infrastructure, including databases. The company’s security engineer must introduce measures to protect the sensitive data against any data breach while minimizing management overhead. The credentials must be regularly rotated. What should the security engineer recommend?",
        "options": {
            "A": "Enable Amazon RDS encryption to encrypt the database and snapshots. Enable Amazon Elastic Block Store (Amazon EBS) encryption on Amazon EC2 instances. Include the database credential in the EC2 user data field. Use an AWS Lambda function to rotate database credentials. Set up TLS for the connection to the database.",
            "B": "Install a database on an Amazon EC2 instance. Enable third-party disk encryption to encrypt the Amazon Elastic Block Store (Amazon EBS) volume. Store the database credentials in AWS CloudHSM with automatic rotation. Set up TLS for the connection to the database.",
            "C": "Enable Amazon RDS encryption to encrypt the database and snapshots. Enable Amazon Elastic Black Store (Amazon EBS) encryption on Amazon EC2 instances. Store the database credentials in AWS Secrets Manager with automatic rotation. Set up TLS for the connection to the RDS hosted database.",
            "D": "Set up an AWS CloudHSM cluster with AWS Key Management Service (AWS KMS) to store KMS keys. Set up Amazon RDS encryption using AWS KMS to encrypt the database. Store database credentials in the AWS Systems Manager Parameter Store with automatic rotation. Set up TLS for the connection to the RDS hosted database."
        },
        "correctAnswer": [
            "C"
        ]
    },
    {
        "type": "single",
        "question": "Question #65: A company is using Amazon Route 53 Resolver for its hybrid DNS infrastructure. The company has set up Route 53 Resolver forwarding rules for authoritative domains that are hosted on on-premises DNS servers. A new security mandate requires the company to implement a solution to log and query DNS traffic that goes to the on-premises DNS servers. The logs must show details of the source IP address of the instance from which the query originated. The logs also must show the DNS name that was requested in Route 53 Resolver. Which solution will meet these requirements?",
        "options": {
            "A": "Use VPC Traffic Mirroring. Configure all relevant elastic network interfaces as the traffic source, include amazon-dns in the mirror filter, and set Amazon CloudWatch Logs as the mirror target. Use CloudWatch Insights on the mirror session logs to run queries on the source IP address and DNS name.",
            "B": "Configure VPC flow logs on all relevant VPCs. Send the logs to an Amazon S3 bucket. Use Amazon Athena to run SQL queries on the source IP address and DNS name.",
            "C": "Configure Route 53 Resolver query logging on all relevant VPCs. Send the logs to Amazon CloudWatch Logs. Use CloudWatch Insights to run queries on the source IP address and DNS name.",
            "D": "Modify the Route 53 Resolver rules on the authoritative domains that forward to the on-premises DNS servers. Send the logs to an Amazon S3 bucket. Use Amazon Athena to run SQL queries on the source IP address and DNS name."
        },
        "correctAnswer": [
            "C"
        ],
        "explanation": "<p>The solution that meets the requirements of logging DNS traffic going to the on-premises DNS servers, including the source IP address and the requested DNS name in Route 53 Resolver, is:</p><p>C. <strong>Configure Route 53 Resolver query logging on all relevant VPCs. Send the logs to Amazon CloudWatch Logs. Use CloudWatch Insights to run queries on the source IP address and DNS name.</strong></p><p>Route 53 Resolver query logging allows you to log all DNS queries that are made through the Resolver in the VPCs where logging is enabled. These logs include the source IP address and the DNS name queried, satisfying the company's mandate. The logs can be sent to CloudWatch Logs, which integrates with CloudWatch Insights for running detailed queries on the logged data.</p><p>Here's why the other options are less suitable:</p><p>A. <strong>Use VPC Traffic Mirroring. Configure all relevant elastic network interfaces as the traffic source, include amazon-dns in the mirror filter, and set Amazon CloudWatch Logs as the mirror target. Use CloudWatch Insights on the mirror session logs to run queries on the source IP address and DNS name.</strong>  - Traffic Mirroring captures a copy of network traffic but is typically used for more in-depth or specific network analysis tasks. It's not specifically designed for DNS query logging and would be more complex and resource-intensive for this use case compared to using Route 53 Resolver query logging.</p><p>B. <strong>Configure VPC flow logs on all relevant VPCs. Send the logs to an Amazon S3 bucket. Use Amazon Athena to run SQL queries on the source IP address and DNS name.</strong> - VPC flow logs provide visibility into network traffic that traverses the VPC, but they do not capture the contents of the traffic, such as the specific DNS queries being made. Therefore, they wouldn't provide the DNS name that was requested, which is a requirement.</p><p>D. <strong>Modify the Route 53 Resolver rules on the authoritative domains that forward to the on-premises DNS servers. Send the logs to an Amazon S3 bucket. Use Amazon Athena to run SQL queries on the source IP address and DNS name.</strong>  - Route 53 Resolver rules are used to define how DNS queries are routed, but they do not themselves generate logs. Logging is separately managed through Route 53 Resolver query logging.</p><p>Therefore, option C is the most effective and appropriate solution for logging DNS queries that meet the specified requirements.</p>"
    },
    {
        "type": "multi",
        "question": "Question #66: A security engineer is configuring account-based access control (ABAC) to allow only specific principals to put objects into an Amazon S3 bucket. The principals already have access to Amazon S3. The security engineer needs to configure a bucket policy that allows principals to put objects into the S3 bucket only if the value of the Team tag on the object matches the value of the Team tag that is associated with the principal. During testing, the security engineer notices that a principal can still put objects into the S3 bucket when the tag values do not match. Which combination of factors are causing the PutObject operation to succeed when the tag values are different? (Choose two.)",
        "options": {
            "A": "The principal's identity-based policy grants access to put objects into the S3 bucket with no conditions.",
            "B": "The principal's identity-based policy overrides the condition because the identity-based policy contains an explicit allow.",
            "C": "The S3 bucket's resource policy does not deny access to put objects.",
            "D": "The S3 bucket's resource policy cannot allow actions to the principal.",
            "E": "The bucket policy does not apply to principals in the same zone of trust."
        },
        "correctAnswer": [
            "A",
            "C"
        ],
        "explanation":"<p>In AWS, if a principal is able to perform an action that should be restricted by a condition in the bucket policy, it is often due to the combination of overly permissive identity-based policies and the absence of an explicit deny in the resource-based policy (in this case, the S3 bucket policy). Let's explore the options provided:</p><p>A. <strong>The principal's identity-based policy grants access to put objects into the S3 bucket with no conditions.</strong>  - If the principal has an identity-based policy that allows them to put objects into the S3 bucket without any conditions, they will be able to perform the action regardless of the tags. In AWS, an allow in an identity-based policy will enable the principal to execute the action unless there is an explicit deny that overrides it.</p><p>C. <strong>The S3 bucket's resource policy does not deny access to put objects.</strong>  - A resource policy that does not explicitly deny an action can result in the action being allowed if the principal's identity-based policy also allows it. Even if the bucket policy has a condition that checks the object's &#96;Team&#96; tag against the principal's &#96;Team&#96; tag, the lack of an explicit deny for non-matching tags means that the condition alone will not prevent access.</p><p>The other options are either incorrect or not relevant in this context:</p><p>B. <strong>The principal's identity-based policy overrides the condition because the identity-based policy contains an explicit allow.</strong>  - Identity-based policies do not override conditions in bucket policies; instead, AWS evaluates all permissions and denies the request only if there is an explicit deny. Since there is no such thing as &quot;override&quot; in this context, this option is not correct.</p><p>D. <strong>The S3 bucket's resource policy cannot allow actions to the principal.</strong>  - A bucket policy can definitely specify permissions for principals. This option does not contribute to the problem described.</p><p>E. <strong>The bucket policy does not apply to principals in the same zone of trust.</strong>  - The concept of a &quot;zone of trust&quot; is not a standard AWS term or feature related to S3 or IAM policies. ABAC and the evaluation of policies apply to all principals as specified, regardless of any trust zone concept.</p><p>Therefore, the combination of factors A and C are the most likely causes for the &#96;PutObject&#96; operation to succeed when the tag values do not match. The principal's identity-based policy allows the action without conditions (A), and the S3 bucket's resource policy does not contain an explicit deny to counteract this allow (C). To rectify this, the security engineer would need to ensure that the bucket policy contains an explicit deny for &#96;PutObject&#96; actions where the tag values do not match, and also review the principal's identity-based policies to ensure they do not grant unrestricted access to &#96;PutObject&#96; actions.</p>"
    }
]