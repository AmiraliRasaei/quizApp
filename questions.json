[
    {
        "type": "single",
        "question": "Question #67: A company is hosting multiple applications within a single VPC in its AWS account. The applications are running behind an Application Load Balancer that is associated with an AWS WAF web ACL. The company's security team has identified that multiple port scans are originating from a specific range of IP addresses on the internet. A security engineer needs to deny access from the offending IP addresses. Which solution will meet these requirements?",
        "options": {
            "A": "Modify the AWS WAF web ACL with an IP set match rule statement to deny incoming requests from the IP address range.",
            "B": "Add a rule to all security groups to deny the incoming requests from the IP address range.",
            "C": "Modify the AWS WAF web ACL with a rate-based rule statement to deny the incoming requests from the IP address range.",
            "D": "Configure the AWS WAF web ACL with regex match conditions. Specify a pattern set to deny the incoming requests based on the match condition."
        },
        "correctAnswer": "A"
    },
    {
        "type": "multi",
        "question": "Question #68: A company has contracted with a third party to audit several AWS accounts. To enable the audit, cross-account IAM roles have been created in each account targeted for audit. The auditor is having trouble accessing some of the accounts. Which of the following may be causing this problem? (Choose three.)",
        "options": {
            "A": "The external ID used by the auditor is missing or incorrect.",
            "B": "The auditor is using the incorrect password.",
            "C": "The auditor has not been granted sts:AssumeRole for the role in the destination account.",
            "D": "The Amazon EC2 role used by the auditor must be set to the destination account role.",
            "E": "The secret key used by the auditor is missing or incorrect.",
            "F": "The role ARN used by the auditor is missing or incorrect."
        },
        "correctAnswer": [
            "A",
            "C",
            "F"
        ]
    },
    {
        "type": "multi",
        "question": "Question #70: A company has a group of Amazon EC2 instances in a single private subnet of a VPC with no internet gateway attached. A security engineer has installed the Amazon CloudWatch agent on all instances in that subnet to capture logs from a specific application. To ensure that the logs flow securely, the company's networking team has created VPC endpoints for CloudWatch monitoring and CloudWatch logs. The networking team has attached the endpoints to the VPC. The application is generating logs. However, when the security engineer queries CloudWatch, the logs do not appear. Which combination of steps should the security engineer take to troubleshoot this issue? (Choose three.)",
        "options": {
            "A": "Ensure that the EC2 instance profile that is attached to the EC2 instances has permissions to create log streams and write logs.",
            "B": "Create a metric filter on the logs so that they can be viewed in the AWS Management Console.",
            "C": "Check the CloudWatch agent configuration file on each EC2 instance to make sure that the CloudWatch agent is collecting the proper log files.",
            "D": "Check the VPC endpoint policies of both VPC endpoints to ensure that the EC2 instances have permissions to use them.",
            "E": "Create a NAT gateway in the subnet so that the EC2 instances can communicate with CloudWatch.",
            "F": "Ensure that the security groups allow all the EC2 instances to communicate with each other to aggregate logs before sending."
        },
        "correctAnswer": [
            "A",
            "C",
            "D"
        ],
        "explanation": "<p>To troubleshoot the issue of logs not appearing in CloudWatch despite the setup of VPC endpoints for CloudWatch monitoring and CloudWatch logs in a subnet with no internet gateway, the security engineer should consider the following steps:</p><p>1. <strong>Ensure that the EC2 instance profile attached to the EC2 instances has permissions to create log streams and write logs (Option A).</strong> This is crucial because the CloudWatch agent needs appropriate permissions to send logs to CloudWatch. The instance profile (IAM role) associated with the EC2 instances must have policies granting permissions to CloudWatch services to create log streams and put log events.</p><p>2. <strong>Check the CloudWatch agent configuration file on each EC2 instance to make sure that the CloudWatch agent is collecting the proper log files (Option C).</strong> The agent's configuration file specifies which log files to monitor and send to CloudWatch. It's possible that the configuration file does not correctly specify the paths to the log files generated by the application or the necessary details about the log group and log stream.</p><p>3. <strong>Check the VPC endpoint policies of both VPC endpoints to ensure that the EC2 instances have permissions to use them (Option D).</strong> VPC endpoint policies can restrict the actions that can be performed or the resources that can be accessed through the endpoint. Ensuring that the endpoint policies explicitly allow the actions required by the CloudWatch agent to send logs is important for the logs to flow through the endpoints to CloudWatch.</p><p>The other options, while relevant to different contexts, are not directly applicable to this scenario:</p><p>- <strong>Creating a metric filter on the logs (Option B)</strong> is not a troubleshooting step for ensuring logs reach CloudWatch; metric filters are used to analyze and act on the log data once it's in CloudWatch. - <strong>Creating a NAT gateway in the subnet (Option E)</strong> is not necessary in this case because the VPC endpoints are meant to provide private access to AWS services without needing internet access or a NAT gateway. - <strong>Ensuring that the security groups allow all the EC2 instances to communicate with each other to aggregate logs before sending (Option F)</strong> is not relevant to this problem. The CloudWatch agent sends logs directly to CloudWatch from each instance, and inter-instance communication is not required for this process.</p>"
    },
    {
        "type": "single",
        "question": "Question #71: A company uses AWS Signer with all of the company's AWS Lambda functions. A developer recently stopped working for the company. The company wants to ensure that all the code that the developer wrote can no longer be deployed to the Lambda functions. Which solution will meet this requirement?",
        "options": {
            "A": "Revoke all versions of the signing profile assigned to the developer.",
            "B": "Examine the developer's IAM roles. Remove all permissions that grant access to Signer.",
            "C": "Re-encrypt all source code with a new AWS Key Management Service (AWS KMS) key.",
            "D": "Use Amazon CodeGuru to profile all the code that the Lambda functions use."
        },
        "correctAnswer": "A"
    },
    {
        "type": "single",
        "question": "Question #72: A company plans to use AWS Key Management Service (AWS KMS) to implement an encryption strategy to protect data at rest. The company requires client-side encryption for company projects. The company is currently conducting multiple projects to test the company's use of AWS KMS. These tests have led to a sudden increase in the company's AWS resource consumption. The test projects include applications that issue multiple requests each second to KMS endpoints for encryption activities. The company needs to develop a solution that does not throttle the company's ability to use AWS KMS. The solution must improve key usage for client-side encryption and must be cost optimized. Which solution will meet these requirements?",
        "options": {
            "A": "Use keyrings with the AWS Encryption SDK. Use each keyring individually or combine keyrings into a multi-keyring. Decrypt the data by using a keyring that has the primary key in the multi-keyring.",
            "B": "Use data key caching. Use the local cache that the AWS Encryption SDK provides with a caching cryptographic materials manager.",
            "C": "Use KMS key rotation. Use a local cache in the AWS Encryption SDK with a caching cryptographic materials manager.",
            "D": "Use keyrings with the AWS Encryption SDK. Use each keyring individually or combine keyrings into a multi-keyring. Use any of the wrapping keys in the multi-keyring to decrypt the data."
        },
        "correctAnswer": "B"
    },
    {
        "type": "single",
        "question": "Question #73: A security team is working on a solution that will use Amazon EventBridge to monitor new Amazon S3 objects. The solution will monitor for public access and for changes to any S3 bucket policy or setting that result in public access. The security team configures EventBridge to watch for specific API calls that are logged from AWS CloudTrail. EventBridge has an action to send an email notification through Amazon Simple Notification Service (Amazon SNS) to the security team immediately with details of the API call. Specifically, the security team wants EventBridge to watch for the s3:PutObjectAcl, s3:DeleteBucketPolicy, and s3:PutBucketPolicy API invocation logs from CloudTrail. While developing the solution in a single account, the security team discovers that the s3:PutObjectAcl API call does not invoke an EventBridge event. However, the s3:DeleteBucketPolicy API call and the s3:PutBucketPolicy API call do invoke an event. The security team has enabled CloudTrail for AWS management events with a basic configuration in the AWS Region in which EventBridge is being tested. Verification of the EventBridge event pattern indicates that the pattern is set up correctly. The security team must implement a solution so that the s3:PutObjectAcl API call will invoke an EventBridge event. The solution must not generate false notifications. Which solution will meet these requirements?",
        "options": {
            "A": "Modify the EventBridge event pattern by selecting Amazon S3. Select All Events as the event type.",
            "B": "Modify the EventBridge event pattern by selecting Amazon S3. Select Bucket Level Operations as the event type.",
            "C": "Enable CloudTrail Insights to identify unusual API activity.",
            "D": "Enable CloudTrail to monitor data events for read and write operations to S3 buckets."
        },
        "correctAnswer": "D",
        "explanation":"<p>The correct solution to ensure that the &#96;s3:PutObjectAcl&#96; API call will invoke an EventBridge event, without generating false notifications, is:</p><p><strong>D. Enable CloudTrail to monitor data events for read and write operations to S3 buckets.</strong></p><p>Explanation:</p><p>- <strong>s3:PutObjectAcl</strong> is considered a data event in AWS CloudTrail. AWS CloudTrail management events track actions taken on AWS resources, while data events track operations performed on or within the resource itself, such as S3 object-level activities. By default, CloudTrail logs management events but not data events. To capture &#96;s3:PutObjectAcl&#96; actions, you must explicitly enable data event logging for S3 buckets in CloudTrail.</p><p>The other options are not suitable because:</p><p>- <strong>A. Modifying the EventBridge event pattern by selecting Amazon S3 and selecting All Events as the event type</strong> would not specifically enable logging of &#96;s3:PutObjectAcl&#96; actions if data events are not enabled in CloudTrail. This approach might increase the scope of captured events beyond what is necessary and could still miss &#96;s3:PutObjectAcl&#96; if data events are not logged.</p><p>- <strong>B. Modifying the EventBridge event pattern by selecting Amazon S3 and selecting Bucket Level Operations as the event type</strong> is also not directly addressing the issue. While it's crucial to have the correct event pattern, without enabling data events for S3 in CloudTrail, &#96;s3:PutObjectAcl&#96; operations won't be logged or captured by EventBridge regardless of the event pattern.</p><p>- <strong>C. Enabling CloudTrail Insights to identify unusual API activity</strong> is designed to detect unusual operational activity, not to log specific API calls like &#96;s3:PutObjectAcl&#96;. While it's useful for identifying patterns that might indicate potential security issues, it does not substitute for logging specific data events required to monitor &#96;s3:PutObjectAcl&#96; actions.</p><p>Therefore, enabling CloudTrail to monitor data events for S3 buckets is necessary to capture &#96;s3:PutObjectAcl&#96; API calls and allow EventBridge to trigger events based on these actions, fulfilling the security team's requirement without generating false notifications.</p>"
    },
    {
        "type": "single",
        "question": "Question #74: A company uses Amazon GuardDuty. The company's security team wants all High severity findings to automatically generate a ticket in a third-party ticketing system through email integration. Which solution will meet this requirement?",
        "options": {
            "A": "Create a verified identity for the third-party ticketing email system in Amazon Simple Email Service (Amazon SES). Create an Amazon EventBridge rule that includes an event pattern that matches High severity GuardDuty findings. Specify the SES identity as the target for the EventBridge rule.",
            "B": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the third-party ticketing email system to the SNS topic. Create an Amazon EventBridge rule that includes an event pattern that matches High severity GuardDuty findings. Specify the SNS topic as the target for the EventBridge rule.",
            "C": "Use the GuardDuty CreateFilter API operation to build a filter in GuardDuty to monitor for High severity findings. Export the results of the filter to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the third-party ticketing email system to the SNS topic.",
            "D": "Use the GuardDuty CreateFilter API operation to build a filter in GuardDuty to monitor for High severity findings. Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the third-party ticketing email system to the SNS topic. Create an Amazon EventBridge rule that includes an event pattern that matches GuardDuty findings that are selected by the filter. Specify the SNS topic as the target for the EventBridge rule."
        },
        "correctAnswer": "B",
        "explanation":"<p>The solution that will meet the requirement to automatically generate a ticket in a third-party ticketing system through email integration for High severity findings from Amazon GuardDuty is:</p><p><strong>B. Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the third-party ticketing email system to the SNS topic. Create an Amazon EventBridge rule that includes an event pattern that matches High severity GuardDuty findings. Specify the SNS topic as the target for the EventBridge rule.</strong></p><p>Explanation:</p><p>- <strong>Amazon SNS</strong> is a flexible, fully managed pub/sub messaging and notifications service for coordinating the delivery of messages to subscribing endpoints and clients. By subscribing an email address (belonging to the third-party ticketing system) to an SNS topic, you can forward notifications to that email automatically when messages are published to the topic.</p><p>- <strong>Amazon EventBridge</strong> is a serverless event bus that makes it easy to connect applications together using data from your own applications, integrated Software-as-a-Service (SaaS) applications, and AWS services. EventBridge can be used to route detailed events from AWS services like GuardDuty to various targets, such as AWS Lambda, Amazon SNS topics, and more, based on the content of the events.</p><p>- This solution leverages EventBridge to catch events from GuardDuty, specifically filtering for High severity findings using an event pattern. When such an event occurs, EventBridge forwards the event to the specified SNS topic, which in turn sends an email notification to the subscribed third-party ticketing system's email address, thereby generating a ticket.</p><p>The other options are less suitable because:</p><p>- <strong>A.</strong> While you can use Amazon SES for direct email sending, it's more complex for this use case since SES is primarily for email sending, not for directly integrating with third-party systems based on event triggers from AWS services like GuardDuty. EventBridge directly to SNS is a more straightforward and efficient approach.</p><p>- <strong>C. and D.</strong> The GuardDuty CreateFilter API operation and the detailed integration process described are unnecessary for the task. EventBridge itself can directly filter GuardDuty findings based on severity, making these steps overly complicated for achieving the desired outcome. Plus, the direct use of CreateFilter and additional EventBridge rules as described in option D adds complexity without providing additional benefits for this specific requirement.</p>"
    },
    {
        "type": "single",
        "question": "Question #75: A company is using AWS Organizations to implement a multi-account strategy. The company does not have on-premises infrastructure. All workloads run on AWS. The company currently has eight member accounts. The company anticipates that it will have no more than 20 AWS accounts total at any time. The company issues a new security policy that contains the following requirements: No AWS account should use a VPC within the AWS account for workloads. The company should use a centrally managed VPC that all AWS accounts can access to launch workloads in subnets. No AWS account should be able to modify another AWS account's application resources within the centrally managed VPC. The centrally managed VPC should reside in an existing AWS account that is named Account-A within an organization. The company uses an AWS CloudFormation template to create a VPC that contains multiple subnets in Account-A. This template exports the subnet IDs through the CloudFormation Outputs section. Which solution will complete the security setup to meet these requirements?",
        "options": {
            "A": "Use a CloudFormation template in the member accounts to launch workloads. Configure the template to use the Fn::ImportValue function to obtain the subnet ID values.",
            "B": "Use a transit gateway in the VPC within Account-A. Configure the member accounts to use the transit gateway to access the subnets in Account-A to launch workloads.",
            "C": "Use AWS Resource Access Manager (AWS RAM) to share Account-A's VPC subnets with the remaining member accounts. Configure the member accounts to use the shared subnets to launch workloads.",
            "D": "Create a peering connection between Account-A and the remaining member accounts. Configure the member accounts to use the subnets in Account-A through the VPC peering connection to launch workloads."
        },
        "correctAnswer": "C",
        "explanation":"<p>A company is using AWS Organizations to implement a multi-account strategy. The company does not have on-premises infrastructure. All workloads run on AWS. The company currently has eight member accounts. The company anticipates that it will have no more than 20 AWS accounts total at any time.</p><p>The company issues a new security policy that contains the following requirements:</p><p>&bull; No AWS account should use a VPC within the AWS account for workloads. &bull; The company should use a centrally managed VPC that all AWS accounts can access to launch workloads in subnets. &bull; No AWS account should be able to modify another AWS account's application resources within the centrally managed VPC. &bull; The centrally managed VPC should reside in an existing AWS account that is named Ac-count-A within an organization.</p><p>The company uses an AWS CloudFormation template to create a VPC that contains multiple subnets in Account-A. This template exports the subnet IDs through the CloudFormation Outputs section.</p><p>Which solution will complete the security setup to meet these requirements?</p><p>A. Use a CloudFormation template in the member accounts to launch workloads. Configure the template to use the Fn::ImportValue function to obtain the subnet ID values. B. Use a transit gateway in the VPC within Account-A. Configure the member accounts to use the transit gateway to access the subnets in Account-A to launch workloads. C. Use AWS Resource Access Manager (AWS RAM) to share Account-A's VPC subnets with the remaining member accounts. Configure the member accounts to use the shared subnets to launch workloads. D. Create a peering connection between Account-A and the remaining member accounts. Configure the member accounts to use the subnets in Account-A through the VPC peering connection to launch workloads.</p>"
    },
    {
        "type": "single",
        "question": "Question #76: A company's security team needs to receive a notification whenever an AWS access key has not been rotated in 90 or more days. A security engineer must develop a solution that provides these notifications automatically. Which solution will meet these requirements with the LEAST amount of effort?",
        "options": {
            "A": "Deploy an AWS Config managed rule to run on a periodic basis of 24 hours. Select the access-keys-rotated managed rule, and set the maxAccessKeyAge parameter to 90 days. Create an Amazon EventBridge rule with an event pattern that matches the compliance type of NON_COMPLIANT from AWS Config for the managed rule. Configure EventBridge to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.",
            "B": "Create a script to export a .csv file from the AWS Trusted Advisor check for IAM access key rotation. Load the script into an AWS Lambda function that will upload the .csv file to an Amazon S3 bucket. Create an Amazon Athena table query that runs when the .csv file is uploaded to the S3 bucket. Publish the results for any keys older than 90 days by using an invocation of an Amazon Simple Notification Service (Amazon SNS) notification to the security team.",
            "C": "Create a script to download the IAM credentials report on a periodic basis. Load the script into an AWS Lambda function that will run on a schedule through Amazon EventBridge. Configure the Lambda script to load the report into memory and to filter the report for records in which the key was last rotated at least 90 days ago. If any records are detected, send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.",
            "D": "Create an AWS Lambda function that queries the IAM API to list all the users. Iterate through the users by using the ListAccessKeys operation. Verify that the value in the CreateDate field is not at least 90 days old. Send an Amazon Simple Notification Service (Amazon SNS) notification to the security team if the value is at least 90 days old. Create an Amazon EventBridge rule to schedule the Lambda function to run each day."
        },
        "correctAnswer": "A"
    },
    {
        "type": "single",
        "question": "Question #77: A company maintains an open-source application that is hosted on a public GitHub repository. While creating a new commit to the repository, an engineer uploaded their AWS access key and secret access key. The engineer reported the mistake to a manager, and the manager immediately disabled the access key. The company needs to assess the impact of the exposed access key. A security engineer must recommend a solution that requires the least possible managerial overhead. Which solution meets these requirements?",
        "options": {
            "A": "Analyze an AWS Identity and Access Management (IAM) use report from AWS Trusted Advisor to see when the access key was last used.",
            "B": "Analyze Amazon CloudWatch Logs for activity by searching for the access key.",
            "C": "Analyze VPC flow logs for activity by searching for the access key.",
            "D": "Analyze a credential report in AWS Identity and Access Management (IAM) to see when the access key was last used."
        },
        "correctAnswer": "D"
    },
    {
        "type": "single",
        "question": "Question #78: A company plans to create individual child accounts within an existing organization in AWS Organizations for each of its DevOps teams. AWS CloudTrail has been enabled and configured on all accounts to write audit logs to an Amazon S3 bucket in a centralized AWS account. A security engineer needs to ensure that DevOps team members are unable to modify or disable this configuration. How can the security engineer meet these requirements?",
        "options": {
            "A": "Create an IAM policy that prohibits changes to the specific CloudTrail trail and apply the policy to the AWS account root user.",
            "B": "Create an S3 bucket policy in the specified destination account for the CloudTrail trail that prohibits configuration changes from the AWS account root user in the source account.",
            "C": "Create an SCP that prohibits changes to the specific CloudTrail trail and apply the SCP to the appropriate organizational unit or account in Organizations.",
            "D": "Create an IAM policy that prohibits changes to the specific CloudTrail trail and apply the policy to a new IAM group. Have team members use individual IAM accounts that are members of the new IAM group."
        },
        "correctAnswer": "C"
    },
    {
        "type": "single",
        "question": "Question #79: A company's policy requires that all API keys be encrypted and stored separately from source code in a centralized security account. This security account is managed by the company's security team. However, an audit revealed that an API key is stored with the source code of an AWS Lambda function in an AWS CodeCommit repository in the DevOps account. How should the security team securely store the API key?",
        "options": {
            "A": "Create a CodeCommit repository in the security account using AWS Key Management Service (AWS KMS) for encryption. Require the development team to migrate the Lambda source code to this repository.",
            "B": "Store the API key in an Amazon S3 bucket in the security account using server-side encryption with Amazon S3 managed encryption keys (SSE-S3) to encrypt the key. Create a presigned URL for the S3 key, and specify the URL in a Lambda environmental variable in the AWS CloudFormation template. Update the Lambda function code to retrieve the key using the URL and call the API.",
            "C": "Create a secret in AWS Secrets Manager in the security account to store the API key using AWS Key Management Service (AWS KMS) for encryption. Grant access to the IAM role used by the Lambda function so that the function can retrieve the key from Secrets Manager and call the API.",
            "D": "Create an encrypted environment variable for the Lambda function to store the API key using AWS Key Management Service (AWS KMS) for encryption. Grant access to the IAM role used by the Lambda function so that the function can decrypt the key at runtime."
        },
        "correctAnswer": "C"
    },
    {
        "type": "single",
        "question": "Question #80: A security engineer is asked to update an AWS CloudTrail log file prefix for an existing trail. When attempting to save the change in the CloudTrail console, the security engineer receives the following error message: \"There is a problem with the bucket policy.\" What will enable the security engineer to save the change?",
        "options": {
            "A": "Create a new trail with the updated log file prefix, and then delete the original trail. Update the existing bucket policy in the Amazon S3 console with the new log file prefix, and then update the log file prefix in the CloudTrail console.",
            "B": "Update the existing bucket policy in the Amazon S3 console to allow the security engineer's principal to perform PutBucketPolicy, and then update the log file prefix in the CloudTrail console.",
            "C": "Update the existing bucket policy in the Amazon S3 console with the new log file prefix, and then update the log file prefix in the CloudTrail console.",
            "D": "Update the existing bucket policy in the Amazon S3 console to allow the security engineer's principal to perform GetBucketPolicy, and then update the log file prefix in the CloudTrail console."
        },
        "correctAnswer": "C"
    },
    {
        "type": "single",
        "question": "Question #81: A company uses AWS Organizations. The company wants to implement short-term credentials for third-party AWS accounts to use to access accounts within the company's organization. Access is for the AWS Management Console and third-party software-as-a-service (SaaS) applications. Trust must be enhanced to prevent two external accounts from using the same credentials. The solution must require the least possible operational effort. Which solution will meet these requirements?",
        "options": {
            "A": "Use a bearer token authentication with OAuth or SAML to manage and share a central Amazon Cognito user pool across multiple Amazon API Gateway APIs.",
            "B": "Implement AWS IAM Identity Center (AWS Single Sign-On), and use an identity source of choice. Grant access to users and groups from other accounts by using permission sets that are assigned by account.",
            "C": "Create a unique IAM role for each external account. Create a trust policy Use AWS Secrets Manager to create a random external key.",
            "D": "Create a unique IAM role for each external account. Create a trust policy that includes a condition that uses the sts:ExternalId condition key."
        },
        "correctAnswer": "D",
        "explanation": "<p>The solution that will meet the company's requirements for implementing short-term credentials for third-party AWS accounts to access accounts within the company's organization, enhancing trust, and requiring the least possible operational effort is:</p><p><strong>D. Create a unique IAM role for each external account. Create a trust policy that includes a condition that uses the sts:ExternalId condition key.</strong></p><p>Explanation:</p><p>- <strong>IAM Roles and Trust Relationships</strong>: IAM roles allow you to delegate access with defined permissions to entities (users, services, or accounts) without having to share long-term access keys. By creating a unique IAM role for each external account, you can specify the permissions that the external account has within your AWS environment. This meets the requirement for access by third-party AWS accounts.</p><p>- <strong>sts:ExternalId Condition Key</strong>: The &#96;sts:ExternalId&#96; is a condition key in IAM policies that helps to prevent the &quot;confused deputy&quot; problem, where a malicious entity could trick a service into misusing its permissions. The &#96;ExternalId&#96; can be used as a secret between the account that owns the role and the account that is allowed to assume the role, ensuring that only the intended external account can assume the role. This enhances trust by preventing two external accounts from using the same credentials.</p><p>- <strong>Least Operational Effort</strong>: This solution is straightforward to implement and manage, especially when compared to managing individual access keys or implementing complex federated authentication systems. It leverages AWS's built-in capabilities for secure access management.</p><p>The other options are less suitable because:</p><p>- <strong>A. Bearer Token Authentication with OAuth or SAML and Amazon Cognito</strong>: While this solution can manage authentication and integrate with external identity providers, it is more complex and geared towards application-level authentication rather than providing short-term AWS console or API access for third-party accounts. It also does not inherently prevent two external accounts from using the same credentials without additional custom implementation.</p><p>- <strong>B. AWS IAM Identity Center (AWS Single Sign-On)</strong>: While AWS IAM Identity Center simplifies access management for AWS accounts and applications, it is primarily used for managing identities within an organization rather than granting access to third-party AWS accounts. It requires an identity source (like a corporate directory) that third-party accounts might not be part of.</p><p>- <strong>C. IAM Role with AWS Secrets Manager</strong>: Using AWS Secrets Manager to create a random external key is unnecessary for the scenario described and adds operational complexity. Secrets Manager is typically used for managing secrets needed by applications or services, not for managing access to AWS accounts.</p><p>Therefore, option D is the most appropriate solution, as it directly addresses the need for secure, short-term access for third-party accounts with minimal operational complexity.</p>"
    },
    {
        "type": "single",
        "question": "Question #82: A company is evaluating its security posture. In the past, the company has observed issues with specific hosts and host header combinations that affected the company's business. The company has configured AWS WAF web ACLs as an initial step to mitigate these issues. The company must create a log analysis solution for the AWS WAF web ACLs to monitor problematic activity. The company wants to process all the AWS WAF logs in a central location. The company must have the ability to filter out requests based on specific hosts. A security engineer starts to enable access logging for the AWS WAF web ACLs. What should the security engineer do next to meet these requirements with the MOST operational efficiency?",
        "options": {
            "A": "Specify Amazon Redshift as the destination for the access logs. Deploy the Amazon Athena Redshift connector. Use Athena to query the data from Amazon Redshift and to filter the logs by host.",
            "B": "Specify Amazon CloudWatch as the destination for the access logs. Use Amazon CloudWatch Logs Insights to design a query to filter the logs by host.",
            "C": "Specify Amazon CloudWatch as the destination for the access logs. Export the CloudWatch logs to an Amazon S3 bucket. Use Amazon Athena to query the logs and to filter the logs by host.",
            "D": "Specify Amazon CloudWatch as the destination for the access logs. Use Amazon Redshift Spectrum to query the logs and to filter the logs by host."
        },
        "correctAnswer": "B"
    },
    {
        "type": "multi",
        "question": "Question #83: A security engineer is trying to use Amazon EC2 Image Builder to create an image of an EC2 instance. The security engineer has configured the pipeline to send logs to an Amazon S3 bucket. When the security engineer runs the pipeline, the build fails with the following error: \"AccessDenied: Access Denied status code: 403\". The security engineer must resolve the error by implementing a solution that complies with best practices for least privilege access. Which combination of steps will meet these requirements? (Choose two.)",
        "options": {
            "A": "Ensure that the following policies are attached to the IAM role that the security engineer is using: EC2InstanceProfileForImageBuilder, EC2InstanceProfileForImageBuilderECRContainerBuilds, and AmazonSSMManagedInstanceCore.",
            "B": "Ensure that the following policies are attached to the instance profile for the EC2 instance: EC2InstanceProfileForImageBuilder, EC2InstanceProfileForImageBuilderECRContainerBuilds, and AmazonSSMManagedInstanceCore.",
            "C": "Ensure that the AWSImageBuilderFullAccess policy is attached to the instance profile for the EC2 instance.",
            "D": "Ensure that the security engineer's IAM role has the s3:PutObject permission for the S3 bucket.",
            "E": "Ensure that the instance profile for the EC2 instance has the s3:PutObject permission for the S3 bucket."
        },
        "correctAnswer": [
            "B",
            "E"
        ],
        "explanation":"<p>To resolve the &quot;AccessDenied: Access Denied status code: 403&quot; error encountered by the security engineer when using Amazon EC2 Image Builder and ensure compliance with the best practices for least privilege access, the correct combination of steps includes:</p><p>1. <strong>B. Ensure that the following policies are attached to the instance profile for the EC2 instance: EC2InstanceProfileForImageBuilder, EC2InstanceProfileForImageBuilderECRContainerBuilds, and AmazonSSMManagedInstanceCore.</strong>    This step ensures that the EC2 instance being used by Amazon EC2 Image Builder has the necessary permissions to perform image building tasks, including interactions with Amazon ECR for container builds and Amazon SSM for managed instance operations. These permissions are crucial for the image building process and for logging activities to succeed.</p><p>2. <strong>E. Ensure that the instance profile for the EC2 instance has the s3:PutObject permission for the S3 bucket.</strong></p><p> Since the build process involves sending logs to an Amazon S3 bucket, it's essential that the EC2 instance (through its instance profile) has permissions to write to the specified S3 bucket. The &#96;s3:PutObject&#96; permission specifically allows the instance to upload logs to the bucket, addressing the &quot;Access Denied&quot; error encountered during the build process.</p><p>The other options are less relevant or incorrectly targeted:</p><p>- <strong>A.</strong> While ensuring the IAM role used by the security engineer has specific policies can be part of a comprehensive access strategy, the error message indicates an issue with accessing the S3 bucket during the build process, which is more directly addressed by configuring permissions on the instance profile used by the EC2 instance involved in the build.</p><p>- <strong>C.</strong> Attaching the &#96;AWSImageBuilderFullAccess&#96; policy to the instance profile might provide broader permissions than necessary for the specific task of sending logs to an S3 bucket and does not directly address the &quot;Access Denied&quot; error related to S3 bucket access.</p><p>- <strong>D.</strong> While ensuring the security engineer's IAM role has &#96;s3:PutObject&#96; permission for the S3 bucket might seem relevant, the error is more likely related to the permissions of the EC2 instance (or the role it assumes) that is performing the build and attempting to log to S3, rather than the permissions of the security engineer's IAM role.</p>"
    },
    {
        "type": "single",
        "question": "Question #84: A security engineer must use AWS Key Management Service (AWS KMS) to design a key management solution for a set of Amazon Elastic Block Store (Amazon EBS) volumes that contain sensitive data. The solution needs to ensure that the key material automatically expires in 90 days. Which solution meets these criteria?",
        "options": {
            "A": "A customer managed key that uses customer provided key material",
            "B": "A customer managed key that uses AWS provided key material",
            "C": "An AWS managed key",
            "D": "Operating system encryption that uses GnuPG"
        },
        "correctAnswer": "A",
        "explanation":"<p>The correct solution to ensure that key material automatically expires in 90 days for encrypting Amazon Elastic Block Store (Amazon EBS) volumes containing sensitive data using AWS Key Management Service (AWS KMS) is:</p><p><strong>A. A customer managed key that uses customer provided key material</strong></p><p>Explanation:</p><p>- <strong>A. A customer managed key that uses customer provided key material:</strong> AWS KMS allows customers to import their own key material for use with customer managed keys. One of the features of importing your own key material into AWS KMS is the ability to set an expiration date for the key material. After the key material expires, it cannot be used for cryptographic operations, which meets the requirement for the key material to automatically expire in 90 days. This option provides the greatest flexibility in managing the lifecycle of the key material, including its expiration.</p><p>- <strong>B. A customer managed key that uses AWS provided key material:</strong> While customer managed keys offer more management features and capabilities than AWS managed keys, including key rotation and usage policies, they do not inherently include a feature to automatically expire the key material based on a set timeframe like 90 days. The key material provided by AWS does not have an expiration feature that can be directly controlled in such a manner.</p><p>- <strong>C. An AWS managed key:</strong> AWS managed keys are managed and rotated by AWS. Users have limited management capabilities over these keys, such as setting the rotation policy. However, users cannot control the expiration of the key material or import their own key material.</p><p>- <strong>D. Operating system encryption that uses GnuPG:</strong> While this method allows for encryption within the operating system using a tool like GnuPG and could be configured to use keys that expire, it does not leverage AWS KMS for managing the encryption keys. This option would not utilize the integrated AWS service designed for such purposes and requires manual management of keys and their expiration outside of AWS KMS.</p><p>Therefore, the best solution given the requirement for automatic expiration of key material in 90 days is to use a customer managed key with customer provided key material, which allows for the specification of an expiration date for the imported key material.</p>"
    },
    {
        "type": "multi",
        "question": "Question #85: A security engineer is building a Java application that is running on Amazon EC2. The application communicates with an Amazon RDS instance and authenticates with a user name and password. Which combination of steps can the engineer take to protect the credentials and minimize downtime when the credentials are rotated? (Choose two.)",
        "options": {
            "A": "Have a database administrator encrypt the credentials and store the ciphertext in Amazon S3. Grant permission to the instance role associated with the EC2 instance to read the object and decrypt the ciphertext.",
            "B": "Configure a scheduled job that updates the credential in AWS Systems Manager Parameter Store and notifies the engineer that the application needs to be restarted.",
            "C": "Configure automatic rotation of credentials in AWS Secrets Manager.",
            "D": "Store the credential in an encrypted string parameter in AWS Systems Manager Parameter Store. Grant permission to the instance role associated with the EC2 instance to access the parameter and the AWS KMS key that is used to encrypt it.",
            "E": "Configure the Java application to catch a connection failure and make a call to AWS Secrets Manager to retrieve updated credentials when the password is rotated. Grant permission to the instance role associated with the EC2 instance to access Secrets Manager."
        },
        "correctAnswer": [
            "C",
            "E"
        ],
        "explanation":"<p>For the scenario of a Java application on Amazon EC2 communicating with an Amazon RDS instance and needing secure management of credentials with minimal downtime during rotation, the best practices involve using AWS Secrets Manager for automatic rotation and coding the application to gracefully handle credential updates. Thus, the correct options are:</p><p>- <strong>C. Configure automatic rotation of credentials in AWS Secrets Manager.</strong> AWS Secrets Manager is specifically designed to manage, rotate, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. The automatic rotation feature helps ensure that credentials are regularly updated without manual intervention, significantly reducing the risk of unauthorized access due to compromised or stale credentials.</p><p>- <strong>E. Configure the Java application to catch a connection failure and make a call to AWS Secrets Manager to retrieve updated credentials when the password is rotated.</strong> This approach allows the application to dynamically update its credentials without needing a restart. By handling connection failures as signals to fetch new credentials, the application can maintain uptime and reduce the potential impact of credential rotation on the service availability.</p><p>The reasons the other options are less suitable or incorrect are:</p><p>- <strong>A. Encrypting credentials and storing them in Amazon S3</strong> is not the best practice for several reasons. Firstly, this approach requires manual management of encryption keys and the process of encrypting and decrypting credentials, which introduces complexity and potential security risks. Additionally, it does not provide built-in mechanisms for rotation or automatic retrieval of updated credentials, making it a less secure and less efficient option compared to using AWS Secrets Manager.</p><p>- <strong>B. Configuring a scheduled job that updates the credential in AWS Systems Manager Parameter Store and notifies the engineer that the application needs to be restarted</strong> is not ideal because it involves downtime for the application. Restarting the application to update credentials is disruptive and can affect service availability. Moreover, this approach requires manual intervention or additional automation to handle the restart, which is not as seamless as an application logic that automatically fetches updated credentials.</p><p>- <strong>D. Storing the credential in AWS Systems Manager Parameter Store</strong> is a valid approach for securely managing secrets. However, by itself, it does not support automatic rotation of RDS credentials, unlike AWS Secrets Manager. While AWS Systems Manager Parameter Store can store and encrypt credentials securely, the lack of native support for automatic rotation makes it less suitable for scenarios where minimizing operational effort and ensuring credentials are rotated frequently and securely is crucial.</p><p>In summary, options C and E directly address the need for secure credential management with automatic rotation and minimal downtime through dynamic credential retrieval, aligning with best practices for security and operational efficiency in cloud applications.</p>"
    },
    {
        "type": "multi",
        "question": "Question #86: A company uses SAML federation to grant users access to AWS accounts. A company workload that is in an isolated AWS account runs on immutable infrastructure with no human access to Amazon EC2. The company requires a specialized user known as a break glass user to have access to the workload AWS account and instances in the case of SAML errors. A recent audit discovered that the company did not create the break glass user for the AWS account that contains the workload. The company must create the break glass user. The company must log any activities of the break glass user and send the logs to a security team. Which combination of solutions will meet these requirements? (Choose two.)",
        "options": {
            "A": "Create a local individual break glass IAM user for the security team. Create a trail in AWS CloudTrail that has Amazon CloudWatch Logs turned on. Use Amazon EventBridge to monitor local user activities.",
            "B": "Create a break glass EC2 key pair for the AWS account. Provide the key pair to the security team. Use AWS CloudTrail to monitor key pair activity. Send notifications to the security team by using Amazon Simple Notification Service (Amazon SNS).",
            "C": "Create a break glass IAM role for the account. Allow security team members to perform the AssumeRoleWithSAML operation. Create an AWS CloudTrail trail that has Amazon CloudWatch Logs turned on. Use Amazon EventBridge to monitor security team activities.",
            "D": "Create a local individual break glass IAM user on the operating system level of each workload instance. Configure unrestricted security groups on the instances to grant access to the break glass IAM users.",
            "E": "Configure AWS Systems Manager Session Manager for Amazon EC2. Configure an AWS CloudTrail filter based on Session Manager. Send the results to an Amazon Simple Notification Service (Amazon SNS) topic."
        },
        "correctAnswer": [
            "A",
            "E"
        ],
        "explanation":"<p>To address the requirements for creating a break glass user in the scenario provided, while also ensuring that any activities of this user are logged and sent to the security team, the combination of solutions that will meet these requirements are:</p><p>1. <strong>A. Create a local individual break glass IAM user for the security team. Create a trail in AWS CloudTrail that has Amazon CloudWatch Logs turned on. Use Amazon EventBridge to monitor local user activities.</strong>    - <strong>Why it meets requirements:</strong> Creating a local IAM user specifically for break glass scenarios provides a direct method for the security team to access the AWS account in case of SAML errors or other emergency situations. Using AWS CloudTrail to log activities and Amazon CloudWatch Logs for storage, coupled with Amazon EventBridge for monitoring, ensures comprehensive oversight of the break glass user&rsquo;s activities. This setup adheres to the need for accountability and transparency in accessing resources under exceptional circumstances.</p><p>2. <strong>E. Configure AWS Systems Manager Session Manager for Amazon EC2. Configure an AWS CloudTrail filter based on Session Manager. Send the results to an Amazon Simple Notification Service (Amazon SNS) topic.</strong>    - <strong>Why it meets requirements:</strong> AWS Systems Manager Session Manager allows for secure instance management without the need for SSH keys or direct instance access, minimizing the security risks associated with traditional SSH access. By configuring Session Manager, the security team can access instances securely. The integration with CloudTrail for logging and SNS for notifications ensures that the security team is alerted to any break glass activities, maintaining oversight and compliance with security policies.</p><p><strong>Why the other options are less suitable:</strong></p><p>- <strong>B. Create a break glass EC2 key pair for the AWS account.</strong> This approach is not recommended for IAM user management and does not directly address the audit's findings regarding the creation of a break glass user within IAM. Key pairs are used for EC2 instance access and don't provide the centralized control or logging of IAM activities.</p><p>- <strong>C. Create a break glass IAM role for the account with AssumeRoleWithSAML.</strong> While creating an IAM role is a good practice for cross-account access or assuming roles within the same account, it relies on SAML for authentication. In the case of SAML errors, which this break glass account is specifically meant to address, this solution would not be viable.</p><p>- <strong>D. Create a local individual break glass IAM user on the operating system level of each workload instance.</strong> This option mixes concepts; IAM users are AWS-level entities and do not directly correspond to operating system-level access. Furthermore, configuring unrestricted security groups poses a significant security risk and goes against the principle of least privilege.</p><p>The chosen solutions (A and E) provide a balanced approach, offering both AWS-level access through an IAM user and secure instance-level access via Systems Manager Session Manager, each with appropriate logging and alerting mechanisms.</p>"
    },
    {
        "type": "multi",
        "question": "Question #87: A security engineer is working with a product team building a web application on AWS. The application uses Amazon S3 to host the static content, Amazon API Gateway to provide RESTful services, and Amazon DynamoDB as the backend data store. The users already exist in a directory that is exposed through a SAML identity provider. Which combination of the following actions should the engineer take to allow users to be authenticated into the web application and call APIs? (Choose three.)",
        "options": {
            "A": "Create a custom authorization service using AWS Lambda.",
            "B": "Configure a SAML identity provider in Amazon Cognito to map attributes to the Amazon Cognito user pool attributes.",
            "C": "Configure the SAML identity provider to add the Amazon Cognito user pool as a relying party.",
            "D": "Configure an Amazon Cognito identity pool to integrate with social login providers.",
            "E": "Update DynamoDB to store the user email addresses and passwords.",
            "F": "Update API Gateway to use a COGNITO_USER_POOLS authorizer."
        },
        "correctAnswer": [
            "B",
            "C",
            "F"
        ],
        "explanation": "<p>To allow users to be authenticated into the web application and call APIs, while leveraging a SAML identity provider and integrating with AWS services like Amazon S3, Amazon API Gateway, and Amazon DynamoDB, the security engineer should take the following actions:</p><p>1. <strong>Configure a SAML identity provider in Amazon Cognito to map attributes to the Amazon Cognito user pool attributes (Option B).</strong> Amazon Cognito allows for the integration of SAML-based identity providers with user pools. By configuring a SAML identity provider and mapping its attributes to Amazon Cognito user pool attributes, the application can authenticate users using their existing directory credentials. This setup provides a seamless authentication experience for users and leverages the existing identity management infrastructure.</p><p>2. <strong>Configure the SAML identity provider to add the Amazon Cognito user pool as a relying party (Option C).</strong> This step involves configuring the SAML identity provider to recognize the Amazon Cognito user pool as a valid entity that can request authentication assertions. This configuration ensures that authentication requests from the application, routed through Amazon Cognito, are accepted and processed by the SAML identity provider, thereby authenticating users against the directory that is exposed through the SAML provider.</p><p>3. <strong>Update API Gateway to use a COGNITO_USER_POOLS authorizer (Option F).</strong> After setting up Amazon Cognito to handle authentication with the SAML identity provider, the next step is to configure Amazon API Gateway to use an Amazon Cognito user pool as the authorizer. This configuration ensures that only authenticated users can call the APIs, securing access to the backend resources and services provided by the application.</p><p>The other options are not suitable for the described scenario:</p><p>- <strong>Create a custom authorization service using AWS Lambda (Option A).</strong> While AWS Lambda can be used to create a custom authorizer for API Gateway, in this scenario, using Amazon Cognito with a SAML identity provider is a more streamlined and integrated approach for authentication, reducing the need for custom development.</p><p>- <strong>Configure an Amazon Cognito identity pool to integrate with social login providers (Option D).</strong> This option is relevant for scenarios where social identity providers (e.g., Facebook, Google, Amazon) are used for authentication. Since the requirement is to authenticate users from a SAML-based directory, this option does not apply.</p><p>- <strong>Update DynamoDB to store the user email addresses and passwords (Option E).</strong> Storing user credentials directly in DynamoDB is not a recommended practice due to security concerns. It is better to handle authentication through secure, dedicated services like Amazon Cognito and SAML identity providers, which are designed to securely manage user identities and credentials.</p>"
    },
    {
        "type": "multi",
        "question": "Question #88: A company needs to improve its ability to identify and prevent IAM policies that grant public access or cross-account access to resources. The company has implemented AWS Organizations and has started using AWS Identity and Access Management Access Analyzer to refine overly broad access to accounts in the organization. A security engineer must automate a response in the company's organization for any newly created policies that are overly permissive. The automation must remediate external access and must notify the company's security team. Which combination of steps should the security engineer take to meet these requirements? (Choose three.)",
        "options": {
            "A": "Create an AWS Step Functions state machine that checks the resource type in the finding and adds an explicit Deny statement in the trust policy for the IAM role. Configure the state machine to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic.",
            "B": "Create an AWS Batch job that forwards any resource type findings to an AWS Lambda function. Configure the Lambda function to add an explicit Deny statement in the trust policy for the IAM role. Configure the AWS Batch job to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic.",
            "C": "In Amazon EventBridge, create an event rule that matches active IAM Access Analyzer findings and invokes AWS Step Functions for resolution.",
            "D": "In Amazon CloudWatch, create a metric filter that matches active IAM Access Analyzer findings and invokes AWS Batch for resolution.",
            "E": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the queue to forward a notification to the security team that an external principal has been granted access to the specific IAM role and has been blocked.",
            "F": "Create an Amazon Simple Notification Service (Amazon SNS) topic for external or cross-account access notices. Subscribe the security team's email addresses to the topic."
        },
        "correctAnswer": [
            "A",
            "C",
            "F"
        ],
        "explanation":"<p>To automate the response to newly created IAM policies that are overly permissive, addressing both the remediation of external access and notification of the security team, the security engineer should take the following steps:</p><p>1. <strong>In Amazon EventBridge, create an event rule that matches active IAM Access Analyzer findings and invokes AWS Step Functions for resolution (Option C).</strong> EventBridge can detect and respond to changes in AWS resources, including findings from IAM Access Analyzer. By creating an event rule that listens for Access Analyzer findings indicating overly permissive policies, and invoking Step Functions, the engineer can automate the process of assessing and remediating these policies.</p><p>2. <strong>Create an AWS Step Functions state machine that checks the resource type in the finding and adds an explicit Deny statement in the trust policy for the IAM role. Configure the state machine to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic (Option A).</strong> AWS Step Functions allows for the orchestration of complex workflows, including the conditional logic required to assess Access Analyzer findings and apply appropriate remediations. By adding explicit Deny statements to overly broad trust policies, the state machine can effectively narrow down permissions. Additionally, integrating notification via an SNS topic ensures that the security team is informed of the remediation actions taken.</p><p>3. <strong>Create an Amazon Simple Notification Service (Amazon SNS) topic for external or cross-account access notices. Subscribe the security team's email addresses to the topic (Option F).</strong> SNS is a flexible, fully-managed messaging service that can distribute messages to subscribed endpoints, such as email addresses. By creating a dedicated topic for notices related to external or cross-account access and subscribing the security team to this topic, the engineer ensures that the team is promptly informed of any findings and remediation actions, facilitating rapid response and further investigation if necessary.</p><p>The other options are not as suitable:</p><p>- <strong>Option B (Create an AWS Batch job)</strong> introduces unnecessary complexity and is not directly suited for real-time event-driven remediation and notification in response to IAM Access Analyzer findings.</p><p>- <strong>Option D (In Amazon CloudWatch, create a metric filter)</strong> is not directly applicable to handling IAM Access Analyzer findings, as CloudWatch is primarily focused on monitoring and observability, rather than event-driven automation or policy remediation.</p><p>- <strong>Option E (Create an Amazon Simple Queue Service (Amazon SQS) queue)</strong> could be part of a larger solution but on its own does not address the immediate need for remediation or the direct notification of security team as effectively as the selected options. SQS is typically used for decoupling components of a cloud application, rather than for immediate action and notification in security use cases.</p>"
    },
    {
        "type": "single",
        "question": "Question #89: A security engineer is configuring a mechanism to send an alert when three or more failed sign-in attempts to the AWS Management Console occur during a 5-minute period. The security engineer creates a trail in AWS CloudTrail to assist in this work. Which solution will meet these requirements?",
        "options": {
            "A": "In CloudTrail, turn on Insights events on the trail. Configure an alarm on the insight with eventName matching ConsoleLogin and errorMessage matching \"Failed authentication''. Configure a threshold of 3 and a period of 5 minutes.",
            "B": "Configure CloudTrail to send events to Amazon CloudWatch Logs. Create a metric filter for the relevant log group. Create a filter pattern with eventName matching ConsoleLogin and errorMessage matching \"Failed authentication\". Create a CloudWatch alarm with a threshold of 3 and a period of 5 minutes.",
            "C": "Create an Amazon Athena table from the CloudTrail events. Run a query for eventName matching ConsoleLogin and for errorMessage matching \"Failed authentication\". Create a notification action from the query to send an Amazon Simple Notification Service (Amazon SNS) notification when the count equals 3 within a period of 5 minutes.",
            "D": "In AWS Identity and Access Management Access Analyzer, create a new analyzer. Configure the analyzer to send an Amazon Simple Notification Service (Amazon SNS) notification when a failed sign-in event occurs 3 times for any IAM user within a period of 5 minutes."
        },
        "correctAnswer": "B"
    },
    {
        "type": "multi",
        "question": "Question #90: A company's security engineer is developing an incident response plan to detect suspicious activity in an AWS account for VPC hosted resources. The security engineer needs to provide visibility for as many AWS Regions as possible. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
        "options": {
            "A": "Turn on VPC Flow Logs for all VPCs in the account.",
            "B": "Activate Amazon GuardDuty across all AWS Regions.",
            "C": "Activate Amazon Detective across all AWS Regions.",
            "D": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Create an Amazon EventBridge rule that responds to findings and publishes the findings to the SNS topic.",
            "E": "Create an AWS Lambda function. Create an Amazon EventBridge rule that invokes the Lambda function to publish findings to Amazon Simple Email Service (Amazon SES)."
        },
        "correctAnswer": [
            "B",
            "D"
        ]
    },
    {
        "type": "multi",
        "question": "Question #91: A company stores images for a website in an Amazon S3 bucket. The company is using Amazon CloudFront to serve the images to end users. The company recently discovered that the images are being accessed from countries where the company does not have a distribution license. Which actions should the company take to secure the images to limit their distribution? (Choose two.)",
        "options": {
            "A": "Update the S3 bucket policy to restrict access to a CloudFront origin access control (OAC).",
            "B": "Update the website DNS record to use an Amazon Route 53 geolocation record deny list of countries where the company lacks a license.",
            "C": "Add a CloudFront geo restriction deny list of countries where the company lacks a license.",
            "D": "Update the S3 bucket policy with a deny list of countries where the company lacks a license.",
            "E": "Enable the Restrict Viewer Access option in CloudFront to create a deny list of countries where the company lacks a license."
        },
        "correctAnswer": [
            "A",
            "C"
        ],
        "explanation":"<p>To secure the images stored in an Amazon S3 bucket and limit their distribution to comply with licensing agreements, the company should take the following actions:</p><p>1. <strong>Update the S3 bucket policy to restrict access to a CloudFront origin access control (OAC) (Option A).</strong> This action ensures that the images in the S3 bucket can only be accessed through CloudFront, effectively blocking direct access to the S3 bucket. By using CloudFront Origin Access Control, the company can ensure that the S3 bucket is not publicly accessible and that content is only served through CloudFront, where further restrictions can be applied.</p><p>2. <strong>Add a CloudFront geo restriction deny list of countries where the company lacks a license (Option C).</strong> CloudFront geo restriction (also known as geoblocking) allows the company to restrict access to the content based on the geographic location of the end users. By configuring a deny list of countries where the company does not have distribution rights, CloudFront will block requests from those locations, ensuring compliance with licensing agreements.</p><p>The other options are less suitable or not feasible for the described scenario:</p><p>- <strong>Update the website DNS record to use an Amazon Route 53 geolocation record deny list of countries where the company lacks a license (Option B).</strong> While Amazon Route 53 can route traffic based on geographic location, it does not prevent users in denied locations from accessing content if they can bypass DNS settings or if the content is cached elsewhere. This method does not provide the same level of access control specific to the content being served through CloudFront.</p><p>- <strong>Update the S3 bucket policy with a deny list of countries where the company lacks a license (Option D).</strong> S3 bucket policies do not have the capability to deny access based on the geographic location of the requester. Access control based on country is a feature specific to services like CloudFront that can interpret the location of a request.</p><p>- <strong>Enable the Restrict Viewer Access option in CloudFront to create a deny list of countries where the company lacks a license (Option E).</strong> The &quot;Restrict Viewer Access&quot; option in CloudFront is used for serving private content with signed URLs or signed cookies, not for creating a deny list of countries. Geo restriction is the correct feature for blocking access from specific countries.</p>"
    },
    {
        "type": "single",
        "question": "Question #92: A company has deployed servers on Amazon EC2 instances in a VPC. External vendors access these servers over the internet. Recently, the company deployed a new application on EC2 instances in a new CIDR range. The company needs to make the application available to the vendors. A security engineer verified that the associated security groups and network ACLs are allowing the required ports in the inbound direction. However, the vendors cannot connect to the application. Which solution will provide the vendors access to the application?",
        "options": {
            "A": "Modify the security group that is associated with the EC2 instances to have the same outbound rules as inbound rules.",
            "B": "Modify the network ACL that is associated with the CIDR range to allow outbound traffic to ephemeral ports.",
            "C": "Modify the inbound rules on the internet gateway to allow the required ports.",
            "D": "Modify the network ACL that is associated with the CIDR range to have the same outbound rules as inbound rules."
        },
        "correctAnswer": "B",
        "explanation":"<p>The solution that will provide the vendors access to the application, given that inbound rules are already configured correctly but vendors still cannot connect, is:</p><p><strong>B. Modify the network ACL that is associated with the CIDR range to allow outbound traffic to ephemeral ports.</strong></p><p>Explanation:</p><p>- <strong>Network Access Control Lists (NACLs)</strong> are an optional layer of security for your VPC that act as a firewall for controlling traffic in and out of one or more subnets. Unlike Security Groups, NACLs are stateless; therefore, they require both inbound and outbound rules to be explicitly defined to allow response traffic from established connections to flow.</p><p>- When external vendors initiate connections to the servers, the return traffic from the servers back to the vendors will use ephemeral ports. These are temporary ports assigned by the server's operating system for the duration of the session. If outbound traffic to these ephemeral ports is not allowed by the NACLs associated with the subnet where the EC2 instances are located, the response packets from the servers cannot reach the vendors, thus preventing successful connections.</p><p>- <strong>Modifying the NACLs to allow outbound traffic to ephemeral ports</strong> addresses this issue by ensuring that responses to inbound requests can be sent back to the clients. The range of ephemeral ports can vary, but commonly used ranges include 1024-65535 for Linux and Unix systems.</p><p>The other options are less suitable or incorrect because:</p><p>- <strong>A. Modifying the security group to have the same outbound rules as inbound rules:</strong> Security groups are stateful, meaning they automatically allow return traffic for allowed inbound connections, regardless of outbound rules. Changing outbound rules in this context would not address the issue if inbound connections are already allowed.</p><p>- <strong>C. Modifying the inbound rules on the internet gateway to allow the required ports:</strong> The internet gateway does not have &quot;rules&quot; that can be modified in this way. It simply routes traffic between the internet and the VPC. The connectivity issue described is not related to internet gateway configurations.</p><p>- <strong>D. Modifying the network ACL to have the same outbound rules as inbound rules:</strong> This option might seem correct, but it's too general. The specific action needed is to allow outbound traffic to ephemeral ports, which is a subset of potentially matching outbound rules to inbound rules. The essential point is to ensure that the NACL allows outbound traffic necessary for the return communication, which typically involves ephemeral ports.</p>"
    },
    {
        "type": "single",
        "question": "Question #93: A company uses infrastructure as code (IaC) to create AWS infrastructure. The company writes the code as AWS CloudFormation templates to deploy the infrastructure. The company has an existing CI/CD pipeline that the company can use to deploy these templates. After a recent security audit, the company decides to adopt a policy-as-code approach to improve the company's security posture on AWS. The company must prevent the deployment of any infrastructure that would violate a security policy, such as an unencrypted Amazon Elastic Block Store (Amazon EBS) volume. Which solution will meet these requirements?",
        "options": {
            "A": "Turn on AWS Trusted Advisor. Configure security notifications as webhooks in the preferences section of the CI/CD pipeline.",
            "B": "Turn on AWS Config. Use the prebuilt rules or customized rules. Subscribe the CI/CD pipeline to an Amazon Simple Notification Service (Amazon SNS) topic that receives notifications from AWS Config.",
            "C": "Create rule sets in AWS CloudFormation Guard. Run validation checks for CloudFormation templates as a phase of the CI/CD process.",
            "D": "Create rule sets as SCPs. Integrate the SCPs as a part of validation control in a phase of the CI/CD process."
        },
        "correctAnswer": "C",
        "explanation":"<p>To meet the requirements of adopting a policy-as-code approach and preventing the deployment of any infrastructure that would violate a security policy, such as deploying an unencrypted Amazon EBS volume, the most suitable solution is:</p><p><strong>C. Create rule sets in AWS CloudFormation Guard. Run validation checks for CloudFormation templates as a phase of the CI/CD process.</strong></p><p>Explanation:</p><p>- <strong>AWS CloudFormation Guard</strong> is a command line tool that helps you enforce compliance and governance of your AWS resources by allowing you to write declarative rules for AWS CloudFormation templates. By creating rule sets with CloudFormation Guard, you can define policies that must be adhered to, such as requiring that all EBS volumes be encrypted. Integrating these validation checks directly into your CI/CD pipeline allows for automated enforcement of your security policies at the infrastructure as code (IaC) level before the infrastructure is deployed. This approach helps ensure that only compliant infrastructure is provisioned, effectively preventing security policy violations.</p><p>The other options are less suitable for directly preventing the deployment of non-compliant infrastructure:</p><p>- <strong>A. AWS Trusted Advisor</strong> provides best practice recommendations across your AWS accounts in five categories: cost optimization, performance, security, fault tolerance, and service limits. While it can identify unencrypted EBS volumes among other recommendations, it does so post-deployment and is not a preventive measure within a CI/CD pipeline.</p><p>- <strong>B. AWS Config</strong> monitors and records AWS resource configurations and allows you to evaluate the recorded configurations against desired configurations. Using AWS Config with prebuilt or custom rules can help identify non-compliant resources, but this happens after resources have been deployed. While AWS Config can trigger notifications for non-compliant resources, it is not inherently a preventive measure within the CI/CD pipeline for stopping the deployment of non-compliant resources.</p><p>- <strong>D. Service Control Policies (SCPs)</strong> are used in AWS Organizations to manage permissions in member accounts. While SCPs can prevent certain actions from being taken (like preventing the creation of unencrypted EBS volumes), they operate at the AWS account level rather than the infrastructure code level. Integrating SCPs into a CI/CD pipeline validation process is not directly applicable, as SCPs are not designed to analyze or validate CloudFormation templates before deployment.</p><p>Therefore, using AWS CloudFormation Guard to create rule sets and integrating these checks into the CI/CD process is the most effective way to ensure compliance with security policies during the deployment of AWS infrastructure.</p>"
    },
    {
        "type": "single",
        "question": "Question #94: A company is running an Amazon RDS for MySQL DB instance in a VPC. The VPC must not send or receive network traffic through the internet. A security engineer wants to use AWS Secrets Manager to rotate the DB instance credentials automatically. Because of a security policy, the security engineer cannot use the standard AWS Lambda function that Secrets Manager provides to rotate the credentials. The security engineer deploys a custom Lambda function in the VPC. The custom Lambda function will be responsible for rotating the secret in Secrets Manager. The security engineer edits the DB instance's security group to allow connections from this function. When the function is invoked, the function cannot communicate with Secrets Manager to rotate the secret properly. What should the security engineer do so that the function can rotate the secret?",
        "options": {
            "A": "Add an egress-only internet gateway to the VPC. Allow only the Lambda function's subnet to route traffic through the egress-only internet gateway.",
            "B": "Add a NAT gateway to the VPC. Configure only the Lambda function's subnet with a default route through the NAT gateway.",
            "C": "Configure a VPC peering connection to the default VPC for Secrets Manager. Configure the Lambda function's subnet to use the peering connection for routes.",
            "D": "Configure a Secrets Manager interface VPC endpoint. Include the Lambda function's private subnet during the configuration process."
        },
        "correctAnswer": "D",
        "explanation":"<p>To enable the custom Lambda function in the VPC to communicate with AWS Secrets Manager for rotating the DB instance credentials automatically, while adhering to the policy of not sending or receiving network traffic through the internet, the security engineer should:</p><p><strong>D. Configure a Secrets Manager interface VPC endpoint. Include the Lambda function's private subnet during the configuration process.</strong></p><p>Explanation:</p><p>- <strong>AWS Secrets Manager interface VPC endpoint</strong> allows you to securely connect your VPC to AWS Secrets Manager without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. VPC endpoints are powered by AWS PrivateLink, a technology that enables you to privately access Secrets Manager APIs by using private IP addresses. By creating a VPC endpoint for Secrets Manager, the Lambda function deployed within the VPC can securely access Secrets Manager to rotate the secret without the need to traverse the public internet.</p><p>The other options are less suitable or incorrect given the security requirements:</p><p>- <strong>A. Add an egress-only internet gateway to the VPC.</strong> This option is not applicable because egress-only internet gateways are used in VPCs with IPv6 to allow outbound internet traffic and prevent inbound traffic. It does not help in scenarios where the VPC must not send or receive network traffic through the internet, and it's specifically designed for IPv6, not IPv4, which is typically used by AWS services.</p><p>- <strong>B. Add a NAT gateway to the VPC.</strong> While a NAT gateway allows instances in a private subnet to initiate outbound IPv4 traffic to the internet or other AWS services, it violates the policy that the VPC must not send or receive network traffic through the internet. Moreover, using a NAT gateway incurs additional costs and does not align with the requirement to avoid internet connectivity.</p><p>- <strong>C. Configure a VPC peering connection to the default VPC for Secrets Manager.</strong> This option is incorrect because AWS Secrets Manager does not have a &quot;default VPC.&quot; VPC peering connections are used to connect two VPCs to allow traffic to route between them, and it's not a method to provide access to AWS services like Secrets Manager without internet access. The appropriate solution for direct service access is through VPC endpoints.</p>"
    },
    {
        "type": "multi",
        "question": "Question #95: The security engineer is managing a traditional three-tier web application that is running on Amazon EC2 instances. The application has become the target of increasing numbers of malicious attacks from the internet. What steps should the security engineer take to check for known vulnerabilities and limit the attack surface? (Choose two.)",
        "options": {
            "A": "Use AWS Certificate Manager to encrypt all traffic between the client and application servers.",
            "B": "Review the application security groups to ensure that only the necessary ports are open.",
            "C": "Use Elastic Load Balancing to offload Secure Sockets Layer encryption.",
            "D": "Use Amazon Inspector to periodically scan the backend instances.",
            "E": "Use AWS Key Management Service (AWS KMS) to encrypt all the traffic between the client and application servers."
        },
        "correctAnswer": [
            "B",
            "D"
        ]
    },
    {
        "type": "single",
        "question": "Question #96: A company is using Amazon Elastic Container Service (Amazon ECS) to run its container-based application on AWS. The company needs to ensure that the container images contain no severe vulnerabilities. The company also must ensure that only specific IAM roles and specific AWS accounts can access the container images. Which solution will meet these requirements with the LEAST management overhead?",
        "options": {
            "A": "Pull images from the public container registry. Publish the images to Amazon Elastic Container Registry (Amazon ECR) repositories with scan on push configured in a centralized AWS account. Use a CI/CD pipeline to deploy the images to different AWS accounts. Use identity-based policies to restrict access to which IAM principals can access the images.",
            "B": "Pull images from the public container registry. Publish the images to a private container registry that is hosted on Amazon EC2 instances in a centralized AWS account. Deploy host-based container scanning tools to EC2 instances that run Amazon ECS. Restrict access to the container images by using basic authentication over HTTPS.",
            "C": "Pull images from the public container registry. Publish the images to Amazon Elastic Container Registry (Amazon ECR) repositories with scan on push configured in a centralized AWS account. Use a CI/CD pipeline to deploy the images to different AWS accounts. Use repository policies and identity-based policies to restrict access to which IAM principals and accounts can access the images.",
            "D": "Pull images from the public container registry. Publish the images to AWS CodeArtifact repositories in a centralized AWS account. Use a CI/CD pipeline to deploy the images to different AWS accounts. Use repository policies and identity-based policies to restrict access to which IAM principals and accounts can access the images."
        },
        "correctAnswer": "C",
        "explanation":"<p>The solution that meets the requirements with the least management overhead is:</p><p><strong>C. Pull images from the public container registry. Publish the images to Amazon Elastic Container Registry (Amazon ECR) repositories with scan on push configured in a centralized AWS account. Use a CI/CD pipeline to deploy the images to different AWS accounts. Use repository policies and identity-based policies to restrict access to which IAM principals and accounts can access the images.</strong></p><p>Explanation:</p><p>- <strong>Amazon Elastic Container Registry (Amazon ECR)</strong> is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. ECR is integrated with Amazon ECS, allowing you to simplify your development and production workflows.</p><p>- <strong>Scan on push configuration</strong> in ECR automatically scans your Docker images for vulnerabilities when they are pushed to the registry, helping to ensure that the container images contain no severe vulnerabilities.</p><p>- <strong>Repository policies and identity-based policies</strong> provide fine-grained access control to ECR repositories. Repository policies can specify which AWS accounts can access your repositories, and identity-based policies can define which IAM roles within those accounts are allowed to access the images. This setup meets the requirement to ensure that only specific IAM roles and specific AWS accounts can access the container images.</p><p>The other options are less suitable:</p><p>- <strong>A.</strong> While this option also involves using ECR and CI/CD pipelines, it only mentions using identity-based policies to restrict access. The key distinction in option C is the mention of both repository policies and identity-based policies, providing a more comprehensive approach to access control.</p><p>- <strong>B.</strong> Hosting a private container registry on Amazon EC2 instances introduces significant management overhead, including maintaining the underlying infrastructure, securing the registry, and implementing host-based container scanning tools. This option does not leverage the managed services' benefits and increases complexity.</p><p>- <strong>D.</strong> AWS CodeArtifact is a fully managed artifact repository service that makes it easy for organizations to securely store, publish, and share software packages used in their software development process. However, CodeArtifact is designed for artifacts like libraries and dependencies, not specifically for container images, making it less suitable for the described use case than ECR.</p>"
    },
    {
        "type": "single",
        "question": "Question #97: A company's data scientists want to create artificial intelligence and machine learning (AI/ML) training models by using Amazon SageMaker. The training models will use large datasets in an Amazon S3 bucket. The datasets contain sensitive information. On average, the data scientists need 30 days to train models. The S3 bucket has been secured appropriately. The company's data retention policy states that all data that is older than 45 days must be removed from the S3 bucket. Which action should a security engineer take to enforce this data retention policy?",
        "options": {
            "A": "Configure an S3 Lifecycle rule on the S3 bucket to delete objects after 45 days.",
            "B": "Create an AWS Lambda function to check the last-modified date of the S3 objects and delete objects that are older than 45 days. Create an S3 event notification to invoke the Lambda function for each PutObject operation.",
            "C": "Create an AWS Lambda function to check the last-modified date of the S3 objects and delete objects that are older than 45 days. Create an Amazon EventBridge rule to invoke the Lambda function each month.",
            "D": "Configure S3 Intelligent-Tiering on the S3 bucket to automatically transition objects to another storage class."
        },
        "correctAnswer": "A"
    },
    {
        "type": "single",
        "question": "Question #98: A security engineer is troubleshooting an AWS Lambda function that is named MyLambdaFunction. The function is encountering an error when the function attempts to read the objects in an Amazon S3 bucket that is named DOC-EXAMPLE-BUCKET. The S3 bucket has the following bucket policy: Which change should the security engineer make to the policy to ensure that the Lambda function can read the bucket objects?",
        "options": {
            "A": "Remove the Condition element. Change the Principal element to the following:",
            "B": "Change the Action element to the following:",
            "C": "Change the Resource element to \"arn:aws:s3:::DOC-EXAMPLE- BUCKET/*\".",
            "D": "Change the Resource element to \"arn:aws:lambda:::function:MyLambdaFunction\". Change the Principal element to the following:"
        },
        "image": "98.png",
        "correctAnswer": "C",
        "explanation":"<p>The community's suggestion that the correct answer is **C** is indeed valid if the policy's &#96;Resource&#96; element is incorrectly specifying the bucket itself without including the objects within the bucket. In AWS S3 bucket policies, to refer to all objects in a bucket, you must include &quot;/*&quot; at the end of the bucket ARN. Here is why:</p><p><strong>C. Change the Resource element to &quot;arn:aws:s3:::DOC-EXAMPLE-BUCKET/*&quot;.</strong></p><p>Explanation:</p><p>- The current &#96;Resource&#96; element in the policy &#96;&quot;arn:aws:s3:::DOC-EXAMPLE-BUCKET&quot;&#96; points to the bucket itself and not the objects within the bucket. To grant access to read objects, the policy must specify the object-level path which includes the &quot;/*&quot; wildcard after the bucket name. This indicates that the permission applies to all objects within the bucket.</p><p>- The &#96;Condition&#96; element with the &#96;aws:SourceArn&#96; key restricts the permission to requests made from a specific source, which in this case, is the Lambda function &#96;MyLambdaFunction&#96;. If the Lambda function's &#96;Invoke&#96; action is configured correctly with the right ARN, and the function's execution role has the necessary permissions, this condition should not prevent the Lambda function from accessing the objects. Hence, the condition does not necessarily need to be removed unless it is confirmed to be the cause of the issue.</p><p>- The &#96;Principal&#96; element is using a service principal (&#96;lambda.amazonaws.com&#96;), which is correct when you want to allow the Lambda service to assume the role attached to a function to perform actions specified in the bucket policy.</p><p>Therefore, correcting the &#96;Resource&#96; element to include all objects in the bucket with &quot;/*&quot; would ensure that the Lambda function has the necessary permissions to read the objects, assuming the other elements like &#96;Principal&#96; and &#96;Condition&#96; are properly configured to match the Lambda function's execution role and ARN respectively.</p>"
    },
    {
        "type": "single",
        "question": "Question #99: An IAM user receives an Access Denied message when the user attempts to access objects in an Amazon S3 bucket. The user and the S3 bucket are in the same AWS account. The S3 bucket is configured to use server-side encryption with AWS KMS keys (SSE-KMS) to encrypt all of its objects at rest by using a customer managed key from the same AWS account. The S3 bucket has no bucket policy defined. The IAM user has been granted permissions through an IAM policy that allows the kms:Decrypt permission to the customer managed key. The IAM policy also allows the s3:List* and s3:Get* permissions for the S3 bucket and its objects. Which of the following is a possible reason that the IAM user cannot access the objects in the S3 bucket?",
        "options": {
            "A": "The IAM policy needs to allow the kms:DescribeKey permission.",
            "B": "The S3 bucket has been changed to use the AWS managed key to encrypt objects at rest.",
            "C": "An S3 bucket policy needs to be added to allow the IAM user to access the objects.",
            "D": "The KMS key policy has been edited to remove the ability for the AWS account to have full access to the key."
        },
        "correctAnswer": "D",
        "explanation":"<p>D. The KMS key policy has been edited to remove the ability for the AWS account to have full access to the key.</p><p>Explanation:</p><p>- <strong>AWS KMS keys (SSE-KMS)</strong> are used to encrypt and decrypt S3 objects in conjunction with S3's server-side encryption feature. The access to use these keys for encryption and decryption operations is controlled through the key policy associated with the KMS key.</p><p>- If the IAM user has the necessary &#96;kms:Decrypt&#96; permission as well as the &#96;s3:List*&#96; and &#96;s3:Get*&#96; permissions for the S3 bucket and its objects, the user should normally be able to access the objects in the S3 bucket. However, if the KMS key policy does not allow the IAM user or the IAM user's role to use the key, or if the account has been removed from the key policy altogether, the user will not be able to perform operations that require decryption with that key, resulting in an Access Denied message.</p><p>The other options are less likely based on the scenario described:</p><p>- <strong>A. The &#96;kms:DescribeKey&#96; permission</strong> is not necessary for decryption operations. It is used to retrieve metadata about the key but not for the actual process of encryption or decryption.</p><p>- <strong>B.</strong> If the S3 bucket had been changed to use the AWS managed key, the IAM user would not typically receive an Access Denied message due to key permissions, as AWS managed keys do not require the same explicit permissions in the key policy as customer managed keys.</p><p>- <strong>C.</strong> Since there is no bucket policy defined, the absence of a bucket policy is not the cause of the Access Denied message. Furthermore, if the IAM user has the necessary S3 permissions through the IAM policy, an S3 bucket policy is not required for access.</p>"
    }
]