[
    {
        "type": "single",
        "question": "Question #67: A company is hosting multiple applications within a single VPC in its AWS account. The applications are running behind an Application Load Balancer that is associated with an AWS WAF web ACL. The company's security team has identified that multiple port scans are originating from a specific range of IP addresses on the internet. A security engineer needs to deny access from the offending IP addresses. Which solution will meet these requirements?",
        "options": {
            "A": "Modify the AWS WAF web ACL with an IP set match rule statement to deny incoming requests from the IP address range.",
            "B": "Add a rule to all security groups to deny the incoming requests from the IP address range.",
            "C": "Modify the AWS WAF web ACL with a rate-based rule statement to deny the incoming requests from the IP address range.",
            "D": "Configure the AWS WAF web ACL with regex match conditions. Specify a pattern set to deny the incoming requests based on the match condition."
        },
        "correctAnswer": "A"
    },
    {
        "type": "multi",
        "question": "Question #68: A company has contracted with a third party to audit several AWS accounts. To enable the audit, cross-account IAM roles have been created in each account targeted for audit. The auditor is having trouble accessing some of the accounts. Which of the following may be causing this problem? (Choose three.)",
        "options": {
            "A": "The external ID used by the auditor is missing or incorrect.",
            "B": "The auditor is using the incorrect password.",
            "C": "The auditor has not been granted sts:AssumeRole for the role in the destination account.",
            "D": "The Amazon EC2 role used by the auditor must be set to the destination account role.",
            "E": "The secret key used by the auditor is missing or incorrect.",
            "F": "The role ARN used by the auditor is missing or incorrect."
        },
        "correctAnswer": [
            "A",
            "C",
            "F"
        ]
    },
    {
        "type": "single",
        "question": "Question #69: A security engineer needs to configure an Amazon S3 bucket policy to restrict access to an S3 bucket that is named DOC-EXAMPLE-BUCKET. The policy must allow access to only DOC-EXAMPLE-BUCKET from only the following endpoint: vpce-1a2b3c4d. The policy must deny all access to DOC-EXAMPLE-BUCKET if the specified endpoint is not used. Which bucket policy statement meets these requirements?",
        "options": {
            "A": "A",
            "B": "B",
            "C": "C",
            "D": "D"
        },
        "correctAnswer": "B",
        "image":"69.png"
    },
    {
        "type": "multi",
        "question": "Question #70: A company has a group of Amazon EC2 instances in a single private subnet of a VPC with no internet gateway attached. A security engineer has installed the Amazon CloudWatch agent on all instances in that subnet to capture logs from a specific application. To ensure that the logs flow securely, the company's networking team has created VPC endpoints for CloudWatch monitoring and CloudWatch logs. The networking team has attached the endpoints to the VPC. The application is generating logs. However, when the security engineer queries CloudWatch, the logs do not appear. Which combination of steps should the security engineer take to troubleshoot this issue? (Choose three.)",
        "options": {
            "A": "Ensure that the EC2 instance profile that is attached to the EC2 instances has permissions to create log streams and write logs.",
            "B": "Create a metric filter on the logs so that they can be viewed in the AWS Management Console.",
            "C": "Check the CloudWatch agent configuration file on each EC2 instance to make sure that the CloudWatch agent is collecting the proper log files.",
            "D": "Check the VPC endpoint policies of both VPC endpoints to ensure that the EC2 instances have permissions to use them.",
            "E": "Create a NAT gateway in the subnet so that the EC2 instances can communicate with CloudWatch.",
            "F": "Ensure that the security groups allow all the EC2 instances to communicate with each other to aggregate logs before sending."
        },
        "correctAnswer": [
            "A",
            "C",
            "D"
        ],
        "explanation": "<p>To troubleshoot the issue of logs not appearing in CloudWatch despite the setup of VPC endpoints for CloudWatch monitoring and CloudWatch logs in a subnet with no internet gateway, the security engineer should consider the following steps:</p><p>1. <strong>Ensure that the EC2 instance profile attached to the EC2 instances has permissions to create log streams and write logs (Option A).</strong> This is crucial because the CloudWatch agent needs appropriate permissions to send logs to CloudWatch. The instance profile (IAM role) associated with the EC2 instances must have policies granting permissions to CloudWatch services to create log streams and put log events.</p><p>2. <strong>Check the CloudWatch agent configuration file on each EC2 instance to make sure that the CloudWatch agent is collecting the proper log files (Option C).</strong> The agent's configuration file specifies which log files to monitor and send to CloudWatch. It's possible that the configuration file does not correctly specify the paths to the log files generated by the application or the necessary details about the log group and log stream.</p><p>3. <strong>Check the VPC endpoint policies of both VPC endpoints to ensure that the EC2 instances have permissions to use them (Option D).</strong> VPC endpoint policies can restrict the actions that can be performed or the resources that can be accessed through the endpoint. Ensuring that the endpoint policies explicitly allow the actions required by the CloudWatch agent to send logs is important for the logs to flow through the endpoints to CloudWatch.</p><p>The other options, while relevant to different contexts, are not directly applicable to this scenario:</p><p>- <strong>Creating a metric filter on the logs (Option B)</strong> is not a troubleshooting step for ensuring logs reach CloudWatch; metric filters are used to analyze and act on the log data once it's in CloudWatch. - <strong>Creating a NAT gateway in the subnet (Option E)</strong> is not necessary in this case because the VPC endpoints are meant to provide private access to AWS services without needing internet access or a NAT gateway. - <strong>Ensuring that the security groups allow all the EC2 instances to communicate with each other to aggregate logs before sending (Option F)</strong> is not relevant to this problem. The CloudWatch agent sends logs directly to CloudWatch from each instance, and inter-instance communication is not required for this process.</p>"
    },
    {
        "type": "single",
        "question": "Question #71: A company uses AWS Signer with all of the company's AWS Lambda functions. A developer recently stopped working for the company. The company wants to ensure that all the code that the developer wrote can no longer be deployed to the Lambda functions. Which solution will meet this requirement?",
        "options": {
            "A": "Revoke all versions of the signing profile assigned to the developer.",
            "B": "Examine the developer's IAM roles. Remove all permissions that grant access to Signer.",
            "C": "Re-encrypt all source code with a new AWS Key Management Service (AWS KMS) key.",
            "D": "Use Amazon CodeGuru to profile all the code that the Lambda functions use."
        },
        "correctAnswer": "A"
    },
    {
        "type": "single",
        "question": "Question #72: A company plans to use AWS Key Management Service (AWS KMS) to implement an encryption strategy to protect data at rest. The company requires client-side encryption for company projects. The company is currently conducting multiple projects to test the company's use of AWS KMS. These tests have led to a sudden increase in the company's AWS resource consumption. The test projects include applications that issue multiple requests each second to KMS endpoints for encryption activities. The company needs to develop a solution that does not throttle the company's ability to use AWS KMS. The solution must improve key usage for client-side encryption and must be cost optimized. Which solution will meet these requirements?",
        "options": {
            "A": "Use keyrings with the AWS Encryption SDK. Use each keyring individually or combine keyrings into a multi-keyring. Decrypt the data by using a keyring that has the primary key in the multi-keyring.",
            "B": "Use data key caching. Use the local cache that the AWS Encryption SDK provides with a caching cryptographic materials manager.",
            "C": "Use KMS key rotation. Use a local cache in the AWS Encryption SDK with a caching cryptographic materials manager.",
            "D": "Use keyrings with the AWS Encryption SDK. Use each keyring individually or combine keyrings into a multi-keyring. Use any of the wrapping keys in the multi-keyring to decrypt the data."
        },
        "correctAnswer": "B"
    },
    {
        "type": "single",
        "question": "Question #73: A security team is working on a solution that will use Amazon EventBridge to monitor new Amazon S3 objects. The solution will monitor for public access and for changes to any S3 bucket policy or setting that result in public access. The security team configures EventBridge to watch for specific API calls that are logged from AWS CloudTrail. EventBridge has an action to send an email notification through Amazon Simple Notification Service (Amazon SNS) to the security team immediately with details of the API call. Specifically, the security team wants EventBridge to watch for the s3:PutObjectAcl, s3:DeleteBucketPolicy, and s3:PutBucketPolicy API invocation logs from CloudTrail. While developing the solution in a single account, the security team discovers that the s3:PutObjectAcl API call does not invoke an EventBridge event. However, the s3:DeleteBucketPolicy API call and the s3:PutBucketPolicy API call do invoke an event. The security team has enabled CloudTrail for AWS management events with a basic configuration in the AWS Region in which EventBridge is being tested. Verification of the EventBridge event pattern indicates that the pattern is set up correctly. The security team must implement a solution so that the s3:PutObjectAcl API call will invoke an EventBridge event. The solution must not generate false notifications. Which solution will meet these requirements?",
        "options": {
            "A": "Modify the EventBridge event pattern by selecting Amazon S3. Select All Events as the event type.",
            "B": "Modify the EventBridge event pattern by selecting Amazon S3. Select Bucket Level Operations as the event type.",
            "C": "Enable CloudTrail Insights to identify unusual API activity.",
            "D": "Enable CloudTrail to monitor data events for read and write operations to S3 buckets."
        },
        "correctAnswer": "D",
        "explanation":"<p>The correct solution to ensure that the &#96;s3:PutObjectAcl&#96; API call will invoke an EventBridge event, without generating false notifications, is:</p><p><strong>D. Enable CloudTrail to monitor data events for read and write operations to S3 buckets.</strong></p><p>Explanation:</p><p>- <strong>s3:PutObjectAcl</strong> is considered a data event in AWS CloudTrail. AWS CloudTrail management events track actions taken on AWS resources, while data events track operations performed on or within the resource itself, such as S3 object-level activities. By default, CloudTrail logs management events but not data events. To capture &#96;s3:PutObjectAcl&#96; actions, you must explicitly enable data event logging for S3 buckets in CloudTrail.</p><p>The other options are not suitable because:</p><p>- <strong>A. Modifying the EventBridge event pattern by selecting Amazon S3 and selecting All Events as the event type</strong> would not specifically enable logging of &#96;s3:PutObjectAcl&#96; actions if data events are not enabled in CloudTrail. This approach might increase the scope of captured events beyond what is necessary and could still miss &#96;s3:PutObjectAcl&#96; if data events are not logged.</p><p>- <strong>B. Modifying the EventBridge event pattern by selecting Amazon S3 and selecting Bucket Level Operations as the event type</strong> is also not directly addressing the issue. While it's crucial to have the correct event pattern, without enabling data events for S3 in CloudTrail, &#96;s3:PutObjectAcl&#96; operations won't be logged or captured by EventBridge regardless of the event pattern.</p><p>- <strong>C. Enabling CloudTrail Insights to identify unusual API activity</strong> is designed to detect unusual operational activity, not to log specific API calls like &#96;s3:PutObjectAcl&#96;. While it's useful for identifying patterns that might indicate potential security issues, it does not substitute for logging specific data events required to monitor &#96;s3:PutObjectAcl&#96; actions.</p><p>Therefore, enabling CloudTrail to monitor data events for S3 buckets is necessary to capture &#96;s3:PutObjectAcl&#96; API calls and allow EventBridge to trigger events based on these actions, fulfilling the security team's requirement without generating false notifications.</p>"
    },
    {
        "type": "single",
        "question": "Question #74: A company uses Amazon GuardDuty. The company's security team wants all High severity findings to automatically generate a ticket in a third-party ticketing system through email integration. Which solution will meet this requirement?",
        "options": {
            "A": "Create a verified identity for the third-party ticketing email system in Amazon Simple Email Service (Amazon SES). Create an Amazon EventBridge rule that includes an event pattern that matches High severity GuardDuty findings. Specify the SES identity as the target for the EventBridge rule.",
            "B": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the third-party ticketing email system to the SNS topic. Create an Amazon EventBridge rule that includes an event pattern that matches High severity GuardDuty findings. Specify the SNS topic as the target for the EventBridge rule.",
            "C": "Use the GuardDuty CreateFilter API operation to build a filter in GuardDuty to monitor for High severity findings. Export the results of the filter to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the third-party ticketing email system to the SNS topic.",
            "D": "Use the GuardDuty CreateFilter API operation to build a filter in GuardDuty to monitor for High severity findings. Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the third-party ticketing email system to the SNS topic. Create an Amazon EventBridge rule that includes an event pattern that matches GuardDuty findings that are selected by the filter. Specify the SNS topic as the target for the EventBridge rule."
        },
        "correctAnswer": "B",
        "explanation":"<p>The solution that will meet the requirement to automatically generate a ticket in a third-party ticketing system through email integration for High severity findings from Amazon GuardDuty is:</p><p><strong>B. Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the third-party ticketing email system to the SNS topic. Create an Amazon EventBridge rule that includes an event pattern that matches High severity GuardDuty findings. Specify the SNS topic as the target for the EventBridge rule.</strong></p><p>Explanation:</p><p>- <strong>Amazon SNS</strong> is a flexible, fully managed pub/sub messaging and notifications service for coordinating the delivery of messages to subscribing endpoints and clients. By subscribing an email address (belonging to the third-party ticketing system) to an SNS topic, you can forward notifications to that email automatically when messages are published to the topic.</p><p>- <strong>Amazon EventBridge</strong> is a serverless event bus that makes it easy to connect applications together using data from your own applications, integrated Software-as-a-Service (SaaS) applications, and AWS services. EventBridge can be used to route detailed events from AWS services like GuardDuty to various targets, such as AWS Lambda, Amazon SNS topics, and more, based on the content of the events.</p><p>- This solution leverages EventBridge to catch events from GuardDuty, specifically filtering for High severity findings using an event pattern. When such an event occurs, EventBridge forwards the event to the specified SNS topic, which in turn sends an email notification to the subscribed third-party ticketing system's email address, thereby generating a ticket.</p><p>The other options are less suitable because:</p><p>- <strong>A.</strong> While you can use Amazon SES for direct email sending, it's more complex for this use case since SES is primarily for email sending, not for directly integrating with third-party systems based on event triggers from AWS services like GuardDuty. EventBridge directly to SNS is a more straightforward and efficient approach.</p><p>- <strong>C. and D.</strong> The GuardDuty CreateFilter API operation and the detailed integration process described are unnecessary for the task. EventBridge itself can directly filter GuardDuty findings based on severity, making these steps overly complicated for achieving the desired outcome. Plus, the direct use of CreateFilter and additional EventBridge rules as described in option D adds complexity without providing additional benefits for this specific requirement.</p>"
    },
    {
        "type": "single",
        "question": "Question #75: A company is using AWS Organizations to implement a multi-account strategy. The company does not have on-premises infrastructure. All workloads run on AWS. The company currently has eight member accounts. The company anticipates that it will have no more than 20 AWS accounts total at any time. The company issues a new security policy that contains the following requirements: No AWS account should use a VPC within the AWS account for workloads. The company should use a centrally managed VPC that all AWS accounts can access to launch workloads in subnets. No AWS account should be able to modify another AWS account's application resources within the centrally managed VPC. The centrally managed VPC should reside in an existing AWS account that is named Account-A within an organization. The company uses an AWS CloudFormation template to create a VPC that contains multiple subnets in Account-A. This template exports the subnet IDs through the CloudFormation Outputs section. Which solution will complete the security setup to meet these requirements?",
        "options": {
            "A": "Use a CloudFormation template in the member accounts to launch workloads. Configure the template to use the Fn::ImportValue function to obtain the subnet ID values.",
            "B": "Use a transit gateway in the VPC within Account-A. Configure the member accounts to use the transit gateway to access the subnets in Account-A to launch workloads.",
            "C": "Use AWS Resource Access Manager (AWS RAM) to share Account-A's VPC subnets with the remaining member accounts. Configure the member accounts to use the shared subnets to launch workloads.",
            "D": "Create a peering connection between Account-A and the remaining member accounts. Configure the member accounts to use the subnets in Account-A through the VPC peering connection to launch workloads."
        },
        "correctAnswer": "C",
        "explanation":"<p>The solution that will complete the security setup to meet the company's requirements is:</p><p><strong>C. Use AWS Resource Access Manager (AWS RAM) to share Account-A's VPC subnets with the remaining member accounts. Configure the member accounts to use the shared subnets to launch workloads.</strong></p><p>Explanation:</p><p>- <strong>AWS Resource Access Manager (AWS RAM)</strong> enables you to share AWS resources with any AWS account or within your AWS Organization. It is designed to facilitate the sharing of resources like subnets and transit gateways across AWS accounts, making it an ideal choice for scenarios where a centrally managed VPC needs to be accessible by multiple accounts.</p><p>- This solution meets the company's security policy by allowing Account-A to share its VPC subnets with the other member accounts in the organization. This way, all workloads can be launched within the centrally managed VPC's subnets, ensuring compliance with the policy that prohibits the use of individual account VPCs for workloads.</p><p>- AWS RAM ensures that while the subnets are shared, no account can modify another account's resources. This satisfies the requirement that no AWS account should be able to modify another account's application resources within the centrally managed VPC.</p><p>The other options are less suitable because:</p><p>- <strong>A. Using a CloudFormation template with the &#96;Fn::ImportValue&#96; function</strong> to obtain subnet ID values does not inherently provide a way for other accounts to launch resources in a VPC that belongs to a different account. The &#96;Fn::ImportValue&#96; function is used to import values that were exported by another stack within the same account or within the same AWS Region but doesn't enable cross-account resource launching.</p><p>- <strong>B. Using a transit gateway</strong> provides connectivity between VPCs and between a VPC and on-premises networks. While it can facilitate communication across VPCs in different accounts, it does not allow accounts to launch workloads in a centrally managed VPC. The transit gateway is more about network routing and access rather than resource sharing.</p><p>- <strong>D. Creating a peering connection</strong> allows for routing traffic between VPCs in different accounts via private IP addresses. However, it does not permit an account to launch resources in another account's VPC. VPC peering is for network connectivity, not for deploying resources across accounts into a shared VPC.</p><p>Therefore, using AWS RAM to share the VPC subnets from Account-A with other member accounts is the best solution to meet the company's requirements.</p>"
    },
    {
        "type": "single",
        "question": "Question #76: A company's security team needs to receive a notification whenever an AWS access key has not been rotated in 90 or more days. A security engineer must develop a solution that provides these notifications automatically. Which solution will meet these requirements with the LEAST amount of effort?",
        "options": {
            "A": "Deploy an AWS Config managed rule to run on a periodic basis of 24 hours. Select the access-keys-rotated managed rule, and set the maxAccessKeyAge parameter to 90 days. Create an Amazon EventBridge rule with an event pattern that matches the compliance type of NON_COMPLIANT from AWS Config for the managed rule. Configure EventBridge to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.",
            "B": "Create a script to export a .csv file from the AWS Trusted Advisor check for IAM access key rotation. Load the script into an AWS Lambda function that will upload the .csv file to an Amazon S3 bucket. Create an Amazon Athena table query that runs when the .csv file is uploaded to the S3 bucket. Publish the results for any keys older than 90 days by using an invocation of an Amazon Simple Notification Service (Amazon SNS) notification to the security team.",
            "C": "Create a script to download the IAM credentials report on a periodic basis. Load the script into an AWS Lambda function that will run on a schedule through Amazon EventBridge. Configure the Lambda script to load the report into memory and to filter the report for records in which the key was last rotated at least 90 days ago. If any records are detected, send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.",
            "D": "Create an AWS Lambda function that queries the IAM API to list all the users. Iterate through the users by using the ListAccessKeys operation. Verify that the value in the CreateDate field is not at least 90 days old. Send an Amazon Simple Notification Service (Amazon SNS) notification to the security team if the value is at least 90 days old. Create an Amazon EventBridge rule to schedule the Lambda function to run each day."
        },
        "correctAnswer": "A"
    },
    {
        "type": "single",
        "question": "Question #77: A company maintains an open-source application that is hosted on a public GitHub repository. While creating a new commit to the repository, an engineer uploaded their AWS access key and secret access key. The engineer reported the mistake to a manager, and the manager immediately disabled the access key. The company needs to assess the impact of the exposed access key. A security engineer must recommend a solution that requires the least possible managerial overhead. Which solution meets these requirements?",
        "options": {
            "A": "Analyze an AWS Identity and Access Management (IAM) use report from AWS Trusted Advisor to see when the access key was last used.",
            "B": "Analyze Amazon CloudWatch Logs for activity by searching for the access key.",
            "C": "Analyze VPC flow logs for activity by searching for the access key.",
            "D": "Analyze a credential report in AWS Identity and Access Management (IAM) to see when the access key was last used."
        },
        "correctAnswer": "D"
    },
    {
        "type": "single",
        "question": "Question #78: A company plans to create individual child accounts within an existing organization in AWS Organizations for each of its DevOps teams. AWS CloudTrail has been enabled and configured on all accounts to write audit logs to an Amazon S3 bucket in a centralized AWS account. A security engineer needs to ensure that DevOps team members are unable to modify or disable this configuration. How can the security engineer meet these requirements?",
        "options": {
            "A": "Create an IAM policy that prohibits changes to the specific CloudTrail trail and apply the policy to the AWS account root user.",
            "B": "Create an S3 bucket policy in the specified destination account for the CloudTrail trail that prohibits configuration changes from the AWS account root user in the source account.",
            "C": "Create an SCP that prohibits changes to the specific CloudTrail trail and apply the SCP to the appropriate organizational unit or account in Organizations.",
            "D": "Create an IAM policy that prohibits changes to the specific CloudTrail trail and apply the policy to a new IAM group. Have team members use individual IAM accounts that are members of the new IAM group."
        },
        "correctAnswer": "C"
    },
    {
        "type": "single",
        "question": "Question #79: A company's policy requires that all API keys be encrypted and stored separately from source code in a centralized security account. This security account is managed by the company's security team. However, an audit revealed that an API key is stored with the source code of an AWS Lambda function in an AWS CodeCommit repository in the DevOps account. How should the security team securely store the API key?",
        "options": {
            "A": "Create a CodeCommit repository in the security account using AWS Key Management Service (AWS KMS) for encryption. Require the development team to migrate the Lambda source code to this repository.",
            "B": "Store the API key in an Amazon S3 bucket in the security account using server-side encryption with Amazon S3 managed encryption keys (SSE-S3) to encrypt the key. Create a presigned URL for the S3 key, and specify the URL in a Lambda environmental variable in the AWS CloudFormation template. Update the Lambda function code to retrieve the key using the URL and call the API.",
            "C": "Create a secret in AWS Secrets Manager in the security account to store the API key using AWS Key Management Service (AWS KMS) for encryption. Grant access to the IAM role used by the Lambda function so that the function can retrieve the key from Secrets Manager and call the API.",
            "D": "Create an encrypted environment variable for the Lambda function to store the API key using AWS Key Management Service (AWS KMS) for encryption. Grant access to the IAM role used by the Lambda function so that the function can decrypt the key at runtime."
        },
        "correctAnswer": "C"
    },
    {
        "type": "single",
        "question": "Question #80: A security engineer is asked to update an AWS CloudTrail log file prefix for an existing trail. When attempting to save the change in the CloudTrail console, the security engineer receives the following error message: \"There is a problem with the bucket policy.\" What will enable the security engineer to save the change?",
        "options": {
            "A": "Create a new trail with the updated log file prefix, and then delete the original trail. Update the existing bucket policy in the Amazon S3 console with the new log file prefix, and then update the log file prefix in the CloudTrail console.",
            "B": "Update the existing bucket policy in the Amazon S3 console to allow the security engineer's principal to perform PutBucketPolicy, and then update the log file prefix in the CloudTrail console.",
            "C": "Update the existing bucket policy in the Amazon S3 console with the new log file prefix, and then update the log file prefix in the CloudTrail console.",
            "D": "Update the existing bucket policy in the Amazon S3 console to allow the security engineer's principal to perform GetBucketPolicy, and then update the log file prefix in the CloudTrail console."
        },
        "correctAnswer": "C"
    },
    {
        "type": "single",
        "question": "Question #81: A company uses AWS Organizations. The company wants to implement short-term credentials for third-party AWS accounts to use to access accounts within the company's organization. Access is for the AWS Management Console and third-party software-as-a-service (SaaS) applications. Trust must be enhanced to prevent two external accounts from using the same credentials. The solution must require the least possible operational effort. Which solution will meet these requirements?",
        "options": {
            "A": "Use a bearer token authentication with OAuth or SAML to manage and share a central Amazon Cognito user pool across multiple Amazon API Gateway APIs.",
            "B": "Implement AWS IAM Identity Center (AWS Single Sign-On), and use an identity source of choice. Grant access to users and groups from other accounts by using permission sets that are assigned by account.",
            "C": "Create a unique IAM role for each external account. Create a trust policy Use AWS Secrets Manager to create a random external key.",
            "D": "Create a unique IAM role for each external account. Create a trust policy that includes a condition that uses the sts:ExternalId condition key."
        },
        "correctAnswer": "D",
        "explanation": "<p>The solution that will meet the company's requirements for implementing short-term credentials for third-party AWS accounts to access accounts within the company's organization, enhancing trust, and requiring the least possible operational effort is:</p><p><strong>D. Create a unique IAM role for each external account. Create a trust policy that includes a condition that uses the sts:ExternalId condition key.</strong></p><p>Explanation:</p><p>- <strong>IAM Roles and Trust Relationships</strong>: IAM roles allow you to delegate access with defined permissions to entities (users, services, or accounts) without having to share long-term access keys. By creating a unique IAM role for each external account, you can specify the permissions that the external account has within your AWS environment. This meets the requirement for access by third-party AWS accounts.</p><p>- <strong>sts:ExternalId Condition Key</strong>: The &#96;sts:ExternalId&#96; is a condition key in IAM policies that helps to prevent the &quot;confused deputy&quot; problem, where a malicious entity could trick a service into misusing its permissions. The &#96;ExternalId&#96; can be used as a secret between the account that owns the role and the account that is allowed to assume the role, ensuring that only the intended external account can assume the role. This enhances trust by preventing two external accounts from using the same credentials.</p><p>- <strong>Least Operational Effort</strong>: This solution is straightforward to implement and manage, especially when compared to managing individual access keys or implementing complex federated authentication systems. It leverages AWS's built-in capabilities for secure access management.</p><p>The other options are less suitable because:</p><p>- <strong>A. Bearer Token Authentication with OAuth or SAML and Amazon Cognito</strong>: While this solution can manage authentication and integrate with external identity providers, it is more complex and geared towards application-level authentication rather than providing short-term AWS console or API access for third-party accounts. It also does not inherently prevent two external accounts from using the same credentials without additional custom implementation.</p><p>- <strong>B. AWS IAM Identity Center (AWS Single Sign-On)</strong>: While AWS IAM Identity Center simplifies access management for AWS accounts and applications, it is primarily used for managing identities within an organization rather than granting access to third-party AWS accounts. It requires an identity source (like a corporate directory) that third-party accounts might not be part of.</p><p>- <strong>C. IAM Role with AWS Secrets Manager</strong>: Using AWS Secrets Manager to create a random external key is unnecessary for the scenario described and adds operational complexity. Secrets Manager is typically used for managing secrets needed by applications or services, not for managing access to AWS accounts.</p><p>Therefore, option D is the most appropriate solution, as it directly addresses the need for secure, short-term access for third-party accounts with minimal operational complexity.</p>"
    },
    {
        "type": "single",
        "question": "Question #82: A company is evaluating its security posture. In the past, the company has observed issues with specific hosts and host header combinations that affected the company's business. The company has configured AWS WAF web ACLs as an initial step to mitigate these issues. The company must create a log analysis solution for the AWS WAF web ACLs to monitor problematic activity. The company wants to process all the AWS WAF logs in a central location. The company must have the ability to filter out requests based on specific hosts. A security engineer starts to enable access logging for the AWS WAF web ACLs. What should the security engineer do next to meet these requirements with the MOST operational efficiency?",
        "options": {
            "A": "Specify Amazon Redshift as the destination for the access logs. Deploy the Amazon Athena Redshift connector. Use Athena to query the data from Amazon Redshift and to filter the logs by host.",
            "B": "Specify Amazon CloudWatch as the destination for the access logs. Use Amazon CloudWatch Logs Insights to design a query to filter the logs by host.",
            "C": "Specify Amazon CloudWatch as the destination for the access logs. Export the CloudWatch logs to an Amazon S3 bucket. Use Amazon Athena to query the logs and to filter the logs by host.",
            "D": "Specify Amazon CloudWatch as the destination for the access logs. Use Amazon Redshift Spectrum to query the logs and to filter the logs by host."
        },
        "correctAnswer": "B"
    },
    {
        "type": "multi",
        "question": "Question #83: A security engineer is trying to use Amazon EC2 Image Builder to create an image of an EC2 instance. The security engineer has configured the pipeline to send logs to an Amazon S3 bucket. When the security engineer runs the pipeline, the build fails with the following error: \"AccessDenied: Access Denied status code: 403\". The security engineer must resolve the error by implementing a solution that complies with best practices for least privilege access. Which combination of steps will meet these requirements? (Choose two.)",
        "options": {
            "A": "Ensure that the following policies are attached to the IAM role that the security engineer is using: EC2InstanceProfileForImageBuilder, EC2InstanceProfileForImageBuilderECRContainerBuilds, and AmazonSSMManagedInstanceCore.",
            "B": "Ensure that the following policies are attached to the instance profile for the EC2 instance: EC2InstanceProfileForImageBuilder, EC2InstanceProfileForImageBuilderECRContainerBuilds, and AmazonSSMManagedInstanceCore.",
            "C": "Ensure that the AWSImageBuilderFullAccess policy is attached to the instance profile for the EC2 instance.",
            "D": "Ensure that the security engineer's IAM role has the s3:PutObject permission for the S3 bucket.",
            "E": "Ensure that the instance profile for the EC2 instance has the s3:PutObject permission for the S3 bucket."
        },
        "correctAnswer": [
            "B",
            "E"
        ],
        "explanation":"<p>To resolve the &quot;AccessDenied: Access Denied status code: 403&quot; error encountered by the security engineer when using Amazon EC2 Image Builder and ensure compliance with the best practices for least privilege access, the correct combination of steps includes:</p><p>1. <strong>B. Ensure that the following policies are attached to the instance profile for the EC2 instance: EC2InstanceProfileForImageBuilder, EC2InstanceProfileForImageBuilderECRContainerBuilds, and AmazonSSMManagedInstanceCore.</strong>    This step ensures that the EC2 instance being used by Amazon EC2 Image Builder has the necessary permissions to perform image building tasks, including interactions with Amazon ECR for container builds and Amazon SSM for managed instance operations. These permissions are crucial for the image building process and for logging activities to succeed.</p><p>2. <strong>E. Ensure that the instance profile for the EC2 instance has the s3:PutObject permission for the S3 bucket.</strong></p><p> Since the build process involves sending logs to an Amazon S3 bucket, it's essential that the EC2 instance (through its instance profile) has permissions to write to the specified S3 bucket. The &#96;s3:PutObject&#96; permission specifically allows the instance to upload logs to the bucket, addressing the &quot;Access Denied&quot; error encountered during the build process.</p><p>The other options are less relevant or incorrectly targeted:</p><p>- <strong>A.</strong> While ensuring the IAM role used by the security engineer has specific policies can be part of a comprehensive access strategy, the error message indicates an issue with accessing the S3 bucket during the build process, which is more directly addressed by configuring permissions on the instance profile used by the EC2 instance involved in the build.</p><p>- <strong>C.</strong> Attaching the &#96;AWSImageBuilderFullAccess&#96; policy to the instance profile might provide broader permissions than necessary for the specific task of sending logs to an S3 bucket and does not directly address the &quot;Access Denied&quot; error related to S3 bucket access.</p><p>- <strong>D.</strong> While ensuring the security engineer's IAM role has &#96;s3:PutObject&#96; permission for the S3 bucket might seem relevant, the error is more likely related to the permissions of the EC2 instance (or the role it assumes) that is performing the build and attempting to log to S3, rather than the permissions of the security engineer's IAM role.</p>"
    },
    {
        "type": "single",
        "question": "Question #84: A security engineer must use AWS Key Management Service (AWS KMS) to design a key management solution for a set of Amazon Elastic Block Store (Amazon EBS) volumes that contain sensitive data. The solution needs to ensure that the key material automatically expires in 90 days. Which solution meets these criteria?",
        "options": {
            "A": "A customer managed key that uses customer provided key material",
            "B": "A customer managed key that uses AWS provided key material",
            "C": "An AWS managed key",
            "D": "Operating system encryption that uses GnuPG"
        },
        "correctAnswer": "A",
        "explanation":"<p>The correct solution to ensure that key material automatically expires in 90 days for encrypting Amazon Elastic Block Store (Amazon EBS) volumes containing sensitive data using AWS Key Management Service (AWS KMS) is:</p><p><strong>A. A customer managed key that uses customer provided key material</strong></p><p>Explanation:</p><p>- <strong>A. A customer managed key that uses customer provided key material:</strong> AWS KMS allows customers to import their own key material for use with customer managed keys. One of the features of importing your own key material into AWS KMS is the ability to set an expiration date for the key material. After the key material expires, it cannot be used for cryptographic operations, which meets the requirement for the key material to automatically expire in 90 days. This option provides the greatest flexibility in managing the lifecycle of the key material, including its expiration.</p><p>- <strong>B. A customer managed key that uses AWS provided key material:</strong> While customer managed keys offer more management features and capabilities than AWS managed keys, including key rotation and usage policies, they do not inherently include a feature to automatically expire the key material based on a set timeframe like 90 days. The key material provided by AWS does not have an expiration feature that can be directly controlled in such a manner.</p><p>- <strong>C. An AWS managed key:</strong> AWS managed keys are managed and rotated by AWS. Users have limited management capabilities over these keys, such as setting the rotation policy. However, users cannot control the expiration of the key material or import their own key material.</p><p>- <strong>D. Operating system encryption that uses GnuPG:</strong> While this method allows for encryption within the operating system using a tool like GnuPG and could be configured to use keys that expire, it does not leverage AWS KMS for managing the encryption keys. This option would not utilize the integrated AWS service designed for such purposes and requires manual management of keys and their expiration outside of AWS KMS.</p><p>Therefore, the best solution given the requirement for automatic expiration of key material in 90 days is to use a customer managed key with customer provided key material, which allows for the specification of an expiration date for the imported key material.</p>"
    },
    {
        "type": "multi",
        "question": "Question #85: A security engineer is building a Java application that is running on Amazon EC2. The application communicates with an Amazon RDS instance and authenticates with a user name and password. Which combination of steps can the engineer take to protect the credentials and minimize downtime when the credentials are rotated? (Choose two.)",
        "options": {
            "A": "Have a database administrator encrypt the credentials and store the ciphertext in Amazon S3. Grant permission to the instance role associated with the EC2 instance to read the object and decrypt the ciphertext.",
            "B": "Configure a scheduled job that updates the credential in AWS Systems Manager Parameter Store and notifies the engineer that the application needs to be restarted.",
            "C": "Configure automatic rotation of credentials in AWS Secrets Manager.",
            "D": "Store the credential in an encrypted string parameter in AWS Systems Manager Parameter Store. Grant permission to the instance role associated with the EC2 instance to access the parameter and the AWS KMS key that is used to encrypt it.",
            "E": "Configure the Java application to catch a connection failure and make a call to AWS Secrets Manager to retrieve updated credentials when the password is rotated. Grant permission to the instance role associated with the EC2 instance to access Secrets Manager."
        },
        "correctAnswer": [
            "C",
            "E"
        ],
        "explanation":"<p>For the scenario of a Java application on Amazon EC2 communicating with an Amazon RDS instance and needing secure management of credentials with minimal downtime during rotation, the best practices involve using AWS Secrets Manager for automatic rotation and coding the application to gracefully handle credential updates. Thus, the correct options are:</p><p>- <strong>C. Configure automatic rotation of credentials in AWS Secrets Manager.</strong> AWS Secrets Manager is specifically designed to manage, rotate, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. The automatic rotation feature helps ensure that credentials are regularly updated without manual intervention, significantly reducing the risk of unauthorized access due to compromised or stale credentials.</p><p>- <strong>E. Configure the Java application to catch a connection failure and make a call to AWS Secrets Manager to retrieve updated credentials when the password is rotated.</strong> This approach allows the application to dynamically update its credentials without needing a restart. By handling connection failures as signals to fetch new credentials, the application can maintain uptime and reduce the potential impact of credential rotation on the service availability.</p><p>The reasons the other options are less suitable or incorrect are:</p><p>- <strong>A. Encrypting credentials and storing them in Amazon S3</strong> is not the best practice for several reasons. Firstly, this approach requires manual management of encryption keys and the process of encrypting and decrypting credentials, which introduces complexity and potential security risks. Additionally, it does not provide built-in mechanisms for rotation or automatic retrieval of updated credentials, making it a less secure and less efficient option compared to using AWS Secrets Manager.</p><p>- <strong>B. Configuring a scheduled job that updates the credential in AWS Systems Manager Parameter Store and notifies the engineer that the application needs to be restarted</strong> is not ideal because it involves downtime for the application. Restarting the application to update credentials is disruptive and can affect service availability. Moreover, this approach requires manual intervention or additional automation to handle the restart, which is not as seamless as an application logic that automatically fetches updated credentials.</p><p>- <strong>D. Storing the credential in AWS Systems Manager Parameter Store</strong> is a valid approach for securely managing secrets. However, by itself, it does not support automatic rotation of RDS credentials, unlike AWS Secrets Manager. While AWS Systems Manager Parameter Store can store and encrypt credentials securely, the lack of native support for automatic rotation makes it less suitable for scenarios where minimizing operational effort and ensuring credentials are rotated frequently and securely is crucial.</p><p>In summary, options C and E directly address the need for secure credential management with automatic rotation and minimal downtime through dynamic credential retrieval, aligning with best practices for security and operational efficiency in cloud applications.</p>"
    },
    {
        "type": "multi",
        "question": "Question #86: A company uses SAML federation to grant users access to AWS accounts. A company workload that is in an isolated AWS account runs on immutable infrastructure with no human access to Amazon EC2. The company requires a specialized user known as a break glass user to have access to the workload AWS account and instances in the case of SAML errors. A recent audit discovered that the company did not create the break glass user for the AWS account that contains the workload. The company must create the break glass user. The company must log any activities of the break glass user and send the logs to a security team. Which combination of solutions will meet these requirements? (Choose two.)",
        "options": {
            "A": "Create a local individual break glass IAM user for the security team. Create a trail in AWS CloudTrail that has Amazon CloudWatch Logs turned on. Use Amazon EventBridge to monitor local user activities.",
            "B": "Create a break glass EC2 key pair for the AWS account. Provide the key pair to the security team. Use AWS CloudTrail to monitor key pair activity. Send notifications to the security team by using Amazon Simple Notification Service (Amazon SNS).",
            "C": "Create a break glass IAM role for the account. Allow security team members to perform the AssumeRoleWithSAML operation. Create an AWS CloudTrail trail that has Amazon CloudWatch Logs turned on. Use Amazon EventBridge to monitor security team activities.",
            "D": "Create a local individual break glass IAM user on the operating system level of each workload instance. Configure unrestricted security groups on the instances to grant access to the break glass IAM users.",
            "E": "Configure AWS Systems Manager Session Manager for Amazon EC2. Configure an AWS CloudTrail filter based on Session Manager. Send the results to an Amazon Simple Notification Service (Amazon SNS) topic."
        },
        "correctAnswer": [
            "A",
            "E"
        ],
        "explanation":"<p>To address the requirements for creating a break glass user in the scenario provided, while also ensuring that any activities of this user are logged and sent to the security team, the combination of solutions that will meet these requirements are:</p><p>1. <strong>A. Create a local individual break glass IAM user for the security team. Create a trail in AWS CloudTrail that has Amazon CloudWatch Logs turned on. Use Amazon EventBridge to monitor local user activities.</strong>    - <strong>Why it meets requirements:</strong> Creating a local IAM user specifically for break glass scenarios provides a direct method for the security team to access the AWS account in case of SAML errors or other emergency situations. Using AWS CloudTrail to log activities and Amazon CloudWatch Logs for storage, coupled with Amazon EventBridge for monitoring, ensures comprehensive oversight of the break glass user&rsquo;s activities. This setup adheres to the need for accountability and transparency in accessing resources under exceptional circumstances.</p><p>2. <strong>E. Configure AWS Systems Manager Session Manager for Amazon EC2. Configure an AWS CloudTrail filter based on Session Manager. Send the results to an Amazon Simple Notification Service (Amazon SNS) topic.</strong>    - <strong>Why it meets requirements:</strong> AWS Systems Manager Session Manager allows for secure instance management without the need for SSH keys or direct instance access, minimizing the security risks associated with traditional SSH access. By configuring Session Manager, the security team can access instances securely. The integration with CloudTrail for logging and SNS for notifications ensures that the security team is alerted to any break glass activities, maintaining oversight and compliance with security policies.</p><p><strong>Why the other options are less suitable:</strong></p><p>- <strong>B. Create a break glass EC2 key pair for the AWS account.</strong> This approach is not recommended for IAM user management and does not directly address the audit's findings regarding the creation of a break glass user within IAM. Key pairs are used for EC2 instance access and don't provide the centralized control or logging of IAM activities.</p><p>- <strong>C. Create a break glass IAM role for the account with AssumeRoleWithSAML.</strong> While creating an IAM role is a good practice for cross-account access or assuming roles within the same account, it relies on SAML for authentication. In the case of SAML errors, which this break glass account is specifically meant to address, this solution would not be viable.</p><p>- <strong>D. Create a local individual break glass IAM user on the operating system level of each workload instance.</strong> This option mixes concepts; IAM users are AWS-level entities and do not directly correspond to operating system-level access. Furthermore, configuring unrestricted security groups poses a significant security risk and goes against the principle of least privilege.</p><p>The chosen solutions (A and E) provide a balanced approach, offering both AWS-level access through an IAM user and secure instance-level access via Systems Manager Session Manager, each with appropriate logging and alerting mechanisms.</p>"
    },
    {
        "type": "multi",
        "question": "Question #87: A security engineer is working with a product team building a web application on AWS. The application uses Amazon S3 to host the static content, Amazon API Gateway to provide RESTful services, and Amazon DynamoDB as the backend data store. The users already exist in a directory that is exposed through a SAML identity provider. Which combination of the following actions should the engineer take to allow users to be authenticated into the web application and call APIs? (Choose three.)",
        "options": {
            "A": "Create a custom authorization service using AWS Lambda.",
            "B": "Configure a SAML identity provider in Amazon Cognito to map attributes to the Amazon Cognito user pool attributes.",
            "C": "Configure the SAML identity provider to add the Amazon Cognito user pool as a relying party.",
            "D": "Configure an Amazon Cognito identity pool to integrate with social login providers.",
            "E": "Update DynamoDB to store the user email addresses and passwords.",
            "F": "Update API Gateway to use a COGNITO_USER_POOLS authorizer."
        },
        "correctAnswer": [
            "B",
            "C",
            "F"
        ],
        "explanation": "<p>To allow users to be authenticated into the web application and call APIs, while leveraging a SAML identity provider and integrating with AWS services like Amazon S3, Amazon API Gateway, and Amazon DynamoDB, the security engineer should take the following actions:</p><p>1. <strong>Configure a SAML identity provider in Amazon Cognito to map attributes to the Amazon Cognito user pool attributes (Option B).</strong> Amazon Cognito allows for the integration of SAML-based identity providers with user pools. By configuring a SAML identity provider and mapping its attributes to Amazon Cognito user pool attributes, the application can authenticate users using their existing directory credentials. This setup provides a seamless authentication experience for users and leverages the existing identity management infrastructure.</p><p>2. <strong>Configure the SAML identity provider to add the Amazon Cognito user pool as a relying party (Option C).</strong> This step involves configuring the SAML identity provider to recognize the Amazon Cognito user pool as a valid entity that can request authentication assertions. This configuration ensures that authentication requests from the application, routed through Amazon Cognito, are accepted and processed by the SAML identity provider, thereby authenticating users against the directory that is exposed through the SAML provider.</p><p>3. <strong>Update API Gateway to use a COGNITO_USER_POOLS authorizer (Option F).</strong> After setting up Amazon Cognito to handle authentication with the SAML identity provider, the next step is to configure Amazon API Gateway to use an Amazon Cognito user pool as the authorizer. This configuration ensures that only authenticated users can call the APIs, securing access to the backend resources and services provided by the application.</p><p>The other options are not suitable for the described scenario:</p><p>- <strong>Create a custom authorization service using AWS Lambda (Option A).</strong> While AWS Lambda can be used to create a custom authorizer for API Gateway, in this scenario, using Amazon Cognito with a SAML identity provider is a more streamlined and integrated approach for authentication, reducing the need for custom development.</p><p>- <strong>Configure an Amazon Cognito identity pool to integrate with social login providers (Option D).</strong> This option is relevant for scenarios where social identity providers (e.g., Facebook, Google, Amazon) are used for authentication. Since the requirement is to authenticate users from a SAML-based directory, this option does not apply.</p><p>- <strong>Update DynamoDB to store the user email addresses and passwords (Option E).</strong> Storing user credentials directly in DynamoDB is not a recommended practice due to security concerns. It is better to handle authentication through secure, dedicated services like Amazon Cognito and SAML identity providers, which are designed to securely manage user identities and credentials.</p>"
    },
    {
        "type": "multi",
        "question": "Question #88: A company needs to improve its ability to identify and prevent IAM policies that grant public access or cross-account access to resources. The company has implemented AWS Organizations and has started using AWS Identity and Access Management Access Analyzer to refine overly broad access to accounts in the organization. A security engineer must automate a response in the company's organization for any newly created policies that are overly permissive. The automation must remediate external access and must notify the company's security team. Which combination of steps should the security engineer take to meet these requirements? (Choose three.)",
        "options": {
            "A": "Create an AWS Step Functions state machine that checks the resource type in the finding and adds an explicit Deny statement in the trust policy for the IAM role. Configure the state machine to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic.",
            "B": "Create an AWS Batch job that forwards any resource type findings to an AWS Lambda function. Configure the Lambda function to add an explicit Deny statement in the trust policy for the IAM role. Configure the AWS Batch job to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic.",
            "C": "In Amazon EventBridge, create an event rule that matches active IAM Access Analyzer findings and invokes AWS Step Functions for resolution.",
            "D": "In Amazon CloudWatch, create a metric filter that matches active IAM Access Analyzer findings and invokes AWS Batch for resolution.",
            "E": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the queue to forward a notification to the security team that an external principal has been granted access to the specific IAM role and has been blocked.",
            "F": "Create an Amazon Simple Notification Service (Amazon SNS) topic for external or cross-account access notices. Subscribe the security team's email addresses to the topic."
        },
        "correctAnswer": [
            "A",
            "C",
            "F"
        ],
        "explanation":"<p>To automate the response to newly created IAM policies that are overly permissive, addressing both the remediation of external access and notification of the security team, the security engineer should take the following steps:</p><p>1. <strong>In Amazon EventBridge, create an event rule that matches active IAM Access Analyzer findings and invokes AWS Step Functions for resolution (Option C).</strong> EventBridge can detect and respond to changes in AWS resources, including findings from IAM Access Analyzer. By creating an event rule that listens for Access Analyzer findings indicating overly permissive policies, and invoking Step Functions, the engineer can automate the process of assessing and remediating these policies.</p><p>2. <strong>Create an AWS Step Functions state machine that checks the resource type in the finding and adds an explicit Deny statement in the trust policy for the IAM role. Configure the state machine to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic (Option A).</strong> AWS Step Functions allows for the orchestration of complex workflows, including the conditional logic required to assess Access Analyzer findings and apply appropriate remediations. By adding explicit Deny statements to overly broad trust policies, the state machine can effectively narrow down permissions. Additionally, integrating notification via an SNS topic ensures that the security team is informed of the remediation actions taken.</p><p>3. <strong>Create an Amazon Simple Notification Service (Amazon SNS) topic for external or cross-account access notices. Subscribe the security team's email addresses to the topic (Option F).</strong> SNS is a flexible, fully-managed messaging service that can distribute messages to subscribed endpoints, such as email addresses. By creating a dedicated topic for notices related to external or cross-account access and subscribing the security team to this topic, the engineer ensures that the team is promptly informed of any findings and remediation actions, facilitating rapid response and further investigation if necessary.</p><p>The other options are not as suitable:</p><p>- <strong>Option B (Create an AWS Batch job)</strong> introduces unnecessary complexity and is not directly suited for real-time event-driven remediation and notification in response to IAM Access Analyzer findings.</p><p>- <strong>Option D (In Amazon CloudWatch, create a metric filter)</strong> is not directly applicable to handling IAM Access Analyzer findings, as CloudWatch is primarily focused on monitoring and observability, rather than event-driven automation or policy remediation.</p><p>- <strong>Option E (Create an Amazon Simple Queue Service (Amazon SQS) queue)</strong> could be part of a larger solution but on its own does not address the immediate need for remediation or the direct notification of security team as effectively as the selected options. SQS is typically used for decoupling components of a cloud application, rather than for immediate action and notification in security use cases.</p>"
    },
    {
        "type": "single",
        "question": "Question #89: A security engineer is configuring a mechanism to send an alert when three or more failed sign-in attempts to the AWS Management Console occur during a 5-minute period. The security engineer creates a trail in AWS CloudTrail to assist in this work. Which solution will meet these requirements?",
        "options": {
            "A": "In CloudTrail, turn on Insights events on the trail. Configure an alarm on the insight with eventName matching ConsoleLogin and errorMessage matching \"Failed authentication''. Configure a threshold of 3 and a period of 5 minutes.",
            "B": "Configure CloudTrail to send events to Amazon CloudWatch Logs. Create a metric filter for the relevant log group. Create a filter pattern with eventName matching ConsoleLogin and errorMessage matching \"Failed authentication\". Create a CloudWatch alarm with a threshold of 3 and a period of 5 minutes.",
            "C": "Create an Amazon Athena table from the CloudTrail events. Run a query for eventName matching ConsoleLogin and for errorMessage matching \"Failed authentication\". Create a notification action from the query to send an Amazon Simple Notification Service (Amazon SNS) notification when the count equals 3 within a period of 5 minutes.",
            "D": "In AWS Identity and Access Management Access Analyzer, create a new analyzer. Configure the analyzer to send an Amazon Simple Notification Service (Amazon SNS) notification when a failed sign-in event occurs 3 times for any IAM user within a period of 5 minutes."
        },
        "correctAnswer": "B"
    },
    {
        "type": "multi",
        "question": "Question #90: A company's security engineer is developing an incident response plan to detect suspicious activity in an AWS account for VPC hosted resources. The security engineer needs to provide visibility for as many AWS Regions as possible. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
        "options": {
            "A": "Turn on VPC Flow Logs for all VPCs in the account.",
            "B": "Activate Amazon GuardDuty across all AWS Regions.",
            "C": "Activate Amazon Detective across all AWS Regions.",
            "D": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Create an Amazon EventBridge rule that responds to findings and publishes the findings to the SNS topic.",
            "E": "Create an AWS Lambda function. Create an Amazon EventBridge rule that invokes the Lambda function to publish findings to Amazon Simple Email Service (Amazon SES)."
        },
        "correctAnswer": [
            "B",
            "D"
        ]
    },
    {
        "type": "multi",
        "question": "Question #91: A company stores images for a website in an Amazon S3 bucket. The company is using Amazon CloudFront to serve the images to end users. The company recently discovered that the images are being accessed from countries where the company does not have a distribution license. Which actions should the company take to secure the images to limit their distribution? (Choose two.)",
        "options": {
            "A": "Update the S3 bucket policy to restrict access to a CloudFront origin access control (OAC).",
            "B": "Update the website DNS record to use an Amazon Route 53 geolocation record deny list of countries where the company lacks a license.",
            "C": "Add a CloudFront geo restriction deny list of countries where the company lacks a license.",
            "D": "Update the S3 bucket policy with a deny list of countries where the company lacks a license.",
            "E": "Enable the Restrict Viewer Access option in CloudFront to create a deny list of countries where the company lacks a license."
        },
        "correctAnswer": [
            "A",
            "C"
        ],
        "explanation":"<p>To secure the images stored in an Amazon S3 bucket and limit their distribution to comply with licensing agreements, the company should take the following actions:</p><p>1. <strong>Update the S3 bucket policy to restrict access to a CloudFront origin access control (OAC) (Option A).</strong> This action ensures that the images in the S3 bucket can only be accessed through CloudFront, effectively blocking direct access to the S3 bucket. By using CloudFront Origin Access Control, the company can ensure that the S3 bucket is not publicly accessible and that content is only served through CloudFront, where further restrictions can be applied.</p><p>2. <strong>Add a CloudFront geo restriction deny list of countries where the company lacks a license (Option C).</strong> CloudFront geo restriction (also known as geoblocking) allows the company to restrict access to the content based on the geographic location of the end users. By configuring a deny list of countries where the company does not have distribution rights, CloudFront will block requests from those locations, ensuring compliance with licensing agreements.</p><p>The other options are less suitable or not feasible for the described scenario:</p><p>- <strong>Update the website DNS record to use an Amazon Route 53 geolocation record deny list of countries where the company lacks a license (Option B).</strong> While Amazon Route 53 can route traffic based on geographic location, it does not prevent users in denied locations from accessing content if they can bypass DNS settings or if the content is cached elsewhere. This method does not provide the same level of access control specific to the content being served through CloudFront.</p><p>- <strong>Update the S3 bucket policy with a deny list of countries where the company lacks a license (Option D).</strong> S3 bucket policies do not have the capability to deny access based on the geographic location of the requester. Access control based on country is a feature specific to services like CloudFront that can interpret the location of a request.</p><p>- <strong>Enable the Restrict Viewer Access option in CloudFront to create a deny list of countries where the company lacks a license (Option E).</strong> The &quot;Restrict Viewer Access&quot; option in CloudFront is used for serving private content with signed URLs or signed cookies, not for creating a deny list of countries. Geo restriction is the correct feature for blocking access from specific countries.</p>"
    },
    {
        "type": "single",
        "question": "Question #92: A company has deployed servers on Amazon EC2 instances in a VPC. External vendors access these servers over the internet. Recently, the company deployed a new application on EC2 instances in a new CIDR range. The company needs to make the application available to the vendors. A security engineer verified that the associated security groups and network ACLs are allowing the required ports in the inbound direction. However, the vendors cannot connect to the application. Which solution will provide the vendors access to the application?",
        "options": {
            "A": "Modify the security group that is associated with the EC2 instances to have the same outbound rules as inbound rules.",
            "B": "Modify the network ACL that is associated with the CIDR range to allow outbound traffic to ephemeral ports.",
            "C": "Modify the inbound rules on the internet gateway to allow the required ports.",
            "D": "Modify the network ACL that is associated with the CIDR range to have the same outbound rules as inbound rules."
        },
        "correctAnswer": "B",
        "explanation":"<p>The solution that will provide the vendors access to the application, given that inbound rules are already configured correctly but vendors still cannot connect, is:</p><p><strong>B. Modify the network ACL that is associated with the CIDR range to allow outbound traffic to ephemeral ports.</strong></p><p>Explanation:</p><p>- <strong>Network Access Control Lists (NACLs)</strong> are an optional layer of security for your VPC that act as a firewall for controlling traffic in and out of one or more subnets. Unlike Security Groups, NACLs are stateless; therefore, they require both inbound and outbound rules to be explicitly defined to allow response traffic from established connections to flow.</p><p>- When external vendors initiate connections to the servers, the return traffic from the servers back to the vendors will use ephemeral ports. These are temporary ports assigned by the server's operating system for the duration of the session. If outbound traffic to these ephemeral ports is not allowed by the NACLs associated with the subnet where the EC2 instances are located, the response packets from the servers cannot reach the vendors, thus preventing successful connections.</p><p>- <strong>Modifying the NACLs to allow outbound traffic to ephemeral ports</strong> addresses this issue by ensuring that responses to inbound requests can be sent back to the clients. The range of ephemeral ports can vary, but commonly used ranges include 1024-65535 for Linux and Unix systems.</p><p>The other options are less suitable or incorrect because:</p><p>- <strong>A. Modifying the security group to have the same outbound rules as inbound rules:</strong> Security groups are stateful, meaning they automatically allow return traffic for allowed inbound connections, regardless of outbound rules. Changing outbound rules in this context would not address the issue if inbound connections are already allowed.</p><p>- <strong>C. Modifying the inbound rules on the internet gateway to allow the required ports:</strong> The internet gateway does not have &quot;rules&quot; that can be modified in this way. It simply routes traffic between the internet and the VPC. The connectivity issue described is not related to internet gateway configurations.</p><p>- <strong>D. Modifying the network ACL to have the same outbound rules as inbound rules:</strong> This option might seem correct, but it's too general. The specific action needed is to allow outbound traffic to ephemeral ports, which is a subset of potentially matching outbound rules to inbound rules. The essential point is to ensure that the NACL allows outbound traffic necessary for the return communication, which typically involves ephemeral ports.</p>"
    },
    {
        "type": "single",
        "question": "Question #93: A company uses infrastructure as code (IaC) to create AWS infrastructure. The company writes the code as AWS CloudFormation templates to deploy the infrastructure. The company has an existing CI/CD pipeline that the company can use to deploy these templates. After a recent security audit, the company decides to adopt a policy-as-code approach to improve the company's security posture on AWS. The company must prevent the deployment of any infrastructure that would violate a security policy, such as an unencrypted Amazon Elastic Block Store (Amazon EBS) volume. Which solution will meet these requirements?",
        "options": {
            "A": "Turn on AWS Trusted Advisor. Configure security notifications as webhooks in the preferences section of the CI/CD pipeline.",
            "B": "Turn on AWS Config. Use the prebuilt rules or customized rules. Subscribe the CI/CD pipeline to an Amazon Simple Notification Service (Amazon SNS) topic that receives notifications from AWS Config.",
            "C": "Create rule sets in AWS CloudFormation Guard. Run validation checks for CloudFormation templates as a phase of the CI/CD process.",
            "D": "Create rule sets as SCPs. Integrate the SCPs as a part of validation control in a phase of the CI/CD process."
        },
        "correctAnswer": "C",
        "explanation":"<p>To meet the requirements of adopting a policy-as-code approach and preventing the deployment of any infrastructure that would violate a security policy, such as deploying an unencrypted Amazon EBS volume, the most suitable solution is:</p><p><strong>C. Create rule sets in AWS CloudFormation Guard. Run validation checks for CloudFormation templates as a phase of the CI/CD process.</strong></p><p>Explanation:</p><p>- <strong>AWS CloudFormation Guard</strong> is a command line tool that helps you enforce compliance and governance of your AWS resources by allowing you to write declarative rules for AWS CloudFormation templates. By creating rule sets with CloudFormation Guard, you can define policies that must be adhered to, such as requiring that all EBS volumes be encrypted. Integrating these validation checks directly into your CI/CD pipeline allows for automated enforcement of your security policies at the infrastructure as code (IaC) level before the infrastructure is deployed. This approach helps ensure that only compliant infrastructure is provisioned, effectively preventing security policy violations.</p><p>The other options are less suitable for directly preventing the deployment of non-compliant infrastructure:</p><p>- <strong>A. AWS Trusted Advisor</strong> provides best practice recommendations across your AWS accounts in five categories: cost optimization, performance, security, fault tolerance, and service limits. While it can identify unencrypted EBS volumes among other recommendations, it does so post-deployment and is not a preventive measure within a CI/CD pipeline.</p><p>- <strong>B. AWS Config</strong> monitors and records AWS resource configurations and allows you to evaluate the recorded configurations against desired configurations. Using AWS Config with prebuilt or custom rules can help identify non-compliant resources, but this happens after resources have been deployed. While AWS Config can trigger notifications for non-compliant resources, it is not inherently a preventive measure within the CI/CD pipeline for stopping the deployment of non-compliant resources.</p><p>- <strong>D. Service Control Policies (SCPs)</strong> are used in AWS Organizations to manage permissions in member accounts. While SCPs can prevent certain actions from being taken (like preventing the creation of unencrypted EBS volumes), they operate at the AWS account level rather than the infrastructure code level. Integrating SCPs into a CI/CD pipeline validation process is not directly applicable, as SCPs are not designed to analyze or validate CloudFormation templates before deployment.</p><p>Therefore, using AWS CloudFormation Guard to create rule sets and integrating these checks into the CI/CD process is the most effective way to ensure compliance with security policies during the deployment of AWS infrastructure.</p>"
    },
    {
        "type": "single",
        "question": "Question #94: A company is running an Amazon RDS for MySQL DB instance in a VPC. The VPC must not send or receive network traffic through the internet. A security engineer wants to use AWS Secrets Manager to rotate the DB instance credentials automatically. Because of a security policy, the security engineer cannot use the standard AWS Lambda function that Secrets Manager provides to rotate the credentials. The security engineer deploys a custom Lambda function in the VPC. The custom Lambda function will be responsible for rotating the secret in Secrets Manager. The security engineer edits the DB instance's security group to allow connections from this function. When the function is invoked, the function cannot communicate with Secrets Manager to rotate the secret properly. What should the security engineer do so that the function can rotate the secret?",
        "options": {
            "A": "Add an egress-only internet gateway to the VPC. Allow only the Lambda function's subnet to route traffic through the egress-only internet gateway.",
            "B": "Add a NAT gateway to the VPC. Configure only the Lambda function's subnet with a default route through the NAT gateway.",
            "C": "Configure a VPC peering connection to the default VPC for Secrets Manager. Configure the Lambda function's subnet to use the peering connection for routes.",
            "D": "Configure a Secrets Manager interface VPC endpoint. Include the Lambda function's private subnet during the configuration process."
        },
        "correctAnswer": "D",
        "explanation":"<p>To enable the custom Lambda function in the VPC to communicate with AWS Secrets Manager for rotating the DB instance credentials automatically, while adhering to the policy of not sending or receiving network traffic through the internet, the security engineer should:</p><p><strong>D. Configure a Secrets Manager interface VPC endpoint. Include the Lambda function's private subnet during the configuration process.</strong></p><p>Explanation:</p><p>- <strong>AWS Secrets Manager interface VPC endpoint</strong> allows you to securely connect your VPC to AWS Secrets Manager without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. VPC endpoints are powered by AWS PrivateLink, a technology that enables you to privately access Secrets Manager APIs by using private IP addresses. By creating a VPC endpoint for Secrets Manager, the Lambda function deployed within the VPC can securely access Secrets Manager to rotate the secret without the need to traverse the public internet.</p><p>The other options are less suitable or incorrect given the security requirements:</p><p>- <strong>A. Add an egress-only internet gateway to the VPC.</strong> This option is not applicable because egress-only internet gateways are used in VPCs with IPv6 to allow outbound internet traffic and prevent inbound traffic. It does not help in scenarios where the VPC must not send or receive network traffic through the internet, and it's specifically designed for IPv6, not IPv4, which is typically used by AWS services.</p><p>- <strong>B. Add a NAT gateway to the VPC.</strong> While a NAT gateway allows instances in a private subnet to initiate outbound IPv4 traffic to the internet or other AWS services, it violates the policy that the VPC must not send or receive network traffic through the internet. Moreover, using a NAT gateway incurs additional costs and does not align with the requirement to avoid internet connectivity.</p><p>- <strong>C. Configure a VPC peering connection to the default VPC for Secrets Manager.</strong> This option is incorrect because AWS Secrets Manager does not have a &quot;default VPC.&quot; VPC peering connections are used to connect two VPCs to allow traffic to route between them, and it's not a method to provide access to AWS services like Secrets Manager without internet access. The appropriate solution for direct service access is through VPC endpoints.</p>"
    },
    {
        "type": "multi",
        "question": "Question #95: The security engineer is managing a traditional three-tier web application that is running on Amazon EC2 instances. The application has become the target of increasing numbers of malicious attacks from the internet. What steps should the security engineer take to check for known vulnerabilities and limit the attack surface? (Choose two.)",
        "options": {
            "A": "Use AWS Certificate Manager to encrypt all traffic between the client and application servers.",
            "B": "Review the application security groups to ensure that only the necessary ports are open.",
            "C": "Use Elastic Load Balancing to offload Secure Sockets Layer encryption.",
            "D": "Use Amazon Inspector to periodically scan the backend instances.",
            "E": "Use AWS Key Management Service (AWS KMS) to encrypt all the traffic between the client and application servers."
        },
        "correctAnswer": [
            "B",
            "D"
        ]
    },
    {
        "type": "single",
        "question": "Question #96: A company is using Amazon Elastic Container Service (Amazon ECS) to run its container-based application on AWS. The company needs to ensure that the container images contain no severe vulnerabilities. The company also must ensure that only specific IAM roles and specific AWS accounts can access the container images. Which solution will meet these requirements with the LEAST management overhead?",
        "options": {
            "A": "Pull images from the public container registry. Publish the images to Amazon Elastic Container Registry (Amazon ECR) repositories with scan on push configured in a centralized AWS account. Use a CI/CD pipeline to deploy the images to different AWS accounts. Use identity-based policies to restrict access to which IAM principals can access the images.",
            "B": "Pull images from the public container registry. Publish the images to a private container registry that is hosted on Amazon EC2 instances in a centralized AWS account. Deploy host-based container scanning tools to EC2 instances that run Amazon ECS. Restrict access to the container images by using basic authentication over HTTPS.",
            "C": "Pull images from the public container registry. Publish the images to Amazon Elastic Container Registry (Amazon ECR) repositories with scan on push configured in a centralized AWS account. Use a CI/CD pipeline to deploy the images to different AWS accounts. Use repository policies and identity-based policies to restrict access to which IAM principals and accounts can access the images.",
            "D": "Pull images from the public container registry. Publish the images to AWS CodeArtifact repositories in a centralized AWS account. Use a CI/CD pipeline to deploy the images to different AWS accounts. Use repository policies and identity-based policies to restrict access to which IAM principals and accounts can access the images."
        },
        "correctAnswer": "C",
        "explanation":"<p>The solution that meets the requirements with the least management overhead is:</p><p><strong>C. Pull images from the public container registry. Publish the images to Amazon Elastic Container Registry (Amazon ECR) repositories with scan on push configured in a centralized AWS account. Use a CI/CD pipeline to deploy the images to different AWS accounts. Use repository policies and identity-based policies to restrict access to which IAM principals and accounts can access the images.</strong></p><p>Explanation:</p><p>- <strong>Amazon Elastic Container Registry (Amazon ECR)</strong> is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. ECR is integrated with Amazon ECS, allowing you to simplify your development and production workflows.</p><p>- <strong>Scan on push configuration</strong> in ECR automatically scans your Docker images for vulnerabilities when they are pushed to the registry, helping to ensure that the container images contain no severe vulnerabilities.</p><p>- <strong>Repository policies and identity-based policies</strong> provide fine-grained access control to ECR repositories. Repository policies can specify which AWS accounts can access your repositories, and identity-based policies can define which IAM roles within those accounts are allowed to access the images. This setup meets the requirement to ensure that only specific IAM roles and specific AWS accounts can access the container images.</p><p>The other options are less suitable:</p><p>- <strong>A.</strong> While this option also involves using ECR and CI/CD pipelines, it only mentions using identity-based policies to restrict access. The key distinction in option C is the mention of both repository policies and identity-based policies, providing a more comprehensive approach to access control.</p><p>- <strong>B.</strong> Hosting a private container registry on Amazon EC2 instances introduces significant management overhead, including maintaining the underlying infrastructure, securing the registry, and implementing host-based container scanning tools. This option does not leverage the managed services' benefits and increases complexity.</p><p>- <strong>D.</strong> AWS CodeArtifact is a fully managed artifact repository service that makes it easy for organizations to securely store, publish, and share software packages used in their software development process. However, CodeArtifact is designed for artifacts like libraries and dependencies, not specifically for container images, making it less suitable for the described use case than ECR.</p>"
    },
    {
        "type": "single",
        "question": "Question #97: A company's data scientists want to create artificial intelligence and machine learning (AI/ML) training models by using Amazon SageMaker. The training models will use large datasets in an Amazon S3 bucket. The datasets contain sensitive information. On average, the data scientists need 30 days to train models. The S3 bucket has been secured appropriately. The company's data retention policy states that all data that is older than 45 days must be removed from the S3 bucket. Which action should a security engineer take to enforce this data retention policy?",
        "options": {
            "A": "Configure an S3 Lifecycle rule on the S3 bucket to delete objects after 45 days.",
            "B": "Create an AWS Lambda function to check the last-modified date of the S3 objects and delete objects that are older than 45 days. Create an S3 event notification to invoke the Lambda function for each PutObject operation.",
            "C": "Create an AWS Lambda function to check the last-modified date of the S3 objects and delete objects that are older than 45 days. Create an Amazon EventBridge rule to invoke the Lambda function each month.",
            "D": "Configure S3 Intelligent-Tiering on the S3 bucket to automatically transition objects to another storage class."
        },
        "correctAnswer": "A"
    },
    {
        "type": "single",
        "question": "Question #98: A security engineer is troubleshooting an AWS Lambda function that is named MyLambdaFunction. The function is encountering an error when the function attempts to read the objects in an Amazon S3 bucket that is named DOC-EXAMPLE-BUCKET. The S3 bucket has the following bucket policy: Which change should the security engineer make to the policy to ensure that the Lambda function can read the bucket objects?",
        "options": {
            "A": "Remove the Condition element. Change the Principal element to the following:",
            "B": "Change the Action element to the following:",
            "C": "Change the Resource element to \"arn:aws:s3:::DOC-EXAMPLE- BUCKET/*\".",
            "D": "Change the Resource element to \"arn:aws:lambda:::function:MyLambdaFunction\". Change the Principal element to the following:"
        },
        "image": "98.png",
        "correctAnswer": "C",
        "explanation":"<p>The community's suggestion that the correct answer is **C** is indeed valid if the policy's &#96;Resource&#96; element is incorrectly specifying the bucket itself without including the objects within the bucket. In AWS S3 bucket policies, to refer to all objects in a bucket, you must include &quot;/*&quot; at the end of the bucket ARN. Here is why:</p><p><strong>C. Change the Resource element to &quot;arn:aws:s3:::DOC-EXAMPLE-BUCKET/*&quot;.</strong></p><p>Explanation:</p><p>- The current &#96;Resource&#96; element in the policy &#96;&quot;arn:aws:s3:::DOC-EXAMPLE-BUCKET&quot;&#96; points to the bucket itself and not the objects within the bucket. To grant access to read objects, the policy must specify the object-level path which includes the &quot;/*&quot; wildcard after the bucket name. This indicates that the permission applies to all objects within the bucket.</p><p>- The &#96;Condition&#96; element with the &#96;aws:SourceArn&#96; key restricts the permission to requests made from a specific source, which in this case, is the Lambda function &#96;MyLambdaFunction&#96;. If the Lambda function's &#96;Invoke&#96; action is configured correctly with the right ARN, and the function's execution role has the necessary permissions, this condition should not prevent the Lambda function from accessing the objects. Hence, the condition does not necessarily need to be removed unless it is confirmed to be the cause of the issue.</p><p>- The &#96;Principal&#96; element is using a service principal (&#96;lambda.amazonaws.com&#96;), which is correct when you want to allow the Lambda service to assume the role attached to a function to perform actions specified in the bucket policy.</p><p>Therefore, correcting the &#96;Resource&#96; element to include all objects in the bucket with &quot;/*&quot; would ensure that the Lambda function has the necessary permissions to read the objects, assuming the other elements like &#96;Principal&#96; and &#96;Condition&#96; are properly configured to match the Lambda function's execution role and ARN respectively.</p>"
    },
    {
        "type": "single",
        "question": "Question #99: An IAM user receives an Access Denied message when the user attempts to access objects in an Amazon S3 bucket. The user and the S3 bucket are in the same AWS account. The S3 bucket is configured to use server-side encryption with AWS KMS keys (SSE-KMS) to encrypt all of its objects at rest by using a customer managed key from the same AWS account. The S3 bucket has no bucket policy defined. The IAM user has been granted permissions through an IAM policy that allows the kms:Decrypt permission to the customer managed key. The IAM policy also allows the s3:List* and s3:Get* permissions for the S3 bucket and its objects. Which of the following is a possible reason that the IAM user cannot access the objects in the S3 bucket?",
        "options": {
            "A": "The IAM policy needs to allow the kms:DescribeKey permission.",
            "B": "The S3 bucket has been changed to use the AWS managed key to encrypt objects at rest.",
            "C": "An S3 bucket policy needs to be added to allow the IAM user to access the objects.",
            "D": "The KMS key policy has been edited to remove the ability for the AWS account to have full access to the key."
        },
        "correctAnswer": "D",
        "explanation":"<p>D. The KMS key policy has been edited to remove the ability for the AWS account to have full access to the key.</p><p>Explanation:</p><p>- <strong>AWS KMS keys (SSE-KMS)</strong> are used to encrypt and decrypt S3 objects in conjunction with S3's server-side encryption feature. The access to use these keys for encryption and decryption operations is controlled through the key policy associated with the KMS key.</p><p>- If the IAM user has the necessary &#96;kms:Decrypt&#96; permission as well as the &#96;s3:List*&#96; and &#96;s3:Get*&#96; permissions for the S3 bucket and its objects, the user should normally be able to access the objects in the S3 bucket. However, if the KMS key policy does not allow the IAM user or the IAM user's role to use the key, or if the account has been removed from the key policy altogether, the user will not be able to perform operations that require decryption with that key, resulting in an Access Denied message.</p><p>The other options are less likely based on the scenario described:</p><p>- <strong>A. The &#96;kms:DescribeKey&#96; permission</strong> is not necessary for decryption operations. It is used to retrieve metadata about the key but not for the actual process of encryption or decryption.</p><p>- <strong>B.</strong> If the S3 bucket had been changed to use the AWS managed key, the IAM user would not typically receive an Access Denied message due to key permissions, as AWS managed keys do not require the same explicit permissions in the key policy as customer managed keys.</p><p>- <strong>C.</strong> Since there is no bucket policy defined, the absence of a bucket policy is not the cause of the Access Denied message. Furthermore, if the IAM user has the necessary S3 permissions through the IAM policy, an S3 bucket policy is not required for access.</p>"
    },
    {
        "type": "single",
        "question": "Question #100: A company has a guideline that mandates the encryption of all Amazon S3 bucket data in transit. A security engineer must implement an S3 bucket policy that denies any S3 operations if data is not encrypted. Which S3 bucket policy will meet this requirement?",
        "options": {
            "A": "A",
            "B": "B",
            "C": "C",
            "D": "D"
        },
        "correctAnswer": "B",
        "image":"100.png",
        "explanation":""
    },
    {
        "type": "single",
        "question": "Question #101: A security engineer wants to use Amazon Simple Notification Service (Amazon SNS) to send email alerts to a company's security team for Amazon GuardDuty findings that have a High severity level. The security engineer also wants to deliver these findings to a visualization tool for further examination. Which solution will meet these requirements?",
        "options": {
            "A": "Set up GuardDuty to send notifications to an Amazon CloudWatch alarm with two targets in CloudWatch. From CloudWatch, stream the findings through Amazon Kinesis Data Streams into an Amazon Open Search Service domain as the first target for delivery. Use Amazon QuickSight to visualize the findings. Use OpenSearch queries for further analysis. Deliver email alerts to the security team by configuring an SNS topic as a second target for the CloudWatch alarm. Use event pattern matching with an Amazon EventBridge event rule to send only High severity findings in the alerts.",
            "B": "Set up GuardDuty to send notifications to AWS CloudTrail with two targets in CloudTrail. From CloudTrail, stream the findings through Amazon Kinesis Data Firehose into an Amazon OpenSearch Service domain as the first target for delivery. Use OpenSearch Dashboards to visualize the findings. Use OpenSearch queries for further analysis. Deliver email alerts to the security team by configuring an SNS topic as a second target for CloudTrail. Use event pattern matching with a CloudTrail event rule to send only High severity findings in the alerts.",
            "C": "Set up GuardDuty to send notifications to Amazon EventBridge with two targets. From EventBridge, stream the findings through Amazon Kinesis Data Firehose into an Amazon OpenSearch Service domain as the first target for delivery. Use OpenSearch Dashboards to visualize the findings. Use OpenSearch queries for further analysis. Deliver email alerts to the security team by configuring an SNS topic as a second target for EventBridge. Use event pattern matching with an EventBridge event rule to send only High severity findings in the alerts.",
            "D": "Set up GuardDuty to send notifications to Amazon EventBridge with two targets. From EventBridge, stream the findings through Amazon Kinesis Data Streams into an Amazon OpenSearch Service domain as the first target for delivery. Use Amazon QuickSight to visualize the findings. Use OpenSearch queries for further analysis. Deliver email alerts to the security team by configuring an SNS topic as a second target for EventBridge. Use event pattern matching with an EventBridge event rule to send only High severity findings in the alerts."
        },
        "correctAnswer": "C"
    },
    {
        "type": "single",
        "question": "Question #102: A security engineer needs to implement a write-once-read-many (WORM) model for data that a company will store in Amazon S3 buckets. The company uses the S3 Standard storage class for all of its S3 buckets. The security engineer must ensure that objects cannot be overwritten or deleted by any user, including the AWS account root user. Which solution will meet these requirements?",
        "options": {
            "A": "Create new S3 buckets with S3 Object Lock enabled in compliance mode. Place objects in the S3 buckets.",
            "B": "Use S3 Glacier Vault Lock to attach a Vault Lock policy to new S3 buckets. Wait 24 hours to complete the Vault Lock process. Place objects in the S3 buckets.",
            "C": "Create new S3 buckets with S3 Object Lock enabled in governance mode. Place objects in the S3 buckets.",
            "D": "Create new S3 buckets with S3 Object Lock enabled in governance mode. Add a legal hold to the S3 buckets. Place o10bjects in the S3 buckets."
        },
        "correctAnswer": "A"
    },
    {
        "type": "single",
        "question": "Question #103: A company needs complete encryption of the traffic between external users and an application. The company hosts the application on a fleet of Amazon EC2 instances that run in an Auto Scaling group behind an Application Load Balancer (ALB). How can a security engineer meet these requirements?",
        "options": {
            "A": "Create a new Amazon-issued certificate in AWS Secrets Manager. Export the certificate from Secrets Manager. Import the certificate into the ALB and the EC2 instances.",
            "B": "Create a new Amazon-issued certificate in AWS Certificate Manager (ACM). Associate the certificate with the ALExport the certificate from ACM. Install the certificate on the EC2 instances.",
            "C": "Import a new third-party certificate into AWS Identity and Access Management (IAM). Export the certificate from IAM. Associate the certificate with the ALB and the EC2 instances.",
            "D": "Import a new third-party certificate into AWS Certificate Manager (ACM). Associate the certificate with the ALB. Install the certificate on the EC2 instances."
        },
        "correctAnswer": "D",
        "explanation":"<p>Option D is the correct solution. To encrypt traffic between external users and the application behind the Application Load Balancer (ALB), a certificate should be imported into AWS Certificate Manager (ACM) and associated with the ALB. The same certificate should also be installed on the EC2 instances.</p><p><strong>Option A Create a new Amazon-issued certificate in AWS Secrets Manager. Export the certificate from Secrets Manager. Import the certificate into the ALB and the EC2 instances.</strong> It is incorrect because Secrets Manager is used for storing secrets, not SSL/TLS certificates.</p><p><strong>Option B Create a new Amazon-issued certificate in AWS Certificate Manager (ACM). Associate the certificate with the ALExport the certificate from ACM. Install the certificate on the EC2 instances.</strong> It is incorrect because Amazon-issued ACM certificates can only be used with Elastic Load Balancers and Amazon CloudFront. They cannot be exported and installed on EC2 instances.</p><p><strong>Option C Import a new third-party certificate into AWS Identity and Access Management (IAM). Export the certificate from IAM. Associate the certificate with the ALB and the EC2 instances.</strong> It is incorrect because IAM does not support importing or managing SSL/TLS certificates.</p><p>Option D uses a third-party certificate imported into ACM, associated with the ALB, and installed on the EC2 instances. This provides complete encryption between the users and application.</p>"
    },
    {
        "type": "single",
        "question": "Question #104: A company has an organization with SCPs in AWS Organizations. The root SCP for the organization is as follows: The company's developers are members of a group that has an IAM policy that allows access to Amazon Simple Email Service (Amazon SES) by allowing ses:* actions. The account is a child to an OU that has an SCP that allows Amazon SES. The developers are receiving a not-authorized error when they try to access Amazon SES through the AWS Management Console. Which change must a security engineer implement so that the developers can access Amazon SES?",
        "options": {
            "A": "Add a resource policy that allows each member of the group to access Amazon SES.",
            "B": "Add a resource policy that allows \"Principal\": {\"AWS\": \"arn:aws:iam::account-number:group/Dev\"}.",
            "C": "Remove the AWS Control Tower control (guardrail) that restricts access to Amazon SES.",
            "D": "Remove Amazon SES from the root SCP."
        },
        "correctAnswer": "D",
        "image": "104.png",
        "explanation":"<p>The JSON structure in the image shows a root Service Control Policy (SCP) in AWS Organizations. It includes two statements: one that allows all actions on all resources (`\"Action\": \"*\"`) and one that explicitly denies all actions on Amazon Simple Email Service (SES) resources (`\"Action\": \"ses:*\"`).</p><p>Given that the account is part of an OU with an SCP that allows Amazon SES and that developers have an IAM policy that allows access to SES, the issue lies with the root SCP denying SES actions. Hence, the developers are receiving a not-authorized error when trying to access Amazon SES through the AWS Management Console.</p><p>To resolve this issue, the security engineer needs to:</p><p>D. Remove Amazon SES from the root SCP.</p><p>This action would remove the explicit deny for Amazon SES actions at the root level, which is currently overriding the allow permissions in the IAM policies at the account level. SCPs are evaluated before IAM policies, and a 'Deny'; at the SCP level cannot be overridden by an /'Allow/'; at the IAM level. Therefore, removing the 'DenySES'; statement from the root SCP should allow the developers to access Amazon SES, assuming there are no other denies at the OU level or other SCPs applied to the account.</p><p>Sure, let's look at why the other options are incorrect given the context of AWS IAM and SCPs:</p><p>A. Add a resource policy that allows each member of the group to access Amazon SES. - Incorrect because resource policies are attached to AWS resources rather than users or groups, and they are used to grant permissions to other AWS accounts or anonymous users. Since the developers are already part of a group with an IAM policy that allows SES actions, the issue is not with the resource policy or the IAM policy at the user/group level. The issue is with the SCP which has an organization-wide effect and overrides IAM policies.</p><p>B. Add a resource policy that allows \"Principal\": {\"AWS\": \"arn:aws:iam::account-number:group/Dev\"}. - Incorrect because, as mentioned above, resource policies are attached to AWS resources, not IAM entities. Also, specifying the Principal in a resource policy would not override the `Deny` in the SCP. SCPs apply to all IAM principals within the specified AWS account or OU, and an SCP `Deny` cannot be overcome by an IAM `Allow`.</p><p>C. Remove the AWS Control Tower control (guardrail) that restricts access to Amazon SES. - Incorrect because the question does not mention that AWS Control Tower is being used, nor does it indicate that a Control Tower guardrail is responsible for the restriction. Even if a guardrail were in place, the SCP's explicit deny would still prevent access. Control Tower uses SCPs as part of its guardrail implementation, but the JSON policy shown is an SCP, not a Control Tower guardrail.</p><p>Option D is the correct one because SCPs take precedence over IAM policies, and an explicit `Deny` in an SCP will prevent any IAM `Allow` from granting access. Removing the SCP that explicitly denies access to Amazon SES would allow the IAM policy permissions to grant access as intended.</p>"
    },
    {
        "type": "multi",
        "question": "Question #105: A company hosts a public website on an Amazon EC2 instance. HTTPS traffic must be able to access the website. The company uses SSH for management of the web server. The website is on the subnet 10.0.1.0/24. The management subnet is 192.168.100.0/24. A security engineer must create a security group for the EC2 instance. Which combination of steps should the security engineer take to meet these requirements in the MOST secure manner? (Choose two.)",
        "options": {
            "A": "Allow port 22 from source 0.0.0.0/0.",
            "B": "Allow port 443 from source 0.0 0 0/0.",
            "C": "Allow port 22 from 192.168.100.0/24.",
            "D": "Allow port 22 from 10.0.1.0/24.",
            "E": "Allow port 443 from 10.0.1.0/24."
        },
        "correctAnswer": ["B", "C"],
        "explanation":"<p>To meet the requirements of allowing HTTPS traffic to the website and permitting SSH for web server management in the most secure manner, the security engineer should take the following steps:</p><p>B. Allow port 443 from source 0.0.0.0/0. - This step ensures that HTTPS traffic can reach the website. Since the website is public, it must be accessible from any IP address on the internet. HTTPS uses port 443, and the source of 0.0.0.0/0 means that the security group will accept traffic from any IP address.</p><p>C. Allow port 22 from 192.168.100.0/24. - This step allows SSH access (which uses port 22) only from the management subnet. Limiting SSH access to a specific, known IP range significantly reduces the surface of attack as opposed to allowing it from anywhere on the internet.</p><p>The other options increase the risk of unauthorized access and are therefore not as secure:</p><p>A. Allow port 22 from source 0.0.0.0/0. - This is not secure because it would allow SSH access from any IP address on the internet, which could expose the server to brute force attacks and other security risks.</p><p>D. Allow port 22 from 10.0.1.0/24. - While this is more secure than allowing SSH from any IP address, it is not the best option because the management subnet is 192.168.100.0/24, not 10.0.1.0/24. Therefore, this rule would not correctly restrict SSH access to the intended management subnet.</p><p>E. Allow port 443 from 10.0.1.0/24. - This would restrict HTTPS access to only the specified subnet, which goes against the requirement that the website should be publicly accessible over HTTPS. Public websites should accept HTTPS traffic from any source IP.</p>"
    },
    {
        "type": "single",
        "question": "Question #106: A security engineer wants to forward custom application-security logs from an Amazon EC2 instance to Amazon CloudWatch. The security engineer installs the CloudWatch agent on the EC2 instance and adds the path of the logs to the CloudWatch configuration file. However, CloudWatch does not receive the logs. The security engineer verifies that the awslogs service is running on the EC2 instance. What should the security engineer do next to resolve the issue?",
        "options": {
            "A": "Add AWS CloudTrail to the trust policy of the EC2 instance. Send the custom logs to CloudTrail instead of CloudWatch.",
            "B": "Add Amazon S3 to the trust policy of the EC2 instance. Configure the application to write the custom logs to an S3 bucket that CloudWatch can use to ingest the logs.",
            "C": "Add Amazon Inspector to the trust policy of the EC2 instance. Use Amazon Inspector instead of the CloudWatch agent to collect the custom logs.",
            "D": "Attach the CloudWatchAgentServerPolicy AWS managed policy to the EC2 instance role."
        },
        "correctAnswer": "D"
    },
    {
        "type": "single",
        "question": "Question #107: A systems engineer is troubleshooting the connectivity of a test environment that includes a virtual security appliance deployed inline. In addition to using the virtual security appliance, the development team wants to use security groups and network ACLs to accomplish various security requirements in the environment. What configuration is necessary to allow the virtual security appliance to route the traffic?",
        "options": {
            "A": "Disable network ACLs.",
            "B": "Configure the security appliance's elastic network interface for promiscuous mode.",
            "C": "Disable the Network Source/Destination check on the security appliance's elastic network interface.",
            "D": "Place the security appliance in the public subnet with the internet gateway."
        },
        "correctAnswer": "C",
        "explanation":"<p>To enable a virtual security appliance to route traffic in AWS, it is necessary to ensure that the appliance can handle traffic that is not specifically destined for its own IP address. The standard behavior of EC2 instances is to not accept traffic that isn't destined for them.</p><p>C. Disable the Network Source/Destination check on the security appliance's elastic network interface. - This is necessary because, by default, an EC2 instance must be the final destination of any traffic it receives. Since a virtual security appliance acts as a router, traffic needs to pass through it to get to other instances, so the source/destination check must be disabled. This tells the EC2 network interface to accept traffic regardless of the destination address, allowing it to route traffic appropriately.</p><p>The other options do not directly address the requirement to route traffic:</p><p>A. Disable network ACLs. - This is not a good security practice. Network ACLs (Access Control Lists) provide a layer of security at the subnet level, controlling inbound and outbound traffic. While disabling them might allow all traffic to pass through unrestricted, it would not specifically enable the virtual appliance to route traffic, and would reduce the security of the environment.</p><p>B. Configure the security appliance's elastic network interface for promiscuous mode. - AWS EC2 instances do not support promiscuous mode, where a network interface controller (NIC) would pass all traffic it receives to the central processing unit (CPU), rather than passing only the frames that the NIC is intended to receive.</p><p>D. Place the security appliance in the public subnet with the internet gateway. - While the security appliance may need to be in a public subnet to handle traffic to and from the internet, this alone does not enable it to route traffic. The source/destination check would still need to be disabled on the appliance's network interface. Moreover, simply placing it in a public subnet does not inherently allow it to route traffic; appropriate routing tables and security group rules are also necessary.</p>"
    },
    {
        "type": "single",
        "question": "Question #108",
        "options": {
            "A": "A",
            "B": "B",
            "C": "C",
            "D": "D"
        },
        "correctAnswer": "A",
        "image": "108.png",
        "explanation":""
    },
    {
        "type": "single",
        "question": "Question #109: A security engineer recently rotated all IAM access keys in an AWS account. The security engineer then configured AWS Config and enabled the following AWS Config managed rules: mfa-enabled-for-iam-console-access, iam-user-mfa-enabled, access-keys-rotated, and iam-user-unused-credentials-check. The security engineer notices that all resources are displaying as noncompliant after the IAM GenerateCredentialReport API operation is invoked. What could be the reason for the noncompliant status?",
        "options": {
            "A": "The IAM credential report was generated within the past 4 hours.",
            "B": "The security engineer does not have the GenerateCredentialReport permission.",
            "C": "The security engineer does not have the GetCredentialReport permission.",
            "D": "The AWS Config rules have a MaximumExecutionFrequency value of 24 hours."
        },
        "correctAnswer": "A",
        "explanation":"<p>The AWS Config service evaluates the configuration settings of your AWS resources according to the rules you specify. Given the scenario where IAM access keys were recently rotated and the AWS Config managed rules were enabled, if all resources are showing as noncompliant after the 'IAM GenerateCredentialReport'; API operation is invoked, the potential reasons for this could be:</p><p>A. The IAM credential report was generated within the past 4 hours. - If the IAM credential report was generated within the past 4 hours, it may not reflect the recent changes made by the security engineer. AWS Config evaluates the compliance status of resources by comparing the current state with the desired configuration settings. If the credential report does not yet include the changes because it's outdated, this could falsely show resources as noncompliant.</p><p>B. The security engineer does not have the 'GenerateCredentialReport' permission. - If the security engineer lacked the necessary permissions to generate the credential report, the operation would not complete successfully. However, in such a case, the security engineer would typically receive an error when trying to invoke the operation, rather than seeing all resources marked as noncompliant.</p><p>C. The security engineer does not have the 'GetCredentialReport' permission. - Similar to option B, lacking the 'GetCredentialReport' permission would typically result in an error when attempting to retrieve the credential report. This would not, in itself, result in resources being marked as noncompliant.</p><p>D. The AWS Config rules have a 'MaximumExecutionFrequency' value of 24 hours. - If the 'MaximumExecutionFrequency' for the AWS Config rules is set to 24 hours, it means the rules will only check for compliance once every 24 hours. Therefore, even if the access keys were rotated, AWS Config will not immediately reflect these changes in the compliance status until the next scheduled evaluation.</p><p>Given these options, the most likely reason for all resources displaying as noncompliant is option A. The IAM credential report may not reflect the recent key rotations if it was generated before the changes took effect. AWS Config relies on the latest credential report to determine the compliance status. If the report is outdated, it could falsely indicate noncompliance. However, it's also important to understand the 'MaximumExecutionFrequency' setting in option D. If it's been less than 24 hours since the keys were rotated and the AWS Config rules were set to check compliance every 24 hours, the recent changes may not yet be reflected in the compliance status.</p>"
    },
    {
        "type": "single",
        "question": "Question #110: A company is using AWS WAF to protect a customized public API service that is based on Amazon EC2 instances. The API uses an Application Load Balancer. The AWS WAF web ACL is configured with an AWS Managed Rules rule group. After a software upgrade to the API and the client application, some types of requests are no longer working and are causing application stability issues. A security engineer discovers that AWS WAF logging is not turned on for the web ACL. The security engineer needs to immediately return the application to service, resolve the issue, and ensure that logging is not turned off in the future. The security engineer turns on logging for the web ACL and specifies Amazon CloudWatch Logs as the destination. Which additional set of steps should the security engineer take to meet the requirements?",
        "options": {
            "A": "Edit the rules in the web ACL to include rules with Count actions. Review the logs to determine which rule is blocking the request. Modify the IAM policy of all AWS WAF administrators so that they cannot remove the logging configuration for any AWS WAF web ACLs.",
            "B": "Edit the rules in the web ACL to include rules with Count actions. Review the logs to determine which rule is blocking the request. Modify the AWS WAF resource policy so that AWS WAF administrators cannot remove the logging configuration for any AWS WAF web ACLs.",
            "C": "Edit the rules in the web ACL to include rules with Count and Challenge actions. Review the logs to determine which rule is blocking the request. Modify the AWS WAF resource policy so that AWS WAF administrators cannot remove the logging configuration for any AWS WAF web ACLs.",
            "D": "Edit the rules in the web ACL to include rules with Count and Challenge actions. Review the logs to determine which rule is blocking the request. Modify the IAM policy of all AWS WAF administrators so that they cannot remove the logging configuration for any AWS WAF web ACLs."
        },
        "correctAnswer": "A",
        "explanation":"<p>The security engineer needs to address the immediate issue of restoring application service and then ensure that logging remains enabled in the future to help troubleshoot such issues. Here are the steps they should take:</p><p>A. Edit the rules in the web ACL to include rules with Count actions. Review the logs to determine which rule is blocking the request. Modify the IAM policy of all AWS WAF administrators so that they cannot remove the logging configuration for any AWS WAF web ACLs. - The first part of this option is to edit the rules to include Count actions. This action allows the security engineer to see which rules are being triggered without actually blocking the requests. By reviewing the logs, the engineer can identify which rules are affecting the API requests. - The second part involves modifying the IAM policy. This is a preventative measure to ensure that AWS WAF administrators do not disable logging in the future. It ensures that logging remains an enforced part of the security posture.</p><p>The other options are either incomplete or contain actions that are not feasible:</p><p>B. Edit the rules in the web ACL to include rules with Count actions. Review the logs to determine which rule is blocking the request. Modify the AWS WAF resource policy so that AWS WAF administrators cannot remove the logging configuration for any AWS WAF web ACLs. - AWS WAF does not support resource policies. Therefore, this step cannot be performed. IAM policies are the correct way to control permissions for AWS WAF administrators.</p><p>C. Edit the rules in the web ACL to include rules with Count and Challenge actions. Review the logs to determine which rule is blocking the request. Modify the AWS WAF resource policy so that AWS WAF administrators cannot remove the logging configuration for any AWS WAF web ACLs. - Adding Challenge actions is not necessary for troubleshooting and may further disrupt the application's stability. Also, as mentioned above, AWS WAF does not use resource policies.</p><p>D. Edit the rules in the web ACL to include rules with Count and Challenge actions. Review the logs to determine which rule is blocking the request. Modify the IAM policy of all AWS WAF administrators so that they cannot remove the logging configuration for any AWS WAF web ACLs. - As in option C, Challenge actions are not necessary and may be disruptive. The correct approach is to use Count actions for troubleshooting.</p><p>Therefore, the best set of steps is reflected in option A, which correctly identifies the necessary immediate troubleshooting steps and the long-term preventative measure to enforce logging without causing further disruption to the API service.</p>"
    },
    {
        "type": "multi",
        "question": "Question #111: A security engineer is creating an AWS Lambda function. The Lambda function needs to use a role that is named LambdaAuditRole to assume a role that is named AcmeAuditFactoryRole in a different AWS account. When the code is processed, the following error message appears: \"An error occurred (AccessDenied) when calling the AssumeRole operation.\" Which combination of steps should the security engineer take to resolve this error? (Choose two.)",
        "options": {
            "A": "Ensure that LambdaAuditRole has the sts:AssumeRole permission for AcmeAuditFactoryRole.",
            "B": "Ensure that LambdaAuditRole has the AWSLambdaBasicExecutionRole managed policy attached.",
            "C": "Ensure that the trust policy for AcmeAuditFactoryRole allows the sts:AssumeRole action from LambdaAuditRole.",
            "D": "Ensure that the trust policy for LambdaAuditRole allows the sts:AssumeRole action from the lambda.amazonaws.com service.",
            "E": "Ensure that the sts:AssumeRole API call is being issued to the us-east-1 Region endpoint."
        },
        "correctAnswer": ["A", "C"],
        "explanation":"<p>To resolve the \"Access Denied\" error when calling the `AssumeRole` operation, the following steps should be taken:</p><p>A. Ensure that LambdaAuditRole has the sts:AssumeRole permission for AcmeAuditFactoryRole. - This step is crucial because the LambdaAuditRole must have the explicit permission to assume the AcmeAuditFactoryRole. This permission is set within the IAM policy attached to LambdaAuditRole and allows the role to perform the `sts:AssumeRole` action for the specified role in the other account.</p><p>C. Ensure that the trust policy for AcmeAuditFactoryRole allows the sts:AssumeRole action from LambdaAuditRole. - The trust policy on the AcmeAuditFactoryRole must explicitly trust the LambdaAuditRole to assume it. This means that the trust policy of AcmeAuditFactoryRole should include a statement that allows the `sts:AssumeRole` action from the LambdaAuditRole's ARN.</p><p>The other options listed are not directly related to the error message:</p><p>B. Ensure that LambdaAuditRole has the AWSLambdaBasicExecutionRole managed policy attached. - While this managed policy is necessary for the Lambda function to write logs to CloudWatch, it is not related to the `AssumeRole` permission and will not resolve the \"Access Denied\" error related to role assumption.</p><p>D. Ensure that the trust policy for LambdaAuditRole allows the sts:AssumeRole action from the lambda.amazonaws.com service. - This is a necessary step when creating a Lambda function, as the role it assumes must trust the Lambda service to assume the role. However, it is not related to assuming the AcmeAuditFactoryRole from another AWS account.</p><p>E. Ensure that the sts:AssumeRole API call is being issued to the us-east-1 Region endpoint. - The `sts:AssumeRole` API call does not require a specific regional endpoint; it can be called against the global STS endpoint. Unless the AcmeAuditFactoryRole is a regional service role or requires calls from a specific region, this is not necessary to resolve the error.</p><p>Therefore, ensuring that LambdaAuditRole has the correct permissions (option A) and that the AcmeAuditFactoryRole has the appropriate trust relationship (option C) are the correct steps to resolve the error.</p>"
    },
    {
        "type": "multi",
        "question": "Question #112: A company has AWS accounts in an organization in AWS Organizations. The organization includes a dedicated security account. All AWS account activity across all member accounts must be logged and reported to the dedicated security account. The company must retain all the activity logs in a secure storage location within the dedicated security account for 2 years. No changes or deletions of the logs are allowed. Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
        "options": {
            "A": "In the dedicated security account, create an Amazon S3 bucket. Configure S3 Object Lock in compliance mode and a retention period of 2 years on the S3 bucket. Set the bucket policy to allow the organization's management account to write to the S3 bucket.",
            "B": "In the dedicated security account, create an Amazon S3 bucket. Configure S3 Object Lock in compliance mode and a retention period of 2 years on the S3 bucket. Set the bucket policy to allow the organization's member accounts to write to the S3 bucket.",
            "C": "In the dedicated security account, create an Amazon S3 bucket that has an S3 Lifecycle configuration that expires objects after 2 years. Set the bucket policy to allow the organization's member accounts to write to the S3 bucket.",
            "D": "Create an AWS CloudTrail trail for the organization. Configure logs to be delivered to the logging Amazon S3 bucket in the dedicated security account.",
            "E": "Turn on AWS CloudTrail in each account. Configure logs to be delivered to an Amazon S3 bucket that is created in the organization's management account. Forward the logs to the S3 bucket in the dedicated security account by using AWS Lambda and Amazon Kinesis Data Firehose."
        },
        "correctAnswer": ["B", "D"],
        "explanation":"<p>To meet the requirements with the least operational overhead, the company should take the following steps:</p><p>B. In the dedicated security account, create an Amazon S3 bucket. Configure S3 Object Lock in compliance mode and a retention period of 2 years on the S3 bucket. Set the bucket policy to allow the organization's member accounts to write to the S3 bucket. - Using Amazon S3 Object Lock with compliance mode ensures that the logs cannot be deleted or altered during the retention period, meeting the requirement for log immutability. Setting a retention period of 2 years will satisfy the log retention requirement. By allowing member accounts to write to the S3 bucket, centralized logging can be achieved with minimal configuration.</p><p>D. Create an AWS CloudTrail trail for the organization. Configure logs to be delivered to the logging Amazon S3 bucket in the dedicated security account. - Creating an organizational CloudTrail trail and configuring it to deliver logs to a centralized S3 bucket in the dedicated security account enables logging of all account activity across all member accounts. This approach reduces the operational overhead of managing individual trails in each member account.</p><p>The other options are not optimal:</p><p>A. This option is similar to B but specifies that the management account should write to the S3 bucket, which is not necessary if an organizational trail is used, as in option D. Member accounts do not write logs directly; CloudTrail in the management account can handle the log delivery for all accounts.</p><p>C. Configuring an S3 Lifecycle policy to expire objects after 2 years does not prevent changes or deletions within that period, which does not comply with the immutability requirement.</p><p>E. This option introduces unnecessary complexity and operational overhead. Using AWS Lambda and Amazon Kinesis Data Firehose to forward logs from the management account's S3 bucket to the dedicated security account's S3 bucket is not required when an organizational CloudTrail can directly deliver logs to the security account's S3 bucket.</p><p>Therefore, options B and D are the correct choices as they meet the requirements with the least operational overhead.</p>"
    },
    {
        "type": "single",
        "question": "Question #113: A company is testing its incident response plan for compromised credentials. The company runs a database on an Amazon EC2 instance and stores the sensitive database credentials as a secret in AWS Secrets Manager. The secret has rotation configured with an AWS Lambda function that uses the generic rotation function template. The EC2 instance and the Lambda function are deployed in the same private subnet. The VPC has a Secrets Manager VPC endpoint. A security engineer discovers that the secret cannot rotate. The security engineer determines that the VPC endpoint is working as intended. The Amazon CloudWatch logs contain the following error: \"setSecret: Unable to log into database\". Which solution will resolve this error?",
        "options": {
            "A": "Use the AWS Management Console to edit the JSON structure of the secret in Secrets Manager so that the secret automatically conforms with the structure that the database requires.",
            "B": "Ensure that the security group that is attached to the Lambda function allows outbound connections to the EC2 instance. Ensure that the security group that is attached to the EC2 instance allows inbound connections from the security group that is attached to the Lambda function.",
            "C": "Use the Secrets Manager list-secrets command in the AWS CLI to list the secret. Identify the database credentials. Use the Secrets Manager rotate-secret command in the AWS CLI to force the immediate rotation of the secret.",
            "D": "Add an internet gateway to the VPC. Create a NAT gateway in a public subnet. Update the VPC route tables so that traffic from the Lambda function and traffic from the EC2 instance can reach the Secrets Manager public endpoint."
        },
        "correctAnswer": "B",
        "explanation":"<p>The error \"setSecret: Unable to log into database\" suggests that the AWS Lambda function, which is responsible for rotating the secret, cannot connect to the database on the Amazon EC2 instance to apply the new credentials. The possible causes could be network-related issues such as security group configurations or the Lambda function's execution role permissions.</p><p>Based on the given options, the most likely solution would be:</p><p>B. Ensure that the security group that is attached to the Lambda function allows outbound connections to the EC2 instance. Ensure that the security group that is attached to the EC2 instance allows inbound connections from the security group that is attached to the Lambda function. - This step is essential because the Lambda function must be able to establish a connection to the EC2 instance hosting the database to rotate the secret. This involves the security group of the Lambda function allowing outbound traffic to the database port on the EC2 instance and the security group of the EC2 instance allowing inbound traffic from the Lambda function.</p><p>The other options are less likely to be the correct solution:</p><p>A. Use the AWS Management Console to edit the JSON structure of the secret in Secrets Manager so that the secret automatically conforms with the structure that the database requires. - While the secret structure must match what the database expects, the error message does not indicate a problem with the secret's structure. Instead, it points to a connectivity issue.</p><p>C. Use the Secrets Manager list-secrets command in the AWS CLI to list the secret. Identify the database credentials. Use the Secrets Manager rotate-secret command in the AWS CLI to force the immediate rotation of the secret. - Forcing rotation of the secret would not resolve a connectivity issue. The error would likely persist if the underlying issue is not addressed.</p><p>D. Add an internet gateway to the VPC. Create a NAT gateway in a public subnet. Update the VPC route tables so that traffic from the Lambda function and traffic from the EC2 instance can reach the Secrets Manager public endpoint. - Since the VPC has a Secrets Manager VPC endpoint and it is working as intended, there is no need for the Lambda function or the EC2 instance to access Secrets Manager through the internet. This step would not solve the connectivity issue between the Lambda function and the EC2 instance.</p><p>Therefore, option B is the best solution to resolve the error because it directly addresses the potential connectivity issue between the Lambda function and the EC2 instance that is preventing the secret rotation.</p>"
    },
    {
        "type": "single",
        "question": "Question #114: A company deploys a set of standard IAM roles in AWS accounts. The IAM roles are based on job functions within the company. To balance operational efficiency and security, a security engineer implemented AWS Organizations SCPs to restrict access to critical security services in all company accounts. All of the company's accounts and OUs within AWS Organizations have a default FullAWSAccess SCP that is attached. The security engineer needs to ensure that no one can disable Amazon GuardDuty and AWS Security Hub. The security engineer also must not override other permissions that are granted by IAM policies that are defined in the accounts. Which SCP should the security engineer attach to the root of the organization to meet these requirements?",
        "options": {
            "A": "A",
            "B": "B",
            "C": "C",
            "D": "D"
        },
        "correctAnswer": "A",
        "image":"114.png",
        "explanation":""
    },
    {
        "type": "single",
        "question": "Question #115: A company needs to follow security best practices to deploy resources from an AWS CloudFormation template. The CloudFormation template must be able to configure sensitive database credentials. The company already uses AWS Key Management Service (AWS KMS) and AWS Secrets Manager. Which solution will meet the requirements?",
        "options": {
            "A": "Use a dynamic reference in the CloudFormation template to reference the database credentials in Secrets Manager.",
            "B": "Use a parameter in the CloudFormation template to reference the database credentials. Encrypt the CloudFormation template by using AWS KMS.",
            "C": "Use a SecureString parameter in the CloudFormation template to reference the database credentials in Secrets Manager.",
            "D": "Use a SecureString parameter in the CloudFormation template to reference an encrypted value in AWS KMS."
        },
        "correctAnswer": "A",
        "explanation":"<p>The best practice for handling sensitive information such as database credentials in AWS CloudFormation templates is to avoid hardcoding the credentials directly into the template and instead use a service designed for secure credential storage, such as AWS Secrets Manager. This approach ensures that sensitive data is encrypted and managed properly.</p><p>Here are the solutions provided:</p><p>A. Use a dynamic reference in the CloudFormation template to reference the database credentials in Secrets Manager. - Dynamic references in CloudFormation templates allow you to reference values in Secrets Manager securely, and this method ensures that the database credentials are not exposed in the CloudFormation template. The credentials are automatically resolved by AWS when the template is deployed.</p><p>B. Use a parameter in the CloudFormation template to reference the database credentials. Encrypt the CloudFormation template by using AWS KMS. - Encrypting the entire CloudFormation template with AWS KMS does not make it secure for handling credentials because the credentials would still need to be inserted into the template in some form, and they could be exposed when the template is used.</p><p>C. Use a SecureString parameter in the CloudFormation template to reference the database credentials in Secrets Manager. - CloudFormation does not support SecureString parameters directly. AWS Secrets Manager should be used for storing sensitive information and then referenced using dynamic references.</p><p>D. Use a SecureString parameter in the CloudFormation template to reference an encrypted value in AWS KMS. - KMS is used to encrypt and decrypt the data but does not manage the storage of secrets. Therefore, using a SecureString to directly reference an encrypted value in KMS is not a practical solution for CloudFormation templates.</p><p>Based on the above, the correct answer is:</p><p>A. Use a dynamic reference in the CloudFormation template to reference the database credentials in Secrets Manager.</p><p>This approach is secure and follows AWS best practices for managing sensitive data within CloudFormation templates.</p>"
    },
    {
        "type": "multi",
        "question": "Question #116: An international company wants to combine AWS Security Hub findings across all the company's AWS Regions and from multiple accounts. In addition, the company wants to create a centralized custom dashboard to correlate these findings with operational data for deeper analysis and insights. The company needs an analytics tool to search and visualize Security Hub findings. Which combination of steps will meet these requirements? (Choose three.)",
        "options": {
            "A": "Designate an AWS account as a delegated administrator for Security Hub. Publish events to Amazon CloudWatch from the delegated administrator account, all member accounts, and required Regions that are enabled for Security Hub findings.",
            "B": "Designate an AWS account in an organization in AWS Organizations as a delegated administrator for Security Hub. Publish events to Amazon EventBridge from the delegated administrator account, all member accounts, and required Regions that are enabled for Security Hub findings.",
            "C": "In each Region, create an Amazon EventBridge rule to deliver findings to an Amazon Kinesis data stream. Configure the Kinesis data streams to output the logs to a single Amazon S3 bucket.",
            "D": "In each Region, create an Amazon EventBridge rule to deliver findings to an Amazon Kinesis Data Firehose delivery stream. Configure the Kinesis Data Firehose delivery streams to deliver the logs to a single Amazon S3 bucket.",
            "E": "Use AWS Glue DataBrew to crawl the Amazon S3 bucket and build the schema. Use AWS Glue Data Catalog to query the data and create views to flatten nested attributes. Build Amazon QuickSight dashboards by using Amazon Athena.",
            "F": "Partition the Amazon S3 data. Use AWS Glue to crawl the S3 bucket and build the schema. Use Amazon Athena to query the data and create views to flatten nested attributes. Build Amazon QuickSight dashboards that use the Athena views."
        },
        "correctAnswer": ["B", "D", "F"],
        "explanation":"<p>To meet the requirements of combining AWS Security Hub findings across all the company's AWS Regions and from multiple accounts, and to create a centralized custom dashboard for deeper analysis, the following combination of steps will be effective:</p><p>B. Designate an AWS account in an organization in AWS Organizations as a delegated administrator for Security Hub. Publish events to Amazon EventBridge from the delegated administrator account, all member accounts, and required Regions that are enabled for Security Hub findings. - This step is critical for aggregating Security Hub findings from multiple accounts and regions in a centralized manner. EventBridge can capture and route these findings to the appropriate destinations for further processing.</p><p>D. In each Region, create an Amazon EventBridge rule to deliver findings to an Amazon Kinesis Data Firehose delivery stream. Configure the Kinesis Data Firehose delivery streams to deliver the logs to a single Amazon S3 bucket. - Using EventBridge to capture Security Hub findings and Kinesis Data Firehose to deliver these findings to an Amazon S3 bucket enables efficient, scalable ingestion of security data. This setup facilitates the centralized storage of findings for subsequent analysis.</p><p>F. Partition the Amazon S3 data. Use AWS Glue to crawl the S3 bucket and build the schema. Use Amazon Athena to query the data and create views to flatten nested attributes. Build Amazon QuickSight dashboards that use the Athena views. - This step involves data preparation and analysis. AWS Glue can catalog the findings stored in S3, making the data queryable with Amazon Athena. Athena can then be used to query and transform the data. Finally, Amazon QuickSight can visualize the data, providing the required custom dashboard for deeper insights.</p><p>The reasons for not choosing the other options:</p><p>A. While publishing events to Amazon CloudWatch could be useful for monitoring, it's not directly relevant to aggregating and analyzing Security Hub findings across regions and accounts for the purpose described. EventBridge serves as a more appropriate service for routing Security Hub findings.</p><p>C. While creating an EventBridge rule to deliver findings to a Kinesis data stream is a plausible step, Kinesis Data Firehose (option D) is more directly suited for delivering logs to S3 without the need for additional processing or management.</p><p>E. AWS Glue DataBrew is a tool for data preparation, but the requirement focuses more on data analysis and visualization. AWS Glue (mentioned in option F) is better suited for cataloging and preparing the data for querying with Athena, which then feeds into QuickSight for visualization.</p>"
    },
    {
        "type": "single",
        "question": "Question #117: An AWS account administrator created an IAM group and applied the following managed policy to require that each individual user authenticate using multi-factor authentication: After implementing the policy, the administrator receives reports that users are unable to perform Amazon EC2 commands using the AWS CLI. What should the administrator do to resolve this problem while still enforcing multi-factor authentication?",
        "options": {
            "A": "Change the value of aws:MultiFactorAuthPresent to true.",
            "B": "Instruct users to run the aws sts get-session-token CLI command and pass the multi-factor authentication --serial-number and -token-code parameters. Use these resulting values to make API/CLI calls.",
            "C": "Implement federated API/CLI access using SAML 2.0, then configure the identity provider to enforce multi-factor authentication.",
            "D": "Create a role and enforce multi-factor authentication in the role trust policy. Instruct users to run the sts assume-role CLI command and pass --serial-number and --token-code parameters. Store the resulting values in environment variables. Add sts:AssumeRole to NotAction in the policy."
        },
        "correctAnswer": "B",
        "image": "117.png",
        "explanation":"<p>The IAM policy in the provided image is intended to enforce multi-factor authentication (MFA) for accessing Amazon EC2 resources. However, users are reporting that they are unable to perform Amazon EC2 commands using the AWS CLI after this policy was implemented. This is likely because the AWS CLI does not inherently support MFA when making direct API calls, and the policy denies all EC2 actions if MFA is not used.</p><p>The administrator needs to resolve this issue while still enforcing MFA. The correct steps to do this would be:</p><p>B. Instruct users to run the aws sts get-session-token CLI command and pass the multi-factor authentication --serial-number and --token-code parameters. Use these resulting values to make API/CLI calls. - By using the `get-session-token` command with MFA, the user receives temporary credentials that include an `access key ID`, a `secret access key`, and a `session token`. When these temporary credentials are used, they satisfy the MFA condition in the policy, allowing users to perform EC2 actions.</p><p>Here's why the other options are incorrect or less suitable:</p><p>A. Changing the value of aws:MultiFactorAuthPresent to true. - This is not a practical solution because the `aws:MultiFactorAuthPresent` condition key is set by AWS based on the context of the API request. It cannot be manually changed by administrators in IAM policies to enforce MFA.</p><p>C. Implement federated API/CLI access using SAML 2.0, then configure the identity provider to enforce multi-factor authentication. - While federation with SAML is a valid way to enforce MFA, it is typically used for single sign-on (SSO) from corporate directories and is more complex to set up. It's not a direct solution to the immediate problem of CLI access and would involve significant changes to the way authentication is handled.</p><p>D. Create a role and enforce multi-factor authentication in the role trust policy. Instruct users to run the sts assume-role CLI command and pass --serial-number and --token-code parameters. Store the resulting values in environment variables. Add sts:AssumeRole to NotAction in the policy. - This option is unnecessarily complex for the given scenario. While assuming a role with MFA is a valid approach, the `NotAction` element in IAM policies is not typically used in this context and could lead to unintended permission grants. Additionally, this would require setting up a new role and modifying the trust policy, which is more involved than simply obtaining a session token with MFA.</p><p>Therefore, the most straightforward and immediate resolution to allow users to use the CLI with MFA enforced is option B.</p>"
    },
    {
        "type": "single",
        "question": "Question #118: A company is developing a mechanism that will help data scientists use Amazon SageMaker to read, process, and output data to an Amazon S3 bucket. Data scientists will have access to a dedicated S3 prefix for each of their projects. The company will implement bucket policies that use the dedicated S3 prefixes to restrict access to the S3 objects. The projects can last up to 60 days. The company's security team mandates that data cannot remain in the S3 bucket after the end of the projects that use the data. Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A": "Create an AWS Lambda function to identify and delete objects in the S3 bucket that have not been accessed for 60 days. Create an Amazon EventBridge scheduled rule that runs every day to invoke the Lambda function.",
            "B": "Create a new S3 bucket. Configure the new S3 bucket to use S3 Intelligent-Tiering. Copy the objects to the new S3 bucket.",
            "C": "Create an S3 Lifecycle configuration for each S3 bucket prefix for each project. Set the S3 Lifecycle configurations to expire objects after 60 days.",
            "D": "Create an AWS Lambda function to delete objects that have not been accessed for 60 days. Create an S3 event notification for S3 Intelligent-Tiering automatic archival events to invoke the Lambda function."
        },
        "correctAnswer": "C",
        "explanation":"<p>The most cost-effective solution that ensures data does not remain in the S3 bucket after the end of the projects is to automate the process using native S3 features. Here&rsquo;s an analysis of the provided options:</p><p>C. Create an S3 Lifecycle configuration for each S3 bucket prefix for each project. Set the S3 Lifecycle configurations to expire objects after 60 days. - This option aligns with the requirements and leverages built-in S3 features to manage object lifecycle. By setting a lifecycle policy to automatically expire objects after 60 days, the objects will be deleted without the need for additional services or manual intervention. This is a cost-effective solution because there are no additional charges for setting up lifecycle policies, and it reduces the storage cost by deleting the data that is no longer needed.</p><p>Now, let's look at why the other options are less suitable:</p><p>A. Create an AWS Lambda function to identify and delete objects in the S3 bucket that have not been accessed for 60 days. Create an Amazon EventBridge scheduled rule that runs every day to invoke the Lambda function. - This option introduces complexity and additional costs associated with Lambda invocations and the development and maintenance of the function. It's also less reliable compared to using native S3 lifecycle policies, which are designed for this exact purpose.</p><p>B. Create a new S3 bucket. Configure the new S3 bucket to use S3 Intelligent-Tiering. Copy the objects to the new S3 bucket. - S3 Intelligent-Tiering is a cost optimization feature that moves objects between access tiers when access patterns change, which may reduce storage costs. However, it does not address the requirement to automatically delete objects after 60 days.</p><p>D. Create an AWS Lambda function to delete objects that have not been accessed for 60 days. Create an S3 event notification for S3 Intelligent-Tiering automatic archival events to invoke the Lambda function. - Similar to option A, this involves unnecessary complexity and cost. S3 event notifications for Intelligent-Tiering archival events are not designed to trigger deletion based on object age. Furthermore, the archival event does not necessarily correlate with the 60-day project end.</p><p>Therefore, the most cost-effective and straightforward solution is option C, which directly meets the requirement to automatically delete the objects after the project ends using S3 Lifecycle policies.</p>"
    }
]