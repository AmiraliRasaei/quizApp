[
    {
        "type": "single",
        "question": "Question #101: A security engineer wants to use Amazon Simple Notification Service (Amazon SNS) to send email alerts to a company's security team for Amazon GuardDuty findings that have a High severity level. The security engineer also wants to deliver these findings to a visualization tool for further examination. Which solution will meet these requirements?",
        "options": {
            "A": "Set up GuardDuty to send notifications to an Amazon CloudWatch alarm with two targets in CloudWatch. From CloudWatch, stream the findings through Amazon Kinesis Data Streams into an Amazon Open Search Service domain as the first target for delivery. Use Amazon QuickSight to visualize the findings. Use OpenSearch queries for further analysis. Deliver email alerts to the security team by configuring an SNS topic as a second target for the CloudWatch alarm. Use event pattern matching with an Amazon EventBridge event rule to send only High severity findings in the alerts.",
            "B": "Set up GuardDuty to send notifications to AWS CloudTrail with two targets in CloudTrail. From CloudTrail, stream the findings through Amazon Kinesis Data Firehose into an Amazon OpenSearch Service domain as the first target for delivery. Use OpenSearch Dashboards to visualize the findings. Use OpenSearch queries for further analysis. Deliver email alerts to the security team by configuring an SNS topic as a second target for CloudTrail. Use event pattern matching with a CloudTrail event rule to send only High severity findings in the alerts.",
            "C": "Set up GuardDuty to send notifications to Amazon EventBridge with two targets. From EventBridge, stream the findings through Amazon Kinesis Data Firehose into an Amazon OpenSearch Service domain as the first target for delivery. Use OpenSearch Dashboards to visualize the findings. Use OpenSearch queries for further analysis. Deliver email alerts to the security team by configuring an SNS topic as a second target for EventBridge. Use event pattern matching with an EventBridge event rule to send only High severity findings in the alerts.",
            "D": "Set up GuardDuty to send notifications to Amazon EventBridge with two targets. From EventBridge, stream the findings through Amazon Kinesis Data Streams into an Amazon OpenSearch Service domain as the first target for delivery. Use Amazon QuickSight to visualize the findings. Use OpenSearch queries for further analysis. Deliver email alerts to the security team by configuring an SNS topic as a second target for EventBridge. Use event pattern matching with an EventBridge event rule to send only High severity findings in the alerts."
        },
        "correctAnswer": "C"
    },
    {
        "type": "single",
        "question": "Question #102: A security engineer needs to implement a write-once-read-many (WORM) model for data that a company will store in Amazon S3 buckets. The company uses the S3 Standard storage class for all of its S3 buckets. The security engineer must ensure that objects cannot be overwritten or deleted by any user, including the AWS account root user. Which solution will meet these requirements?",
        "options": {
            "A": "Create new S3 buckets with S3 Object Lock enabled in compliance mode. Place objects in the S3 buckets.",
            "B": "Use S3 Glacier Vault Lock to attach a Vault Lock policy to new S3 buckets. Wait 24 hours to complete the Vault Lock process. Place objects in the S3 buckets.",
            "C": "Create new S3 buckets with S3 Object Lock enabled in governance mode. Place objects in the S3 buckets.",
            "D": "Create new S3 buckets with S3 Object Lock enabled in governance mode. Add a legal hold to the S3 buckets. Place o10bjects in the S3 buckets."
        },
        "correctAnswer": "A"
    },
    {
        "type": "single",
        "question": "Question #103: A company needs complete encryption of the traffic between external users and an application. The company hosts the application on a fleet of Amazon EC2 instances that run in an Auto Scaling group behind an Application Load Balancer (ALB). How can a security engineer meet these requirements?",
        "options": {
            "A": "Create a new Amazon-issued certificate in AWS Secrets Manager. Export the certificate from Secrets Manager. Import the certificate into the ALB and the EC2 instances.",
            "B": "Create a new Amazon-issued certificate in AWS Certificate Manager (ACM). Associate the certificate with the ALExport the certificate from ACM. Install the certificate on the EC2 instances.",
            "C": "Import a new third-party certificate into AWS Identity and Access Management (IAM). Export the certificate from IAM. Associate the certificate with the ALB and the EC2 instances.",
            "D": "Import a new third-party certificate into AWS Certificate Manager (ACM). Associate the certificate with the ALB. Install the certificate on the EC2 instances."
        },
        "correctAnswer": "D",
        "explanation":"<p>Option D is the correct solution. To encrypt traffic between external users and the application behind the Application Load Balancer (ALB), a certificate should be imported into AWS Certificate Manager (ACM) and associated with the ALB. The same certificate should also be installed on the EC2 instances.</p><p><strong>Option A Create a new Amazon-issued certificate in AWS Secrets Manager. Export the certificate from Secrets Manager. Import the certificate into the ALB and the EC2 instances.</strong> It is incorrect because Secrets Manager is used for storing secrets, not SSL/TLS certificates.</p><p><strong>Option B Create a new Amazon-issued certificate in AWS Certificate Manager (ACM). Associate the certificate with the ALExport the certificate from ACM. Install the certificate on the EC2 instances.</strong> It is incorrect because Amazon-issued ACM certificates can only be used with Elastic Load Balancers and Amazon CloudFront. They cannot be exported and installed on EC2 instances.</p><p><strong>Option C Import a new third-party certificate into AWS Identity and Access Management (IAM). Export the certificate from IAM. Associate the certificate with the ALB and the EC2 instances.</strong> It is incorrect because IAM does not support importing or managing SSL/TLS certificates.</p><p>Option D uses a third-party certificate imported into ACM, associated with the ALB, and installed on the EC2 instances. This provides complete encryption between the users and application.</p>"
    },
    {
        "type": "single",
        "question": "Question #104: A company has an organization with SCPs in AWS Organizations. The root SCP for the organization is as follows: The company's developers are members of a group that has an IAM policy that allows access to Amazon Simple Email Service (Amazon SES) by allowing ses:* actions. The account is a child to an OU that has an SCP that allows Amazon SES. The developers are receiving a not-authorized error when they try to access Amazon SES through the AWS Management Console. Which change must a security engineer implement so that the developers can access Amazon SES?",
        "options": {
            "A": "Add a resource policy that allows each member of the group to access Amazon SES.",
            "B": "Add a resource policy that allows \"Principal\": {\"AWS\": \"arn:aws:iam::account-number:group/Dev\"}.",
            "C": "Remove the AWS Control Tower control (guardrail) that restricts access to Amazon SES.",
            "D": "Remove Amazon SES from the root SCP."
        },
        "correctAnswer": "D",
        "image": "104.png",
        "explanation":"<p>The JSON structure in the image shows a root Service Control Policy (SCP) in AWS Organizations. It includes two statements: one that allows all actions on all resources (`\"Action\": \"*\"`) and one that explicitly denies all actions on Amazon Simple Email Service (SES) resources (`\"Action\": \"ses:*\"`).</p><p>Given that the account is part of an OU with an SCP that allows Amazon SES and that developers have an IAM policy that allows access to SES, the issue lies with the root SCP denying SES actions. Hence, the developers are receiving a not-authorized error when trying to access Amazon SES through the AWS Management Console.</p><p>To resolve this issue, the security engineer needs to:</p><p>D. Remove Amazon SES from the root SCP.</p><p>This action would remove the explicit deny for Amazon SES actions at the root level, which is currently overriding the allow permissions in the IAM policies at the account level. SCPs are evaluated before IAM policies, and a 'Deny'; at the SCP level cannot be overridden by an /'Allow/'; at the IAM level. Therefore, removing the 'DenySES'; statement from the root SCP should allow the developers to access Amazon SES, assuming there are no other denies at the OU level or other SCPs applied to the account.</p><p>Sure, let's look at why the other options are incorrect given the context of AWS IAM and SCPs:</p><p>A. Add a resource policy that allows each member of the group to access Amazon SES. - Incorrect because resource policies are attached to AWS resources rather than users or groups, and they are used to grant permissions to other AWS accounts or anonymous users. Since the developers are already part of a group with an IAM policy that allows SES actions, the issue is not with the resource policy or the IAM policy at the user/group level. The issue is with the SCP which has an organization-wide effect and overrides IAM policies.</p><p>B. Add a resource policy that allows \"Principal\": {\"AWS\": \"arn:aws:iam::account-number:group/Dev\"}. - Incorrect because, as mentioned above, resource policies are attached to AWS resources, not IAM entities. Also, specifying the Principal in a resource policy would not override the `Deny` in the SCP. SCPs apply to all IAM principals within the specified AWS account or OU, and an SCP `Deny` cannot be overcome by an IAM `Allow`.</p><p>C. Remove the AWS Control Tower control (guardrail) that restricts access to Amazon SES. - Incorrect because the question does not mention that AWS Control Tower is being used, nor does it indicate that a Control Tower guardrail is responsible for the restriction. Even if a guardrail were in place, the SCP's explicit deny would still prevent access. Control Tower uses SCPs as part of its guardrail implementation, but the JSON policy shown is an SCP, not a Control Tower guardrail.</p><p>Option D is the correct one because SCPs take precedence over IAM policies, and an explicit `Deny` in an SCP will prevent any IAM `Allow` from granting access. Removing the SCP that explicitly denies access to Amazon SES would allow the IAM policy permissions to grant access as intended.</p>"
    },
    {
        "type": "multi",
        "question": "Question #105: A company hosts a public website on an Amazon EC2 instance. HTTPS traffic must be able to access the website. The company uses SSH for management of the web server. The website is on the subnet 10.0.1.0/24. The management subnet is 192.168.100.0/24. A security engineer must create a security group for the EC2 instance. Which combination of steps should the security engineer take to meet these requirements in the MOST secure manner? (Choose two.)",
        "options": {
            "A": "Allow port 22 from source 0.0.0.0/0.",
            "B": "Allow port 443 from source 0.0 0 0/0.",
            "C": "Allow port 22 from 192.168.100.0/24.",
            "D": "Allow port 22 from 10.0.1.0/24.",
            "E": "Allow port 443 from 10.0.1.0/24."
        },
        "correctAnswer": ["B", "C"],
        "explanation":"<p>To meet the requirements of allowing HTTPS traffic to the website and permitting SSH for web server management in the most secure manner, the security engineer should take the following steps:</p><p>B. Allow port 443 from source 0.0.0.0/0. - This step ensures that HTTPS traffic can reach the website. Since the website is public, it must be accessible from any IP address on the internet. HTTPS uses port 443, and the source of 0.0.0.0/0 means that the security group will accept traffic from any IP address.</p><p>C. Allow port 22 from 192.168.100.0/24. - This step allows SSH access (which uses port 22) only from the management subnet. Limiting SSH access to a specific, known IP range significantly reduces the surface of attack as opposed to allowing it from anywhere on the internet.</p><p>The other options increase the risk of unauthorized access and are therefore not as secure:</p><p>A. Allow port 22 from source 0.0.0.0/0. - This is not secure because it would allow SSH access from any IP address on the internet, which could expose the server to brute force attacks and other security risks.</p><p>D. Allow port 22 from 10.0.1.0/24. - While this is more secure than allowing SSH from any IP address, it is not the best option because the management subnet is 192.168.100.0/24, not 10.0.1.0/24. Therefore, this rule would not correctly restrict SSH access to the intended management subnet.</p><p>E. Allow port 443 from 10.0.1.0/24. - This would restrict HTTPS access to only the specified subnet, which goes against the requirement that the website should be publicly accessible over HTTPS. Public websites should accept HTTPS traffic from any source IP.</p>"
    },
    {
        "type": "single",
        "question": "Question #106: A security engineer wants to forward custom application-security logs from an Amazon EC2 instance to Amazon CloudWatch. The security engineer installs the CloudWatch agent on the EC2 instance and adds the path of the logs to the CloudWatch configuration file. However, CloudWatch does not receive the logs. The security engineer verifies that the awslogs service is running on the EC2 instance. What should the security engineer do next to resolve the issue?",
        "options": {
            "A": "Add AWS CloudTrail to the trust policy of the EC2 instance. Send the custom logs to CloudTrail instead of CloudWatch.",
            "B": "Add Amazon S3 to the trust policy of the EC2 instance. Configure the application to write the custom logs to an S3 bucket that CloudWatch can use to ingest the logs.",
            "C": "Add Amazon Inspector to the trust policy of the EC2 instance. Use Amazon Inspector instead of the CloudWatch agent to collect the custom logs.",
            "D": "Attach the CloudWatchAgentServerPolicy AWS managed policy to the EC2 instance role."
        },
        "correctAnswer": "D"
    },
    {
        "type": "single",
        "question": "Question #107: A systems engineer is troubleshooting the connectivity of a test environment that includes a virtual security appliance deployed inline. In addition to using the virtual security appliance, the development team wants to use security groups and network ACLs to accomplish various security requirements in the environment. What configuration is necessary to allow the virtual security appliance to route the traffic?",
        "options": {
            "A": "Disable network ACLs.",
            "B": "Configure the security appliance's elastic network interface for promiscuous mode.",
            "C": "Disable the Network Source/Destination check on the security appliance's elastic network interface.",
            "D": "Place the security appliance in the public subnet with the internet gateway."
        },
        "correctAnswer": "C",
        "explanation":"<p>To enable a virtual security appliance to route traffic in AWS, it is necessary to ensure that the appliance can handle traffic that is not specifically destined for its own IP address. The standard behavior of EC2 instances is to not accept traffic that isn't destined for them.</p><p>C. Disable the Network Source/Destination check on the security appliance's elastic network interface. - This is necessary because, by default, an EC2 instance must be the final destination of any traffic it receives. Since a virtual security appliance acts as a router, traffic needs to pass through it to get to other instances, so the source/destination check must be disabled. This tells the EC2 network interface to accept traffic regardless of the destination address, allowing it to route traffic appropriately.</p><p>The other options do not directly address the requirement to route traffic:</p><p>A. Disable network ACLs. - This is not a good security practice. Network ACLs (Access Control Lists) provide a layer of security at the subnet level, controlling inbound and outbound traffic. While disabling them might allow all traffic to pass through unrestricted, it would not specifically enable the virtual appliance to route traffic, and would reduce the security of the environment.</p><p>B. Configure the security appliance's elastic network interface for promiscuous mode. - AWS EC2 instances do not support promiscuous mode, where a network interface controller (NIC) would pass all traffic it receives to the central processing unit (CPU), rather than passing only the frames that the NIC is intended to receive.</p><p>D. Place the security appliance in the public subnet with the internet gateway. - While the security appliance may need to be in a public subnet to handle traffic to and from the internet, this alone does not enable it to route traffic. The source/destination check would still need to be disabled on the appliance's network interface. Moreover, simply placing it in a public subnet does not inherently allow it to route traffic; appropriate routing tables and security group rules are also necessary.</p>"
    },
    {
        "type": "single",
        "question": "Question #109: A security engineer recently rotated all IAM access keys in an AWS account. The security engineer then configured AWS Config and enabled the following AWS Config managed rules: mfa-enabled-for-iam-console-access, iam-user-mfa-enabled, access-keys-rotated, and iam-user-unused-credentials-check. The security engineer notices that all resources are displaying as noncompliant after the IAM GenerateCredentialReport API operation is invoked. What could be the reason for the noncompliant status?",
        "options": {
            "A": "The IAM credential report was generated within the past 4 hours.",
            "B": "The security engineer does not have the GenerateCredentialReport permission.",
            "C": "The security engineer does not have the GetCredentialReport permission.",
            "D": "The AWS Config rules have a MaximumExecutionFrequency value of 24 hours."
        },
        "correctAnswer": "A",
        "explanation":"<p>The AWS Config service evaluates the configuration settings of your AWS resources according to the rules you specify. Given the scenario where IAM access keys were recently rotated and the AWS Config managed rules were enabled, if all resources are showing as noncompliant after the 'IAM GenerateCredentialReport'; API operation is invoked, the potential reasons for this could be:</p><p>A. The IAM credential report was generated within the past 4 hours. - If the IAM credential report was generated within the past 4 hours, it may not reflect the recent changes made by the security engineer. AWS Config evaluates the compliance status of resources by comparing the current state with the desired configuration settings. If the credential report does not yet include the changes because it's outdated, this could falsely show resources as noncompliant.</p><p>B. The security engineer does not have the 'GenerateCredentialReport' permission. - If the security engineer lacked the necessary permissions to generate the credential report, the operation would not complete successfully. However, in such a case, the security engineer would typically receive an error when trying to invoke the operation, rather than seeing all resources marked as noncompliant.</p><p>C. The security engineer does not have the 'GetCredentialReport' permission. - Similar to option B, lacking the 'GetCredentialReport' permission would typically result in an error when attempting to retrieve the credential report. This would not, in itself, result in resources being marked as noncompliant.</p><p>D. The AWS Config rules have a 'MaximumExecutionFrequency' value of 24 hours. - If the 'MaximumExecutionFrequency' for the AWS Config rules is set to 24 hours, it means the rules will only check for compliance once every 24 hours. Therefore, even if the access keys were rotated, AWS Config will not immediately reflect these changes in the compliance status until the next scheduled evaluation.</p><p>Given these options, the most likely reason for all resources displaying as noncompliant is option A. The IAM credential report may not reflect the recent key rotations if it was generated before the changes took effect. AWS Config relies on the latest credential report to determine the compliance status. If the report is outdated, it could falsely indicate noncompliance. However, it's also important to understand the 'MaximumExecutionFrequency' setting in option D. If it's been less than 24 hours since the keys were rotated and the AWS Config rules were set to check compliance every 24 hours, the recent changes may not yet be reflected in the compliance status.</p>"
    },
    {
        "type": "single",
        "question": "Question #110: A company is using AWS WAF to protect a customized public API service that is based on Amazon EC2 instances. The API uses an Application Load Balancer. The AWS WAF web ACL is configured with an AWS Managed Rules rule group. After a software upgrade to the API and the client application, some types of requests are no longer working and are causing application stability issues. A security engineer discovers that AWS WAF logging is not turned on for the web ACL. The security engineer needs to immediately return the application to service, resolve the issue, and ensure that logging is not turned off in the future. The security engineer turns on logging for the web ACL and specifies Amazon CloudWatch Logs as the destination. Which additional set of steps should the security engineer take to meet the requirements?",
        "options": {
            "A": "Edit the rules in the web ACL to include rules with Count actions. Review the logs to determine which rule is blocking the request. Modify the IAM policy of all AWS WAF administrators so that they cannot remove the logging configuration for any AWS WAF web ACLs.",
            "B": "Edit the rules in the web ACL to include rules with Count actions. Review the logs to determine which rule is blocking the request. Modify the AWS WAF resource policy so that AWS WAF administrators cannot remove the logging configuration for any AWS WAF web ACLs.",
            "C": "Edit the rules in the web ACL to include rules with Count and Challenge actions. Review the logs to determine which rule is blocking the request. Modify the AWS WAF resource policy so that AWS WAF administrators cannot remove the logging configuration for any AWS WAF web ACLs.",
            "D": "Edit the rules in the web ACL to include rules with Count and Challenge actions. Review the logs to determine which rule is blocking the request. Modify the IAM policy of all AWS WAF administrators so that they cannot remove the logging configuration for any AWS WAF web ACLs."
        },
        "correctAnswer": "A",
        "explanation":"<p>The security engineer needs to address the immediate issue of restoring application service and then ensure that logging remains enabled in the future to help troubleshoot such issues. Here are the steps they should take:</p><p>A. Edit the rules in the web ACL to include rules with Count actions. Review the logs to determine which rule is blocking the request. Modify the IAM policy of all AWS WAF administrators so that they cannot remove the logging configuration for any AWS WAF web ACLs. - The first part of this option is to edit the rules to include Count actions. This action allows the security engineer to see which rules are being triggered without actually blocking the requests. By reviewing the logs, the engineer can identify which rules are affecting the API requests. - The second part involves modifying the IAM policy. This is a preventative measure to ensure that AWS WAF administrators do not disable logging in the future. It ensures that logging remains an enforced part of the security posture.</p><p>The other options are either incomplete or contain actions that are not feasible:</p><p>B. Edit the rules in the web ACL to include rules with Count actions. Review the logs to determine which rule is blocking the request. Modify the AWS WAF resource policy so that AWS WAF administrators cannot remove the logging configuration for any AWS WAF web ACLs. - AWS WAF does not support resource policies. Therefore, this step cannot be performed. IAM policies are the correct way to control permissions for AWS WAF administrators.</p><p>C. Edit the rules in the web ACL to include rules with Count and Challenge actions. Review the logs to determine which rule is blocking the request. Modify the AWS WAF resource policy so that AWS WAF administrators cannot remove the logging configuration for any AWS WAF web ACLs. - Adding Challenge actions is not necessary for troubleshooting and may further disrupt the application's stability. Also, as mentioned above, AWS WAF does not use resource policies.</p><p>D. Edit the rules in the web ACL to include rules with Count and Challenge actions. Review the logs to determine which rule is blocking the request. Modify the IAM policy of all AWS WAF administrators so that they cannot remove the logging configuration for any AWS WAF web ACLs. - As in option C, Challenge actions are not necessary and may be disruptive. The correct approach is to use Count actions for troubleshooting.</p><p>Therefore, the best set of steps is reflected in option A, which correctly identifies the necessary immediate troubleshooting steps and the long-term preventative measure to enforce logging without causing further disruption to the API service.</p>"
    },
    {
        "type": "multi",
        "question": "Question #111: A security engineer is creating an AWS Lambda function. The Lambda function needs to use a role that is named LambdaAuditRole to assume a role that is named AcmeAuditFactoryRole in a different AWS account. When the code is processed, the following error message appears: \"An error occurred (AccessDenied) when calling the AssumeRole operation.\" Which combination of steps should the security engineer take to resolve this error? (Choose two.)",
        "options": {
            "A": "Ensure that LambdaAuditRole has the sts:AssumeRole permission for AcmeAuditFactoryRole.",
            "B": "Ensure that LambdaAuditRole has the AWSLambdaBasicExecutionRole managed policy attached.",
            "C": "Ensure that the trust policy for AcmeAuditFactoryRole allows the sts:AssumeRole action from LambdaAuditRole.",
            "D": "Ensure that the trust policy for LambdaAuditRole allows the sts:AssumeRole action from the lambda.amazonaws.com service.",
            "E": "Ensure that the sts:AssumeRole API call is being issued to the us-east-1 Region endpoint."
        },
        "correctAnswer": ["A", "C"],
        "explanation":"<p>To resolve the \"Access Denied\" error when calling the `AssumeRole` operation, the following steps should be taken:</p><p>A. Ensure that LambdaAuditRole has the sts:AssumeRole permission for AcmeAuditFactoryRole. - This step is crucial because the LambdaAuditRole must have the explicit permission to assume the AcmeAuditFactoryRole. This permission is set within the IAM policy attached to LambdaAuditRole and allows the role to perform the `sts:AssumeRole` action for the specified role in the other account.</p><p>C. Ensure that the trust policy for AcmeAuditFactoryRole allows the sts:AssumeRole action from LambdaAuditRole. - The trust policy on the AcmeAuditFactoryRole must explicitly trust the LambdaAuditRole to assume it. This means that the trust policy of AcmeAuditFactoryRole should include a statement that allows the `sts:AssumeRole` action from the LambdaAuditRole's ARN.</p><p>The other options listed are not directly related to the error message:</p><p>B. Ensure that LambdaAuditRole has the AWSLambdaBasicExecutionRole managed policy attached. - While this managed policy is necessary for the Lambda function to write logs to CloudWatch, it is not related to the `AssumeRole` permission and will not resolve the \"Access Denied\" error related to role assumption.</p><p>D. Ensure that the trust policy for LambdaAuditRole allows the sts:AssumeRole action from the lambda.amazonaws.com service. - This is a necessary step when creating a Lambda function, as the role it assumes must trust the Lambda service to assume the role. However, it is not related to assuming the AcmeAuditFactoryRole from another AWS account.</p><p>E. Ensure that the sts:AssumeRole API call is being issued to the us-east-1 Region endpoint. - The `sts:AssumeRole` API call does not require a specific regional endpoint; it can be called against the global STS endpoint. Unless the AcmeAuditFactoryRole is a regional service role or requires calls from a specific region, this is not necessary to resolve the error.</p><p>Therefore, ensuring that LambdaAuditRole has the correct permissions (option A) and that the AcmeAuditFactoryRole has the appropriate trust relationship (option C) are the correct steps to resolve the error.</p>"
    },
    {
        "type": "multi",
        "question": "Question #112: A company has AWS accounts in an organization in AWS Organizations. The organization includes a dedicated security account. All AWS account activity across all member accounts must be logged and reported to the dedicated security account. The company must retain all the activity logs in a secure storage location within the dedicated security account for 2 years. No changes or deletions of the logs are allowed. Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
        "options": {
            "A": "In the dedicated security account, create an Amazon S3 bucket. Configure S3 Object Lock in compliance mode and a retention period of 2 years on the S3 bucket. Set the bucket policy to allow the organization's management account to write to the S3 bucket.",
            "B": "In the dedicated security account, create an Amazon S3 bucket. Configure S3 Object Lock in compliance mode and a retention period of 2 years on the S3 bucket. Set the bucket policy to allow the organization's member accounts to write to the S3 bucket.",
            "C": "In the dedicated security account, create an Amazon S3 bucket that has an S3 Lifecycle configuration that expires objects after 2 years. Set the bucket policy to allow the organization's member accounts to write to the S3 bucket.",
            "D": "Create an AWS CloudTrail trail for the organization. Configure logs to be delivered to the logging Amazon S3 bucket in the dedicated security account.",
            "E": "Turn on AWS CloudTrail in each account. Configure logs to be delivered to an Amazon S3 bucket that is created in the organization's management account. Forward the logs to the S3 bucket in the dedicated security account by using AWS Lambda and Amazon Kinesis Data Firehose."
        },
        "correctAnswer": ["B", "D"],
        "explanation":"<p>To meet the requirements with the least operational overhead, the company should take the following steps:</p><p>B. In the dedicated security account, create an Amazon S3 bucket. Configure S3 Object Lock in compliance mode and a retention period of 2 years on the S3 bucket. Set the bucket policy to allow the organization's member accounts to write to the S3 bucket. - Using Amazon S3 Object Lock with compliance mode ensures that the logs cannot be deleted or altered during the retention period, meeting the requirement for log immutability. Setting a retention period of 2 years will satisfy the log retention requirement. By allowing member accounts to write to the S3 bucket, centralized logging can be achieved with minimal configuration.</p><p>D. Create an AWS CloudTrail trail for the organization. Configure logs to be delivered to the logging Amazon S3 bucket in the dedicated security account. - Creating an organizational CloudTrail trail and configuring it to deliver logs to a centralized S3 bucket in the dedicated security account enables logging of all account activity across all member accounts. This approach reduces the operational overhead of managing individual trails in each member account.</p><p>The other options are not optimal:</p><p>A. This option is similar to B but specifies that the management account should write to the S3 bucket, which is not necessary if an organizational trail is used, as in option D. Member accounts do not write logs directly; CloudTrail in the management account can handle the log delivery for all accounts.</p><p>C. Configuring an S3 Lifecycle policy to expire objects after 2 years does not prevent changes or deletions within that period, which does not comply with the immutability requirement.</p><p>E. This option introduces unnecessary complexity and operational overhead. Using AWS Lambda and Amazon Kinesis Data Firehose to forward logs from the management account's S3 bucket to the dedicated security account's S3 bucket is not required when an organizational CloudTrail can directly deliver logs to the security account's S3 bucket.</p><p>Therefore, options B and D are the correct choices as they meet the requirements with the least operational overhead.</p>"
    },
    {
        "type": "single",
        "question": "Question #113: A company is testing its incident response plan for compromised credentials. The company runs a database on an Amazon EC2 instance and stores the sensitive database credentials as a secret in AWS Secrets Manager. The secret has rotation configured with an AWS Lambda function that uses the generic rotation function template. The EC2 instance and the Lambda function are deployed in the same private subnet. The VPC has a Secrets Manager VPC endpoint. A security engineer discovers that the secret cannot rotate. The security engineer determines that the VPC endpoint is working as intended. The Amazon CloudWatch logs contain the following error: \"setSecret: Unable to log into database\". Which solution will resolve this error?",
        "options": {
            "A": "Use the AWS Management Console to edit the JSON structure of the secret in Secrets Manager so that the secret automatically conforms with the structure that the database requires.",
            "B": "Ensure that the security group that is attached to the Lambda function allows outbound connections to the EC2 instance. Ensure that the security group that is attached to the EC2 instance allows inbound connections from the security group that is attached to the Lambda function.",
            "C": "Use the Secrets Manager list-secrets command in the AWS CLI to list the secret. Identify the database credentials. Use the Secrets Manager rotate-secret command in the AWS CLI to force the immediate rotation of the secret.",
            "D": "Add an internet gateway to the VPC. Create a NAT gateway in a public subnet. Update the VPC route tables so that traffic from the Lambda function and traffic from the EC2 instance can reach the Secrets Manager public endpoint."
        },
        "correctAnswer": "B",
        "explanation":"<p>The error \"setSecret: Unable to log into database\" suggests that the AWS Lambda function, which is responsible for rotating the secret, cannot connect to the database on the Amazon EC2 instance to apply the new credentials. The possible causes could be network-related issues such as security group configurations or the Lambda function's execution role permissions.</p><p>Based on the given options, the most likely solution would be:</p><p>B. Ensure that the security group that is attached to the Lambda function allows outbound connections to the EC2 instance. Ensure that the security group that is attached to the EC2 instance allows inbound connections from the security group that is attached to the Lambda function. - This step is essential because the Lambda function must be able to establish a connection to the EC2 instance hosting the database to rotate the secret. This involves the security group of the Lambda function allowing outbound traffic to the database port on the EC2 instance and the security group of the EC2 instance allowing inbound traffic from the Lambda function.</p><p>The other options are less likely to be the correct solution:</p><p>A. Use the AWS Management Console to edit the JSON structure of the secret in Secrets Manager so that the secret automatically conforms with the structure that the database requires. - While the secret structure must match what the database expects, the error message does not indicate a problem with the secret's structure. Instead, it points to a connectivity issue.</p><p>C. Use the Secrets Manager list-secrets command in the AWS CLI to list the secret. Identify the database credentials. Use the Secrets Manager rotate-secret command in the AWS CLI to force the immediate rotation of the secret. - Forcing rotation of the secret would not resolve a connectivity issue. The error would likely persist if the underlying issue is not addressed.</p><p>D. Add an internet gateway to the VPC. Create a NAT gateway in a public subnet. Update the VPC route tables so that traffic from the Lambda function and traffic from the EC2 instance can reach the Secrets Manager public endpoint. - Since the VPC has a Secrets Manager VPC endpoint and it is working as intended, there is no need for the Lambda function or the EC2 instance to access Secrets Manager through the internet. This step would not solve the connectivity issue between the Lambda function and the EC2 instance.</p><p>Therefore, option B is the best solution to resolve the error because it directly addresses the potential connectivity issue between the Lambda function and the EC2 instance that is preventing the secret rotation.</p>"
    },
    {
        "type": "single",
        "question": "Question #115: A company needs to follow security best practices to deploy resources from an AWS CloudFormation template. The CloudFormation template must be able to configure sensitive database credentials. The company already uses AWS Key Management Service (AWS KMS) and AWS Secrets Manager. Which solution will meet the requirements?",
        "options": {
            "A": "Use a dynamic reference in the CloudFormation template to reference the database credentials in Secrets Manager.",
            "B": "Use a parameter in the CloudFormation template to reference the database credentials. Encrypt the CloudFormation template by using AWS KMS.",
            "C": "Use a SecureString parameter in the CloudFormation template to reference the database credentials in Secrets Manager.",
            "D": "Use a SecureString parameter in the CloudFormation template to reference an encrypted value in AWS KMS."
        },
        "correctAnswer": "A",
        "explanation":"<p>The best practice for handling sensitive information such as database credentials in AWS CloudFormation templates is to avoid hardcoding the credentials directly into the template and instead use a service designed for secure credential storage, such as AWS Secrets Manager. This approach ensures that sensitive data is encrypted and managed properly.</p><p>Here are the solutions provided:</p><p>A. Use a dynamic reference in the CloudFormation template to reference the database credentials in Secrets Manager. - Dynamic references in CloudFormation templates allow you to reference values in Secrets Manager securely, and this method ensures that the database credentials are not exposed in the CloudFormation template. The credentials are automatically resolved by AWS when the template is deployed.</p><p>B. Use a parameter in the CloudFormation template to reference the database credentials. Encrypt the CloudFormation template by using AWS KMS. - Encrypting the entire CloudFormation template with AWS KMS does not make it secure for handling credentials because the credentials would still need to be inserted into the template in some form, and they could be exposed when the template is used.</p><p>C. Use a SecureString parameter in the CloudFormation template to reference the database credentials in Secrets Manager. - CloudFormation does not support SecureString parameters directly. AWS Secrets Manager should be used for storing sensitive information and then referenced using dynamic references.</p><p>D. Use a SecureString parameter in the CloudFormation template to reference an encrypted value in AWS KMS. - KMS is used to encrypt and decrypt the data but does not manage the storage of secrets. Therefore, using a SecureString to directly reference an encrypted value in KMS is not a practical solution for CloudFormation templates.</p><p>Based on the above, the correct answer is:</p><p>A. Use a dynamic reference in the CloudFormation template to reference the database credentials in Secrets Manager.</p><p>This approach is secure and follows AWS best practices for managing sensitive data within CloudFormation templates.</p>"
    },
    {
        "type": "multi",
        "question": "Question #116: An international company wants to combine AWS Security Hub findings across all the company's AWS Regions and from multiple accounts. In addition, the company wants to create a centralized custom dashboard to correlate these findings with operational data for deeper analysis and insights. The company needs an analytics tool to search and visualize Security Hub findings. Which combination of steps will meet these requirements? (Choose three.)",
        "options": {
            "A": "Designate an AWS account as a delegated administrator for Security Hub. Publish events to Amazon CloudWatch from the delegated administrator account, all member accounts, and required Regions that are enabled for Security Hub findings.",
            "B": "Designate an AWS account in an organization in AWS Organizations as a delegated administrator for Security Hub. Publish events to Amazon EventBridge from the delegated administrator account, all member accounts, and required Regions that are enabled for Security Hub findings.",
            "C": "In each Region, create an Amazon EventBridge rule to deliver findings to an Amazon Kinesis data stream. Configure the Kinesis data streams to output the logs to a single Amazon S3 bucket.",
            "D": "In each Region, create an Amazon EventBridge rule to deliver findings to an Amazon Kinesis Data Firehose delivery stream. Configure the Kinesis Data Firehose delivery streams to deliver the logs to a single Amazon S3 bucket.",
            "E": "Use AWS Glue DataBrew to crawl the Amazon S3 bucket and build the schema. Use AWS Glue Data Catalog to query the data and create views to flatten nested attributes. Build Amazon QuickSight dashboards by using Amazon Athena.",
            "F": "Partition the Amazon S3 data. Use AWS Glue to crawl the S3 bucket and build the schema. Use Amazon Athena to query the data and create views to flatten nested attributes. Build Amazon QuickSight dashboards that use the Athena views."
        },
        "correctAnswer": ["B", "D", "F"],
        "explanation":"<p>To meet the requirements of combining AWS Security Hub findings across all the company's AWS Regions and from multiple accounts, and to create a centralized custom dashboard for deeper analysis, the following combination of steps will be effective:</p><p>B. Designate an AWS account in an organization in AWS Organizations as a delegated administrator for Security Hub. Publish events to Amazon EventBridge from the delegated administrator account, all member accounts, and required Regions that are enabled for Security Hub findings. - This step is critical for aggregating Security Hub findings from multiple accounts and regions in a centralized manner. EventBridge can capture and route these findings to the appropriate destinations for further processing.</p><p>D. In each Region, create an Amazon EventBridge rule to deliver findings to an Amazon Kinesis Data Firehose delivery stream. Configure the Kinesis Data Firehose delivery streams to deliver the logs to a single Amazon S3 bucket. - Using EventBridge to capture Security Hub findings and Kinesis Data Firehose to deliver these findings to an Amazon S3 bucket enables efficient, scalable ingestion of security data. This setup facilitates the centralized storage of findings for subsequent analysis.</p><p>F. Partition the Amazon S3 data. Use AWS Glue to crawl the S3 bucket and build the schema. Use Amazon Athena to query the data and create views to flatten nested attributes. Build Amazon QuickSight dashboards that use the Athena views. - This step involves data preparation and analysis. AWS Glue can catalog the findings stored in S3, making the data queryable with Amazon Athena. Athena can then be used to query and transform the data. Finally, Amazon QuickSight can visualize the data, providing the required custom dashboard for deeper insights.</p><p>The reasons for not choosing the other options:</p><p>A. While publishing events to Amazon CloudWatch could be useful for monitoring, it's not directly relevant to aggregating and analyzing Security Hub findings across regions and accounts for the purpose described. EventBridge serves as a more appropriate service for routing Security Hub findings.</p><p>C. While creating an EventBridge rule to deliver findings to a Kinesis data stream is a plausible step, Kinesis Data Firehose (option D) is more directly suited for delivering logs to S3 without the need for additional processing or management.</p><p>E. AWS Glue DataBrew is a tool for data preparation, but the requirement focuses more on data analysis and visualization. AWS Glue (mentioned in option F) is better suited for cataloging and preparing the data for querying with Athena, which then feeds into QuickSight for visualization.</p>"
    },
    {
        "type": "single",
        "question": "Question #117: An AWS account administrator created an IAM group and applied the following managed policy to require that each individual user authenticate using multi-factor authentication: After implementing the policy, the administrator receives reports that users are unable to perform Amazon EC2 commands using the AWS CLI. What should the administrator do to resolve this problem while still enforcing multi-factor authentication?",
        "options": {
            "A": "Change the value of aws:MultiFactorAuthPresent to true.",
            "B": "Instruct users to run the aws sts get-session-token CLI command and pass the multi-factor authentication --serial-number and -token-code parameters. Use these resulting values to make API/CLI calls.",
            "C": "Implement federated API/CLI access using SAML 2.0, then configure the identity provider to enforce multi-factor authentication.",
            "D": "Create a role and enforce multi-factor authentication in the role trust policy. Instruct users to run the sts assume-role CLI command and pass --serial-number and --token-code parameters. Store the resulting values in environment variables. Add sts:AssumeRole to NotAction in the policy."
        },
        "correctAnswer": "B",
        "image": "117.png",
        "explanation":"<p>The IAM policy in the provided image is intended to enforce multi-factor authentication (MFA) for accessing Amazon EC2 resources. However, users are reporting that they are unable to perform Amazon EC2 commands using the AWS CLI after this policy was implemented. This is likely because the AWS CLI does not inherently support MFA when making direct API calls, and the policy denies all EC2 actions if MFA is not used.</p><p>The administrator needs to resolve this issue while still enforcing MFA. The correct steps to do this would be:</p><p>B. Instruct users to run the aws sts get-session-token CLI command and pass the multi-factor authentication --serial-number and --token-code parameters. Use these resulting values to make API/CLI calls. - By using the `get-session-token` command with MFA, the user receives temporary credentials that include an `access key ID`, a `secret access key`, and a `session token`. When these temporary credentials are used, they satisfy the MFA condition in the policy, allowing users to perform EC2 actions.</p><p>Here's why the other options are incorrect or less suitable:</p><p>A. Changing the value of aws:MultiFactorAuthPresent to true. - This is not a practical solution because the `aws:MultiFactorAuthPresent` condition key is set by AWS based on the context of the API request. It cannot be manually changed by administrators in IAM policies to enforce MFA.</p><p>C. Implement federated API/CLI access using SAML 2.0, then configure the identity provider to enforce multi-factor authentication. - While federation with SAML is a valid way to enforce MFA, it is typically used for single sign-on (SSO) from corporate directories and is more complex to set up. It's not a direct solution to the immediate problem of CLI access and would involve significant changes to the way authentication is handled.</p><p>D. Create a role and enforce multi-factor authentication in the role trust policy. Instruct users to run the sts assume-role CLI command and pass --serial-number and --token-code parameters. Store the resulting values in environment variables. Add sts:AssumeRole to NotAction in the policy. - This option is unnecessarily complex for the given scenario. While assuming a role with MFA is a valid approach, the `NotAction` element in IAM policies is not typically used in this context and could lead to unintended permission grants. Additionally, this would require setting up a new role and modifying the trust policy, which is more involved than simply obtaining a session token with MFA.</p><p>Therefore, the most straightforward and immediate resolution to allow users to use the CLI with MFA enforced is option B.</p>"
    },
    {
        "type": "single",
        "question": "Question #118: A company is developing a mechanism that will help data scientists use Amazon SageMaker to read, process, and output data to an Amazon S3 bucket. Data scientists will have access to a dedicated S3 prefix for each of their projects. The company will implement bucket policies that use the dedicated S3 prefixes to restrict access to the S3 objects. The projects can last up to 60 days. The company's security team mandates that data cannot remain in the S3 bucket after the end of the projects that use the data. Which solution will meet these requirements MOST cost-effectively?",
        "options": {
            "A": "Create an AWS Lambda function to identify and delete objects in the S3 bucket that have not been accessed for 60 days. Create an Amazon EventBridge scheduled rule that runs every day to invoke the Lambda function.",
            "B": "Create a new S3 bucket. Configure the new S3 bucket to use S3 Intelligent-Tiering. Copy the objects to the new S3 bucket.",
            "C": "Create an S3 Lifecycle configuration for each S3 bucket prefix for each project. Set the S3 Lifecycle configurations to expire objects after 60 days.",
            "D": "Create an AWS Lambda function to delete objects that have not been accessed for 60 days. Create an S3 event notification for S3 Intelligent-Tiering automatic archival events to invoke the Lambda function."
        },
        "correctAnswer": "C",
        "explanation":"<p>The most cost-effective solution that ensures data does not remain in the S3 bucket after the end of the projects is to automate the process using native S3 features. Here&rsquo;s an analysis of the provided options:</p><p>C. Create an S3 Lifecycle configuration for each S3 bucket prefix for each project. Set the S3 Lifecycle configurations to expire objects after 60 days. - This option aligns with the requirements and leverages built-in S3 features to manage object lifecycle. By setting a lifecycle policy to automatically expire objects after 60 days, the objects will be deleted without the need for additional services or manual intervention. This is a cost-effective solution because there are no additional charges for setting up lifecycle policies, and it reduces the storage cost by deleting the data that is no longer needed.</p><p>Now, let's look at why the other options are less suitable:</p><p>A. Create an AWS Lambda function to identify and delete objects in the S3 bucket that have not been accessed for 60 days. Create an Amazon EventBridge scheduled rule that runs every day to invoke the Lambda function. - This option introduces complexity and additional costs associated with Lambda invocations and the development and maintenance of the function. It's also less reliable compared to using native S3 lifecycle policies, which are designed for this exact purpose.</p><p>B. Create a new S3 bucket. Configure the new S3 bucket to use S3 Intelligent-Tiering. Copy the objects to the new S3 bucket. - S3 Intelligent-Tiering is a cost optimization feature that moves objects between access tiers when access patterns change, which may reduce storage costs. However, it does not address the requirement to automatically delete objects after 60 days.</p><p>D. Create an AWS Lambda function to delete objects that have not been accessed for 60 days. Create an S3 event notification for S3 Intelligent-Tiering automatic archival events to invoke the Lambda function. - Similar to option A, this involves unnecessary complexity and cost. S3 event notifications for Intelligent-Tiering archival events are not designed to trigger deletion based on object age. Furthermore, the archival event does not necessarily correlate with the 60-day project end.</p><p>Therefore, the most cost-effective and straightforward solution is option C, which directly meets the requirement to automatically delete the objects after the project ends using S3 Lifecycle policies.</p>"
    }
]
